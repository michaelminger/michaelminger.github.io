
<!doctype html>
<html class="theme-next use-motion theme-next-mala">
<head>
  
    


<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="大地茫茫一片真干净" />



  <meta name="keywords" content="卷积神经网络,风格迁移," />



  <link rel="alternate" href="/atom.xml" title="Michaelming's Blog" type="application/atom+xml" />



  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="卷积神经网络：（二）风格迁移——原理部分引言本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_41108515&#x2F;article&#x2F;details&#x2F;103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若">
<meta property="og:type" content="article">
<meta property="og:title" content="风格迁移——原理部分">
<meta property="og:url" content="http://ailous.top/2018/10/28/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%BA%8C%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/index.html">
<meta property="og:site_name" content="Michaelming&#39;s Blog">
<meta property="og:description" content="卷积神经网络：（二）风格迁移——原理部分引言本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_41108515&#x2F;article&#x2F;details&#x2F;103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2019122210122665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222101416373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222102217444.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222102503552.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222102557759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222102628441.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222102708425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222104202202.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222110732731.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222110804515.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222111053849.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222111102313.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222111118616.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222111244537.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222111337543.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222111355281.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191222104239449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2018-10-28T05:47:40.000Z">
<meta property="article:modified_time" content="2020-04-14T06:03:00.771Z">
<meta property="article:author" content="Michaelming">
<meta property="article:tag" content="卷积神经网络">
<meta property="article:tag" content="风格迁移">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2019122210122665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mala',
    sidebar: 'post'
  };
</script>

  <title> 风格迁移——原理部分 | Michaelming's Blog </title>
<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode" target="_blank" rel="noopener">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">不见你如我一般</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            <i class="menu-item-icon icon-next-about"></i> <br />
            关于
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    
      

      
        <style type="text/css">

    .circle {
        width: 40px;
        height: 40px;
        background: #555 no-repeat;
        cursor: move;
    }

    .assist-btn {
        position: fixed;
        top: 50％;
        left: 10px;
        -moz-border-radius: 50px;
        -webkit-border-radius: 50px;
        border-radius: 50px;
        outline: none;
        border: none;
        color: #87daff;
    }

</style>

<script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>
<script type="text/javascript">
    // 浮动圆点展开与收缩
    /*
    $(function () {
        var assist_box = $('.assist-box');
        $('#assist_btn').hover(function () {
            assist_box.stop().show(300);
        }, function () {
            assist_box.stop().hide(150);
        })
    });
    */  
    //浮动圆点拖动
    $(function () {
        var box = document.getElementById('assist_btn');
        box.onmousedown = function (event) {
            var e = event || window.event,
                t = e.target || e.srcElement,
                // 鼠标按下时的坐标x1,y1
                x1 = e.clientX,
                y1 = e.clientY,
                //鼠标按下时的左右偏移量
                dragLeft = this.offsetLeft,
                dragTop = this.offsetTop;

            document.onmousemove = function (event) {
                var e = event || window.event,
                    t = e.target || e.srcElement,
                    // 鼠标移动时的动态坐标
                    x2 = e.clientX,
                    y2 = e.clientY,
                    // 鼠标移动时的坐标的变化量
                    x = x2 - x1,
                    y = y2 - y1;
                box.style.left = (dragLeft + x) + 'px';
                box.style.top = (dragTop + y) + 'px';
            }

            document.onmouseup = function () {
                this.onmousemove = null;
            }
        }
    });

/*
    $whitesmoke   = #f5f5f5
    $gainsboro    = #eee
    $gray-lighter = #ddd
    $grey-light   = #ccc
    $grey         = #bbb
    $grey-dark    = #999
    $grey-dim     = #666
    $black-light  = #555
    $black-deep   = #222
    $red          = #ff2a2a
    $blue-bright  = #87daff
    $blue         = #0684bd
    $blue-deep    = #262a30
*/
    // white theme
    var body = {color: "#555", background: "white"};
    var a_tag = {color: "#222"};
    var header = { background: "#f5f5f5"};
    var logo_line_i = {background: "#222"};
    // var post_code = {background: "#eee", color: "#222"};

    function switch_theme() {
        $("body").css(body);
        $("a:not('.links-of-author-item a, .site-state-item a, .site-state-posts a, .feed-link a, .motion-element a, .post-tags a, .show-commit-cls a, #donate_board a')").css(a_tag);
        $(".header, .footer").css(header);
        $(".logo-line-before i, .logo-line-after i").css(logo_line_i);
        //$(".post code").css(post_code);
        $("#idhyt-surprise-ball #idhyt-surprise-ball-animation .drag").css(a_tag);
        $(".post-title-link, .posts-expand .post-meta, .post-comments-count, .disqus-comment-count, .post-category a, .post-nav-next a, .post-nav-item a").css(a_tag);
        
        // $("code").css({color: '#c5c8c6', background: '#1d1f21'});
        $("#assist_btn").hide(1500);
    }

    $(function () {
        $("#assist_btn").dblclick(function() {
            switch_theme();
        });
    });

</script>

<div>

    <button class="assist-btn circle" id="assist_btn" title="双击切换">
        亮
    </button>

</div>









      

    

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              风格迁移——原理部分
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2018-10-28T13:47:40+08:00" content="2018-10-28">
            2018-10-28
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                  <span itemprop="name">人工智能</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><h1 id="卷积神经网络：（二）风格迁移——原理部分"><a href="#卷积神经网络：（二）风格迁移——原理部分" class="headerlink" title="卷积神经网络：（二）风格迁移——原理部分"></a>卷积神经网络：（二）风格迁移——原理部分</h1><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h4 id="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。"><a href="#本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。" class="headerlink" title="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。"></a>本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击<a href="https://blog.csdn.net/weixin_41108515/article/details/103636284" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103636284</a>，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。</h4><p>&nbsp;<br>转载请注明出处：<a href="https://blog.csdn.net/weixin_41108515/article/details/103650964" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103650964</a><br>&nbsp;<br>这里引用的是：<br><a href="http://zh.gluon.ai/chapter_computer-vision/neural-style.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_computer-vision/neural-style.html</a><br><a href="https://blog.csdn.net/aaronjny/article/details/79681080" target="_blank" rel="noopener">https://blog.csdn.net/aaronjny/article/details/79681080</a><br>这两篇都非常详细，并且经调试可以使用。</p>
<h1 id="涉及到的相关原理："><a href="#涉及到的相关原理：" class="headerlink" title="涉及到的相关原理："></a>涉及到的相关原理：</h1><h1 id="1、神经网络部分原理："><a href="#1、神经网络部分原理：" class="headerlink" title="1、神经网络部分原理："></a>1、神经网络部分原理：</h1><h2 id="1-1-神经网络基础介绍"><a href="#1-1-神经网络基础介绍" class="headerlink" title="1.1 神经网络基础介绍"></a>1.1 神经网络基础介绍</h2><h4 id="神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。"><a href="#神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。" class="headerlink" title="神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。"></a>神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。</h4><h4 id="生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。"><a href="#生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。" class="headerlink" title="生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。"></a>生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。<img src="https://img-blog.csdnimg.cn/2019122210122665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。</h4><h4 id="神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面："><a href="#神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：" class="headerlink" title="神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面："></a>神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：</h4><h5 id="（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。"><a href="#（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。" class="headerlink" title="（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。"></a>（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。</h5><h5 id="（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。"><a href="#（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。" class="headerlink" title="（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。"></a>（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。</h5><h5 id="（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。"><a href="#（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。" class="headerlink" title="（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。"></a>（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。</h5><h5 id="（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。"><a href="#（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。" class="headerlink" title="（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。"></a>（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。</h5><h4 id="纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。"><a href="#纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。" class="headerlink" title="纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。"></a>纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。<img src="https://img-blog.csdnimg.cn/20191222101416373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h2 id="1-2-卷积神经网络基本结构"><a href="#1-2-卷积神经网络基本结构" class="headerlink" title="1.2 卷积神经网络基本结构"></a>1.2 卷积神经网络基本结构</h2><h4 id="卷积神经网络-Convolutional-Neural-Network，CNN-是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注-16-。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络-Convolutional-Neural-Networks简称CNN-的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。"><a href="#卷积神经网络-Convolutional-Neural-Network，CNN-是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注-16-。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络-Convolutional-Neural-Networks简称CNN-的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。" class="headerlink" title="卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。"></a>卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。</h4><h3 id="1-2-1-输入层"><a href="#1-2-1-输入层" class="headerlink" title="1.2.1 输入层"></a>1.2.1 输入层</h3><h4 id="卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。"><a href="#卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。" class="headerlink" title="卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。"></a>卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。</h4><h3 id="1-2-2-隐含层"><a href="#1-2-2-隐含层" class="headerlink" title="1.2.2 隐含层"></a>1.2.2 隐含层</h3><h4 id="1-卷积层"><a href="#1-卷积层" class="headerlink" title="1.卷积层"></a>1.卷积层</h4><h5 id="（1）卷积层"><a href="#（1）卷积层" class="headerlink" title="（1）卷积层"></a>（1）卷积层</h5><h4 id="利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度-即颜色的三原色，以RGB表示-。"><a href="#利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度-即颜色的三原色，以RGB表示-。" class="headerlink" title="利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。"></a>利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。<img src="https://img-blog.csdnimg.cn/20191222102217444.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。"><a href="#如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。" class="headerlink" title="如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。"></a>如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。</h4><p><img src="https://img-blog.csdnimg.cn/20191222102503552.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="（2）卷积层参数"><a href="#（2）卷积层参数" class="headerlink" title="（2）卷积层参数"></a>（2）卷积层参数</h5><h4 id="卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。"><a href="#卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。" class="headerlink" title="卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。"></a>卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。</h4><h4 id="卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。"><a href="#卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。" class="headerlink" title="卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。"></a>卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。</h4><h4 id="由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类："><a href="#由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：" class="headerlink" title="由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类："></a>由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：</h4><h5 id="有效填充（valid-padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为-L-f-s-1。"><a href="#有效填充（valid-padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为-L-f-s-1。" class="headerlink" title="有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。"></a>有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。</h5><h5 id="相同填充-半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。"><a href="#相同填充-半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。" class="headerlink" title="相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。"></a>相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。</h5><h5 id="全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L-f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。"><a href="#全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L-f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。" class="headerlink" title="全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。"></a>全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。</h5><h5 id="任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。"><a href="#任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。" class="headerlink" title="任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。"></a>任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。</h5><h5 id="（3）激励函数"><a href="#（3）激励函数" class="headerlink" title="（3）激励函数"></a>（3）激励函数</h5><h4 id="一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质："><a href="#一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：" class="headerlink" title="一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质："></a>一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：</h4><h5 id="1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。"><a href="#1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。" class="headerlink" title="1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。"></a>1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。</h5><h5 id="2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。"><a href="#2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。" class="headerlink" title="2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。"></a>2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。</h5><h5 id="3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。"><a href="#3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。" class="headerlink" title="3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。"></a>3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。</h5><h4 id="经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层-比较常见，后者ReLU常见于卷积层。"><a href="#经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层-比较常见，后者ReLU常见于卷积层。" class="headerlink" title="经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。"></a>经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。</h4><h4 id="2-池化层"><a href="#2-池化层" class="headerlink" title="2.池化层"></a>2.池化层</h4><h4 id="池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化："><a href="#池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：" class="headerlink" title="池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化："></a>池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：</h4><h5 id="（1）一般池化-General-Pooling"><a href="#（1）一般池化-General-Pooling" class="headerlink" title="（1）一般池化(General Pooling)"></a>（1）一般池化(General Pooling)</h5><h5 id="1）mean-pooling，即只要求邻域中特征点的平均值；"><a href="#1）mean-pooling，即只要求邻域中特征点的平均值；" class="headerlink" title="1）mean-pooling，即只要求邻域中特征点的平均值；"></a>1）mean-pooling，即只要求邻域中特征点的平均值；</h5><h5 id="2）max-pooling，即在邻域中提取最大特征点；"><a href="#2）max-pooling，即在邻域中提取最大特征点；" class="headerlink" title="2）max-pooling，即在邻域中提取最大特征点；"></a>2）max-pooling，即在邻域中提取最大特征点；</h5><h5 id="3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。"><a href="#3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。" class="headerlink" title="3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。"></a>3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。</h5><h4 id="特征提取的误差主要来自两个方面："><a href="#特征提取的误差主要来自两个方面：" class="headerlink" title="特征提取的误差主要来自两个方面："></a>特征提取的误差主要来自两个方面：</h4><h5 id="1）邻域大小受限造成的估计值方差增大；"><a href="#1）邻域大小受限造成的估计值方差增大；" class="headerlink" title="1）邻域大小受限造成的估计值方差增大；"></a>1）邻域大小受限造成的估计值方差增大；</h5><h5 id="2）卷积层参数误差导致估计均值的偏移。"><a href="#2）卷积层参数误差导致估计均值的偏移。" class="headerlink" title="2）卷积层参数误差导致估计均值的偏移。"></a>2）卷积层参数误差导致估计均值的偏移。</h5><h4 id="一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean"><a href="#一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean" class="headerlink" title="一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-"></a>一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-</h4><h4 id="pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，"><a href="#pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，" class="headerlink" title="pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，"></a>pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，</h4><p><img src="https://img-blog.csdnimg.cn/20191222102557759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="（2）空间金字塔池化-Spatial-pyramid-pooling"><a href="#（2）空间金字塔池化-Spatial-pyramid-pooling" class="headerlink" title="（2）空间金字塔池化(Spatial pyramid pooling)"></a>（2）空间金字塔池化(Spatial pyramid pooling)</h5><h4 id="一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。"><a href="#一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。" class="headerlink" title="一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。"></a>一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。</h4><h4 id="空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。"><a href="#空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。" class="headerlink" title="空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。"></a>空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。</h4><h4 id="3-全连接层"><a href="#3-全连接层" class="headerlink" title="3.全连接层"></a>3.全连接层</h4><h4 id="卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。"><a href="#卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。" class="headerlink" title="卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。"></a>卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。</h4><h3 id="1-2-3-输出层"><a href="#1-2-3-输出层" class="headerlink" title="1.2.3 输出层"></a>1.2.3 输出层</h3><h4 id="卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。"><a href="#卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。" class="headerlink" title="卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。"></a>卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。</h4><h2 id="1-3-卷积神经网络的卷积过程"><a href="#1-3-卷积神经网络的卷积过程" class="headerlink" title="1.3 卷积神经网络的卷积过程"></a>1.3 卷积神经网络的卷积过程</h2><h4 id="卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层-convolutional-layer-、池化层-pooling-layer-、全连接层-fully-connected-layer-。"><a href="#卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层-convolutional-layer-、池化层-pooling-layer-、全连接层-fully-connected-layer-。" class="headerlink" title="卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。"></a>卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。</h4><p><img src="https://img-blog.csdnimg.cn/20191222102628441.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="图中的卷积网络工作流程如下，-输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述："><a href="#图中的卷积网络工作流程如下，-输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：" class="headerlink" title="图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述："></a>图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：</h4><h4 id="第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5-的接受域。"><a href="#第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5-的接受域。" class="headerlink" title="第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。"></a>第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。</h4><h4 id="第二隐藏层实现子采样和局部平均，它同样由-6个特征图组成，但其每个特征图由14×14-个神经元组成。每个神经元具有2×2-的接受域。"><a href="#第二隐藏层实现子采样和局部平均，它同样由-6个特征图组成，但其每个特征图由14×14-个神经元组成。每个神经元具有2×2-的接受域。" class="headerlink" title="第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。"></a>第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。</h4><h4 id="第三隐藏层进行第二次卷积，它由-16个特征图组成，每个特征图由-10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。"><a href="#第三隐藏层进行第二次卷积，它由-16个特征图组成，每个特征图由-10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。" class="headerlink" title="第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。"></a>第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。</h4><h4 id="第四个隐藏层进行第二次子采样和局部平均计算。它由-16个特征图组成，但每个特征图由-5×5个神经元构成，它以与第一次采样相同的方式进行工作。"><a href="#第四个隐藏层进行第二次子采样和局部平均计算。它由-16个特征图组成，但每个特征图由-5×5个神经元构成，它以与第一次采样相同的方式进行工作。" class="headerlink" title="第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。"></a>第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。</h4><h4 id="第五个隐藏层实现了卷积的最后阶段，它由-120个神经元组成，每个神经元指定5×5-的接受域。"><a href="#第五个隐藏层实现了卷积的最后阶段，它由-120个神经元组成，每个神经元指定5×5-的接受域。" class="headerlink" title="第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。"></a>第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。</h4><h4 id="端部是个全连接层，得到输出向量。"><a href="#端部是个全连接层，得到输出向量。" class="headerlink" title="端部是个全连接层，得到输出向量。"></a>端部是个全连接层，得到输出向量。</h4><h4 id="卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加-17-。"><a href="#卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加-17-。" class="headerlink" title="卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。"></a>卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。</h4><h4 id="卷积层研究输入数据的特征。卷积层由卷积核-convolutional-kernel-组成，卷积核用来计算不同的特征图；激励函数-activation-function-给卷积神经网络引入了非线性，常用的有sigmid、tanh、-ReLU函数；池化层减少了卷积层输出的特征向量，改良结果-使结构不易过拟合-，典型应用有average-pooling-和-max-pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。"><a href="#卷积层研究输入数据的特征。卷积层由卷积核-convolutional-kernel-组成，卷积核用来计算不同的特征图；激励函数-activation-function-给卷积神经网络引入了非线性，常用的有sigmid、tanh、-ReLU函数；池化层减少了卷积层输出的特征向量，改良结果-使结构不易过拟合-，典型应用有average-pooling-和-max-pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。" class="headerlink" title="卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。"></a>卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。</h4><h1 id="2、迁移学习相关原理"><a href="#2、迁移学习相关原理" class="headerlink" title="2、迁移学习相关原理"></a>2、迁移学习相关原理</h1><h2 id="2-1-迁移学习"><a href="#2-1-迁移学习" class="headerlink" title="2.1 迁移学习"></a>2.1 迁移学习</h2><h4 id="在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception-V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。"><a href="#在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception-V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。" class="headerlink" title="在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。"></a>在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。</h4><h2 id="2-2TensorFlow"><a href="#2-2TensorFlow" class="headerlink" title="2.2TensorFlow"></a>2.2TensorFlow</h2><h4 id="TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor-张量-意味着N维数组，Flow-流-意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。"><a href="#TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor-张量-意味着N维数组，Flow-流-意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。" class="headerlink" title="TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。"></a>TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。</h4><h4 id="TensorFlow-表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU-GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。"><a href="#TensorFlow-表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU-GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。" class="headerlink" title="TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。"></a>TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。</h4><h4 id="TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。"><a href="#TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。" class="headerlink" title="TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。"></a>TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。</h4><h5 id="（1）支持多种硬件的平台"><a href="#（1）支持多种硬件的平台" class="headerlink" title="（1）支持多种硬件的平台"></a>（1）支持多种硬件的平台</h5><h4 id="例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。"><a href="#例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。" class="headerlink" title="例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。"></a>例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。</h4><h5 id="（2）支持多种开发环境"><a href="#（2）支持多种开发环境" class="headerlink" title="（2）支持多种开发环境"></a>（2）支持多种开发环境</h5><h4 id="支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。"><a href="#支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。" class="headerlink" title="支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。"></a>支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。</h4><h4 id="目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。"><a href="#目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。" class="headerlink" title="目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。"></a>目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。</h4><h2 id="2-3VGG卷积神经网络模型"><a href="#2-3VGG卷积神经网络模型" class="headerlink" title="2.3VGG卷积神经网络模型"></a>2.3VGG卷积神经网络模型</h2><h4 id="VGG全称是Visual-Geometry-Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19-20-。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为-GG-Very-Deep-16-CNN-，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层-全连接层总数目的不同可以从VGG11-～-VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层-3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11-～GVV19的结构图："><a href="#VGG全称是Visual-Geometry-Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19-20-。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为-GG-Very-Deep-16-CNN-，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层-全连接层总数目的不同可以从VGG11-～-VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层-3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11-～GVV19的结构图：" class="headerlink" title="VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图："></a>VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图：</h4><p><img src="https://img-blog.csdnimg.cn/20191222102708425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3-stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。"><a href="#在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3-stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。" class="headerlink" title="在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。"></a>在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层3<em>3卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层1</em>1的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层3<em>3的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加3</em>3的卷积层，一共19层。模型E就是VGG19网络。</h4><h1 id="3、通过VGG实现风格迁移"><a href="#3、通过VGG实现风格迁移" class="headerlink" title="3、通过VGG实现风格迁移"></a>3、通过VGG实现风格迁移</h1><h2 id="3-1-图像风格迁移的原理"><a href="#3-1-图像风格迁移的原理" class="headerlink" title="3.1 图像风格迁移的原理"></a>3.1 图像风格迁移的原理</h2><h4 id="VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1-1-conv1-2-），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5-1-conv5-2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。"><a href="#VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1-1-conv1-2-），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5-1-conv5-2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。" class="headerlink" title="VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。"></a>VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。</h4><h4 id="VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示："><a href="#VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：" class="headerlink" title="VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示："></a>VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：</h4><p><img src="https://img-blog.csdnimg.cn/20191222104202202.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像-研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。"><a href="#具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像-研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。" class="headerlink" title="具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。"></a>具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。</h4><h2 id="3-2-代价函数"><a href="#3-2-代价函数" class="headerlink" title="3.2 代价函数"></a>3.2 代价函数</h2><h4 id="要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J-G-，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。"><a href="#要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J-G-，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。" class="headerlink" title="要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。"></a>要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。</h4><h4 id="Jcontent-C-G"><a href="#Jcontent-C-G" class="headerlink" title="Jcontent(C,G)"></a>Jcontent(C,G)</h4><h4 id="第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。"><a href="#第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。" class="headerlink" title="第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。"></a>第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。</h4><h4 id="Jstyle-S-G"><a href="#Jstyle-S-G" class="headerlink" title="Jstyle(S,G)"></a>Jstyle(S,G)</h4><h4 id="然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。"><a href="#然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。" class="headerlink" title="然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。"></a>然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。</h4><h4 id="J-G-αJcontent-C-G-βJstyle-S-G"><a href="#J-G-αJcontent-C-G-βJstyle-S-G" class="headerlink" title="J(G)=αJcontent(C,G)+βJstyle(S,G)"></a>J(G)=αJcontent(C,G)+βJstyle(S,G)</h4><h4 id="最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。"><a href="#最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。" class="headerlink" title="最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。"></a>最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。</h4><h2 id="3-3-内容代价函数"><a href="#3-3-内容代价函数" class="headerlink" title="3.3 内容代价函数"></a>3.3 内容代价函数</h2><h4 id="风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。"><a href="#风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。" class="headerlink" title="风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。"></a>风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。</h4><h4 id="首先定义内容代价部分。"><a href="#首先定义内容代价部分。" class="headerlink" title="首先定义内容代价部分。"></a>首先定义内容代价部分。</h4><h4 id="用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层-1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是-VGG-网络。"><a href="#用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层-1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是-VGG-网络。" class="headerlink" title="用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。"></a>用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。</h4><h4 id="之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α-L-C-和α-L-G-的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。"><a href="#之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α-L-C-和α-L-G-的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。" class="headerlink" title="之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。"></a>之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。</h4><p> <img src="https://img-blog.csdnimg.cn/20191222110732731.png" alt="在这里插入图片描述"></p>
<h4 id="为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1-2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。"><a href="#为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1-2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。" class="headerlink" title="为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。"></a>为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。</h4><h2 id="3-4-风格代价函数"><a href="#3-4-风格代价函数" class="headerlink" title="3.4 风格代价函数"></a>3.4 风格代价函数</h2><h4 id="图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。"><a href="#图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。" class="headerlink" title="图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。"></a>图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。</h4><h4 id="风格矩阵是一组向量的内积对称矩阵，比如向量组"><a href="#风格矩阵是一组向量的内积对称矩阵，比如向量组" class="headerlink" title="风格矩阵是一组向量的内积对称矩阵，比如向量组"></a>风格矩阵是一组向量的内积对称矩阵，比如向量组<img src="https://img-blog.csdnimg.cn/20191222110804515.png" alt="在这里插入图片描述"></h4><h4 id="的Gram矩阵是"><a href="#的Gram矩阵是" class="headerlink" title="的Gram矩阵是"></a>的Gram矩阵是</h4><p><img src="https://img-blog.csdnimg.cn/20191222111053849.png" alt="在这里插入图片描述"></p>
<h4 id="取内积即欧几里得空间上的标准内积，即"><a href="#取内积即欧几里得空间上的标准内积，即" class="headerlink" title="取内积即欧几里得空间上的标准内积，即"></a>取内积即欧几里得空间上的标准内积，即</h4><p> <img src="https://img-blog.csdnimg.cn/20191222111102313.png" alt="在这里插入图片描述"></p>
<h4 id="假设卷积层的输出为F-ij-l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为"><a href="#假设卷积层的输出为F-ij-l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为" class="headerlink" title="假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为"></a>假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为</h4><p><img src="https://img-blog.csdnimg.cn/20191222111118616.png" alt="在这里插入图片描述"></p>
<h4 id="设在第l层中，卷积特征的通道数为N-l-卷积的高、宽乘积数为M-l-那么F-ij-l满足"><a href="#设在第l层中，卷积特征的通道数为N-l-卷积的高、宽乘积数为M-l-那么F-ij-l满足" class="headerlink" title="设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足"></a>设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足</h4><h4 id="l≤i≤N-l，l≤j≤M-l"><a href="#l≤i≤N-l，l≤j≤M-l" class="headerlink" title="l≤i≤N_l，l≤j≤M_l"></a>l≤i≤N_l，l≤j≤M_l</h4><h4 id="Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。"><a href="#Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。" class="headerlink" title="Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。"></a>Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。</h4><h2 id="3-5-模型训练过程"><a href="#3-5-模型训练过程" class="headerlink" title="3.5 模型训练过程"></a>3.5 模型训练过程</h2><h4 id="首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。"><a href="#首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。" class="headerlink" title="首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。"></a>首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。</h4><h4 id="使用-‘conv4-2’-’conv5-2’-表示内容特征，使用-‘conv1-1’-’conv2-1’-’conv3-1’-’conv4-1’-表示风格特征。"><a href="#使用-‘conv4-2’-’conv5-2’-表示内容特征，使用-‘conv1-1’-’conv2-1’-’conv3-1’-’conv4-1’-表示风格特征。" class="headerlink" title="使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。"></a>使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。</h4><h4 id="将内容图片输入网络，计算内容图片在网络指定层上的输出值。"><a href="#将内容图片输入网络，计算内容图片在网络指定层上的输出值。" class="headerlink" title="将内容图片输入网络，计算内容图片在网络指定层上的输出值。"></a>将内容图片输入网络，计算内容图片在网络指定层上的输出值。</h4><h4 id="计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。"><a href="#计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。" class="headerlink" title="计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。"></a>计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。</h4><h4 id="对应每一层的内容损失函数："><a href="#对应每一层的内容损失函数：" class="headerlink" title="对应每一层的内容损失函数："></a>对应每一层的内容损失函数：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111244537.png" alt="在这里插入图片描述"></p>
<h4 id="其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长-宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。"><a href="#其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长-宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。" class="headerlink" title="其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。"></a>其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。</h4><h4 id="将风格图片输入网络，计算风格图片在网络指定层上的输出值。"><a href="#将风格图片输入网络，计算风格图片在网络指定层上的输出值。" class="headerlink" title="将风格图片输入网络，计算风格图片在网络指定层上的输出值。"></a>将风格图片输入网络，计算风格图片在网络指定层上的输出值。</h4><h4 id="计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。"><a href="#计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。" class="headerlink" title="计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。"></a>计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。</h4><h4 id="对于每一层的风格损失函数："><a href="#对于每一层的风格损失函数：" class="headerlink" title="对于每一层的风格损失函数："></a>对于每一层的风格损失函数：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111337543.png" alt="在这里插入图片描述"></p>
<h4 id="其中M是特征矩阵的长-宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。"><a href="#其中M是特征矩阵的长-宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。" class="headerlink" title="其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。"></a>其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。</h4><h4 id="最终的风格损失为，每一层的风格损失加权和，再对层数取平均。"><a href="#最终的风格损失为，每一层的风格损失加权和，再对层数取平均。" class="headerlink" title="最终的风格损失为，每一层的风格损失加权和，再对层数取平均。"></a>最终的风格损失为，每一层的风格损失加权和，再对层数取平均。</h4><h4 id="函数为内容损失和风格损失的加权和："><a href="#函数为内容损失和风格损失的加权和：" class="headerlink" title="函数为内容损失和风格损失的加权和："></a>函数为内容损失和风格损失的加权和：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111355281.png" alt="在这里插入图片描述"></p>
<h4 id="当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。"><a href="#当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。" class="headerlink" title="当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。"></a>当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。</h4><p><img src="https://img-blog.csdnimg.cn/20191222104239449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！"><a href="#原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！" class="headerlink" title="原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！"></a>原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！</h2><p><a href="https://blog.csdn.net/weixin_41108515/article/details/103651784" target="_blank" rel="noopener">卷积神经网络：（三）风格迁移——代码部分</a></p>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">#卷积神经网络</a>
          
            <a href="/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/" rel="tag">#风格迁移</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%89%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86%20(1)/" rel="prev">风格迁移——代码部分</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%80%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%20(1)/" rel="next">风格迁移——环境配置</a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div>
      
    </div>

    <div class="post-spread">
      
    </div>
  </div>

 </div>

        
          <div class="comments" id="comments">
            
            -->
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="Michaelming" itemprop="image"/>
          <p class="site-author-name" itemprop="name">Michaelming</p>
        </div>
        <p class="site-description motion-element" itemprop="description">大地茫茫一片真干净</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">21</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">17</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="menu-item-icon icon-next-feed"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/michaelminger/michaelminger.github.io" target="_blank">GitHub</a>
              </span>
            
          
        </div>

        <div class="links-of-friendly motion-element">
          
        </div>

        
        

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络：（二）风格迁移——原理部分"><span class="nav-number">1.</span> <span class="nav-text">卷积神经网络：（二）风格迁移——原理部分</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#引言"><span class="nav-number">2.</span> <span class="nav-text">引言</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_41108515&#x2F;article&#x2F;details&#x2F;103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#涉及到的相关原理："><span class="nav-number">3.</span> <span class="nav-text">涉及到的相关原理：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1、神经网络部分原理："><span class="nav-number">4.</span> <span class="nav-text">1、神经网络部分原理：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-神经网络基础介绍"><span class="nav-number">4.1.</span> <span class="nav-text">1.1 神经网络基础介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。"><span class="nav-number">4.1.0.1.</span> <span class="nav-text">神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。"><span class="nav-number">4.1.0.2.</span> <span class="nav-text">生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面："><span class="nav-number">4.1.0.3.</span> <span class="nav-text">神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。"><span class="nav-number">4.1.0.3.1.</span> <span class="nav-text">（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。"><span class="nav-number">4.1.0.3.2.</span> <span class="nav-text">（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。"><span class="nav-number">4.1.0.3.3.</span> <span class="nav-text">（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。"><span class="nav-number">4.1.0.3.4.</span> <span class="nav-text">（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。"><span class="nav-number">4.1.0.4.</span> <span class="nav-text">纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-卷积神经网络基本结构"><span class="nav-number">4.2.</span> <span class="nav-text">1.2 卷积神经网络基本结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络-Convolutional-Neural-Network，CNN-是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注-16-。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络-Convolutional-Neural-Networks简称CNN-的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。"><span class="nav-number">4.2.0.1.</span> <span class="nav-text">卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-输入层"><span class="nav-number">4.2.1.</span> <span class="nav-text">1.2.1 输入层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-隐含层"><span class="nav-number">4.2.2.</span> <span class="nav-text">1.2.2 隐含层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-卷积层"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">1.卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#（1）卷积层"><span class="nav-number">4.2.2.1.1.</span> <span class="nav-text">（1）卷积层</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度-即颜色的三原色，以RGB表示-。"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#（2）卷积层参数"><span class="nav-number">4.2.2.3.1.</span> <span class="nav-text">（2）卷积层参数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。"><span class="nav-number">4.2.2.4.</span> <span class="nav-text">卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。"><span class="nav-number">4.2.2.5.</span> <span class="nav-text">卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类："><span class="nav-number">4.2.2.6.</span> <span class="nav-text">由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#有效填充（valid-padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为-L-f-s-1。"><span class="nav-number">4.2.2.6.1.</span> <span class="nav-text">有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)&#x2F;s+1。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#相同填充-半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。"><span class="nav-number">4.2.2.6.2.</span> <span class="nav-text">相同填充&#x2F;半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L-f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。"><span class="nav-number">4.2.2.6.3.</span> <span class="nav-text">全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。"><span class="nav-number">4.2.2.6.4.</span> <span class="nav-text">任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#（3）激励函数"><span class="nav-number">4.2.2.6.5.</span> <span class="nav-text">（3）激励函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质："><span class="nav-number">4.2.2.7.</span> <span class="nav-text">一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。"><span class="nav-number">4.2.2.7.1.</span> <span class="nav-text">1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。"><span class="nav-number">4.2.2.7.2.</span> <span class="nav-text">2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。"><span class="nav-number">4.2.2.7.3.</span> <span class="nav-text">3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层-比较常见，后者ReLU常见于卷积层。"><span class="nav-number">4.2.2.8.</span> <span class="nav-text">经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-池化层"><span class="nav-number">4.2.2.9.</span> <span class="nav-text">2.池化层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化："><span class="nav-number">4.2.2.10.</span> <span class="nav-text">池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#（1）一般池化-General-Pooling"><span class="nav-number">4.2.2.10.1.</span> <span class="nav-text">（1）一般池化(General Pooling)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1）mean-pooling，即只要求邻域中特征点的平均值；"><span class="nav-number">4.2.2.10.2.</span> <span class="nav-text">1）mean-pooling，即只要求邻域中特征点的平均值；</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2）max-pooling，即在邻域中提取最大特征点；"><span class="nav-number">4.2.2.10.3.</span> <span class="nav-text">2）max-pooling，即在邻域中提取最大特征点；</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。"><span class="nav-number">4.2.2.10.4.</span> <span class="nav-text">3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征提取的误差主要来自两个方面："><span class="nav-number">4.2.2.11.</span> <span class="nav-text">特征提取的误差主要来自两个方面：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1）邻域大小受限造成的估计值方差增大；"><span class="nav-number">4.2.2.11.1.</span> <span class="nav-text">1）邻域大小受限造成的估计值方差增大；</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2）卷积层参数误差导致估计均值的偏移。"><span class="nav-number">4.2.2.11.2.</span> <span class="nav-text">2）卷积层参数误差导致估计均值的偏移。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean"><span class="nav-number">4.2.2.12.</span> <span class="nav-text">一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，"><span class="nav-number">4.2.2.13.</span> <span class="nav-text">pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#（2）空间金字塔池化-Spatial-pyramid-pooling"><span class="nav-number">4.2.2.13.1.</span> <span class="nav-text">（2）空间金字塔池化(Spatial pyramid pooling)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。"><span class="nav-number">4.2.2.14.</span> <span class="nav-text">一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。"><span class="nav-number">4.2.2.15.</span> <span class="nav-text">空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-全连接层"><span class="nav-number">4.2.2.16.</span> <span class="nav-text">3.全连接层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。"><span class="nav-number">4.2.2.17.</span> <span class="nav-text">卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-3-输出层"><span class="nav-number">4.2.3.</span> <span class="nav-text">1.2.3 输出层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。"><span class="nav-number">4.2.3.1.</span> <span class="nav-text">卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-卷积神经网络的卷积过程"><span class="nav-number">4.3.</span> <span class="nav-text">1.3 卷积神经网络的卷积过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层-convolutional-layer-、池化层-pooling-layer-、全连接层-fully-connected-layer-。"><span class="nav-number">4.3.0.1.</span> <span class="nav-text">卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图中的卷积网络工作流程如下，-输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述："><span class="nav-number">4.3.0.2.</span> <span class="nav-text">图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5-的接受域。"><span class="nav-number">4.3.0.3.</span> <span class="nav-text">第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第二隐藏层实现子采样和局部平均，它同样由-6个特征图组成，但其每个特征图由14×14-个神经元组成。每个神经元具有2×2-的接受域。"><span class="nav-number">4.3.0.4.</span> <span class="nav-text">第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第三隐藏层进行第二次卷积，它由-16个特征图组成，每个特征图由-10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。"><span class="nav-number">4.3.0.5.</span> <span class="nav-text">第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第四个隐藏层进行第二次子采样和局部平均计算。它由-16个特征图组成，但每个特征图由-5×5个神经元构成，它以与第一次采样相同的方式进行工作。"><span class="nav-number">4.3.0.6.</span> <span class="nav-text">第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第五个隐藏层实现了卷积的最后阶段，它由-120个神经元组成，每个神经元指定5×5-的接受域。"><span class="nav-number">4.3.0.7.</span> <span class="nav-text">第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#端部是个全连接层，得到输出向量。"><span class="nav-number">4.3.0.8.</span> <span class="nav-text">端部是个全连接层，得到输出向量。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加-17-。"><span class="nav-number">4.3.0.9.</span> <span class="nav-text">卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积层研究输入数据的特征。卷积层由卷积核-convolutional-kernel-组成，卷积核用来计算不同的特征图；激励函数-activation-function-给卷积神经网络引入了非线性，常用的有sigmid、tanh、-ReLU函数；池化层减少了卷积层输出的特征向量，改良结果-使结构不易过拟合-，典型应用有average-pooling-和-max-pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。"><span class="nav-number">4.3.0.10.</span> <span class="nav-text">卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2、迁移学习相关原理"><span class="nav-number">5.</span> <span class="nav-text">2、迁移学习相关原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-迁移学习"><span class="nav-number">5.1.</span> <span class="nav-text">2.1 迁移学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception-V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。"><span class="nav-number">5.1.0.1.</span> <span class="nav-text">在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2TensorFlow"><span class="nav-number">5.2.</span> <span class="nav-text">2.2TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor-张量-意味着N维数组，Flow-流-意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。"><span class="nav-number">5.2.0.1.</span> <span class="nav-text">TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow-表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU-GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。"><span class="nav-number">5.2.0.2.</span> <span class="nav-text">TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU &#x2F; GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。"><span class="nav-number">5.2.0.3.</span> <span class="nav-text">TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#（1）支持多种硬件的平台"><span class="nav-number">5.2.0.3.1.</span> <span class="nav-text">（1）支持多种硬件的平台</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。"><span class="nav-number">5.2.0.4.</span> <span class="nav-text">例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#（2）支持多种开发环境"><span class="nav-number">5.2.0.4.1.</span> <span class="nav-text">（2）支持多种开发环境</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。"><span class="nav-number">5.2.0.5.</span> <span class="nav-text">支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。"><span class="nav-number">5.2.0.6.</span> <span class="nav-text">目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3VGG卷积神经网络模型"><span class="nav-number">5.3.</span> <span class="nav-text">2.3VGG卷积神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG全称是Visual-Geometry-Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19-20-。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为-GG-Very-Deep-16-CNN-，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层-全连接层总数目的不同可以从VGG11-～-VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层-3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11-～GVV19的结构图："><span class="nav-number">5.3.0.1.</span> <span class="nav-text">VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3-stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。"><span class="nav-number">5.3.0.2.</span> <span class="nav-text">在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3、通过VGG实现风格迁移"><span class="nav-number">6.</span> <span class="nav-text">3、通过VGG实现风格迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-图像风格迁移的原理"><span class="nav-number">6.1.</span> <span class="nav-text">3.1 图像风格迁移的原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1-1-conv1-2-），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5-1-conv5-2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。"><span class="nav-number">6.1.0.1.</span> <span class="nav-text">VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示："><span class="nav-number">6.1.0.2.</span> <span class="nav-text">VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像-研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。"><span class="nav-number">6.1.0.3.</span> <span class="nav-text">具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-代价函数"><span class="nav-number">6.2.</span> <span class="nav-text">3.2 代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J-G-，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。"><span class="nav-number">6.2.0.1.</span> <span class="nav-text">要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Jcontent-C-G"><span class="nav-number">6.2.0.2.</span> <span class="nav-text">Jcontent(C,G)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。"><span class="nav-number">6.2.0.3.</span> <span class="nav-text">第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Jstyle-S-G"><span class="nav-number">6.2.0.4.</span> <span class="nav-text">Jstyle(S,G)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。"><span class="nav-number">6.2.0.5.</span> <span class="nav-text">然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#J-G-αJcontent-C-G-βJstyle-S-G"><span class="nav-number">6.2.0.6.</span> <span class="nav-text">J(G)&#x3D;αJcontent(C,G)+βJstyle(S,G)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。"><span class="nav-number">6.2.0.7.</span> <span class="nav-text">最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-内容代价函数"><span class="nav-number">6.3.</span> <span class="nav-text">3.3 内容代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。"><span class="nav-number">6.3.0.1.</span> <span class="nav-text">风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#首先定义内容代价部分。"><span class="nav-number">6.3.0.2.</span> <span class="nav-text">首先定义内容代价部分。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层-1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是-VGG-网络。"><span class="nav-number">6.3.0.3.</span> <span class="nav-text">用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α-L-C-和α-L-G-的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。"><span class="nav-number">6.3.0.4.</span> <span class="nav-text">之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1-2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。"><span class="nav-number">6.3.0.5.</span> <span class="nav-text">为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1&#x2F;2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-风格代价函数"><span class="nav-number">6.4.</span> <span class="nav-text">3.4 风格代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。"><span class="nav-number">6.4.0.1.</span> <span class="nav-text">图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#风格矩阵是一组向量的内积对称矩阵，比如向量组"><span class="nav-number">6.4.0.2.</span> <span class="nav-text">风格矩阵是一组向量的内积对称矩阵，比如向量组</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#的Gram矩阵是"><span class="nav-number">6.4.0.3.</span> <span class="nav-text">的Gram矩阵是</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#取内积即欧几里得空间上的标准内积，即"><span class="nav-number">6.4.0.4.</span> <span class="nav-text">取内积即欧几里得空间上的标准内积，即</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#假设卷积层的输出为F-ij-l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为"><span class="nav-number">6.4.0.5.</span> <span class="nav-text">假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#设在第l层中，卷积特征的通道数为N-l-卷积的高、宽乘积数为M-l-那么F-ij-l满足"><span class="nav-number">6.4.0.6.</span> <span class="nav-text">设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l≤i≤N-l，l≤j≤M-l"><span class="nav-number">6.4.0.7.</span> <span class="nav-text">l≤i≤N_l，l≤j≤M_l</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。"><span class="nav-number">6.4.0.8.</span> <span class="nav-text">Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-模型训练过程"><span class="nav-number">6.5.</span> <span class="nav-text">3.5 模型训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。"><span class="nav-number">6.5.0.1.</span> <span class="nav-text">首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-‘conv4-2’-’conv5-2’-表示内容特征，使用-‘conv1-1’-’conv2-1’-’conv3-1’-’conv4-1’-表示风格特征。"><span class="nav-number">6.5.0.2.</span> <span class="nav-text">使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#将内容图片输入网络，计算内容图片在网络指定层上的输出值。"><span class="nav-number">6.5.0.3.</span> <span class="nav-text">将内容图片输入网络，计算内容图片在网络指定层上的输出值。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。"><span class="nav-number">6.5.0.4.</span> <span class="nav-text">计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对应每一层的内容损失函数："><span class="nav-number">6.5.0.5.</span> <span class="nav-text">对应每一层的内容损失函数：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长-宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。"><span class="nav-number">6.5.0.6.</span> <span class="nav-text">其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#将风格图片输入网络，计算风格图片在网络指定层上的输出值。"><span class="nav-number">6.5.0.7.</span> <span class="nav-text">将风格图片输入网络，计算风格图片在网络指定层上的输出值。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。"><span class="nav-number">6.5.0.8.</span> <span class="nav-text">计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对于每一层的风格损失函数："><span class="nav-number">6.5.0.9.</span> <span class="nav-text">对于每一层的风格损失函数：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其中M是特征矩阵的长-宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。"><span class="nav-number">6.5.0.10.</span> <span class="nav-text">其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最终的风格损失为，每一层的风格损失加权和，再对层数取平均。"><span class="nav-number">6.5.0.11.</span> <span class="nav-text">最终的风格损失为，每一层的风格损失加权和，再对层数取平均。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#函数为内容损失和风格损失的加权和："><span class="nav-number">6.5.0.12.</span> <span class="nav-text">函数为内容损失和风格损失的加权和：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。"><span class="nav-number">6.5.0.13.</span> <span class="nav-text">当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！"><span class="nav-number">6.6.</span> <span class="nav-text">原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！</span></a></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2020
  </span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Michaelming
  </span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io" target="_blank" rel="noopener">Hexo</a>
</div>

<div class="theme-info">
  Theme by <a class="theme-link" href="https://idhyt.github.io" target="_blank" rel="noopener">idhyt</a>.<a class="theme-link" href="https://github.com/idhyt/hexo-theme-next/tree/magiclamp" target="_blank" rel="noopener">Mala</a>
</div>

<!-- busuanzi -->



 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
