<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Michaelming&#39;s Blog</title>
  
  <subtitle>不见你如我一般</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ailous.top/"/>
  <updated>2020-04-09T04:28:00.536Z</updated>
  <id>http://ailous.top/</id>
  
  <author>
    <name>Michaelming</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>卷积神经网络：（三）风格迁移——代码部分 (1)</title>
    <link href="http://ailous.top/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%89%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86%20(1)/"/>
    <id>http://ailous.top/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%89%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86%20(1)/</id>
    <published>2020-04-09T04:27:59.908Z</published>
    <updated>2020-04-09T04:28:00.536Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络：（三）风格迁移——代码部分"><a href="#卷积神经网络：（三）风格迁移——代码部分" class="headerlink" title="卷积神经网络：（三）风格迁移——代码部分"></a>卷积神经网络：（三）风格迁移——代码部分</h1><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h4 id="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。"><a href="#本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。" class="headerlink" title="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。"></a>本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。</h4><h5 id="友情提示：风格迁移跑的时间会很长。有点耐心哦。"><a href="#友情提示：风格迁移跑的时间会很长。有点耐心哦。" class="headerlink" title="友情提示：风格迁移跑的时间会很长。有点耐心哦。"></a>友情提示：风格迁移跑的时间会很长。有点耐心哦。</h5><h5 id="若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，"><a href="#若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，" class="headerlink" title="若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，"></a>若想查看环境配置步骤，请点击<a href="https://blog.csdn.net/weixin_41108515/article/details/103636284" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103636284</a>，</h5><h5 id="想知道原理，请点击https-blog-csdn-net-weixin-41108515-article-details-103650964"><a href="#想知道原理，请点击https-blog-csdn-net-weixin-41108515-article-details-103650964" class="headerlink" title="想知道原理，请点击https://blog.csdn.net/weixin_41108515/article/details/103650964"></a>想知道原理，请点击<a href="https://blog.csdn.net/weixin_41108515/article/details/103650964" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103650964</a></h5><p>&nbsp;<br>转载请注明出处：<a href="https://blog.csdn.net/weixin_41108515/article/details/103651784" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103651784</a><br>&nbsp;<br>这里引用的是：<br><a href="https://blog.csdn.net/aaronjny/article/details/79681080" target="_blank" rel="noopener">https://blog.csdn.net/aaronjny/article/details/79681080</a><br><a href="http://zh.gluon.ai/chapter_computer-vision/neural-style.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_computer-vision/neural-style.html</a><br>这两篇都非常详细，并且经调试可以使用，但是第二个并未使用tensorflow。<br>以及理论帮助的一篇<a href="https://juejin.im/post/5d29e818e51d454f73356de0" target="_blank" rel="noopener">https://juejin.im/post/5d29e818e51d454f73356de0</a></p><h1 id="第一部分-：简介"><a href="#第一部分-：简介" class="headerlink" title="第一部分 ：简介"></a>第一部分 ：简介</h1><h4 id="主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。"><a href="#主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。" class="headerlink" title="主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。"></a>主要操作以这部分为主，这篇引用的是<a href="https://blog.csdn.net/aaronjny/article/details/79681080" target="_blank" rel="noopener">tensorflow练手项目三</a>。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。</h4><h4 id="所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画"><a href="#所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画" class="headerlink" title="所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画"></a>所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画<img src="https://img-blog.csdnimg.cn/20191222152716368.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting-jpg），数据集将存放在百度网盘。"><a href="#风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting-jpg），数据集将存放在百度网盘。" class="headerlink" title="风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting.jpg），数据集将存放在百度网盘。"></a>风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting.jpg），数据集将存放在<a href="https://pan.baidu.com/s/1lTWsIRpMCxkopcsqUfdMFw" target="_blank" rel="noopener">百度网盘</a>。</h4><h4 id="content文件选取的是qd-jpg，在content文件夹下。"><a href="#content文件选取的是qd-jpg，在content文件夹下。" class="headerlink" title="content文件选取的是qd.jpg，在content文件夹下。"></a>content文件选取的是qd.jpg，在content文件夹下。</h4><p><img src="https://img-blog.csdnimg.cn/20191222152726511.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>最终实现效果如下。</p><p><img src="https://img-blog.csdnimg.cn/20191222152856860.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="第二部分-：操作"><a href="#第二部分-：操作" class="headerlink" title="第二部分 ：操作"></a>第二部分 ：操作</h1><h2 id="1-获取模型"><a href="#1-获取模型" class="headerlink" title="1.获取模型"></a>1.获取模型</h2><h4 id="VGG是Visual-Geometry-Group-这个实验室发明的，VGG是在2014年的-ILSVRC-localization-and-classification-两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max-pooling。"><a href="#VGG是Visual-Geometry-Group-这个实验室发明的，VGG是在2014年的-ILSVRC-localization-and-classification-两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max-pooling。" class="headerlink" title="VGG是Visual Geometry Group 这个实验室发明的，VGG是在2014年的 ILSVRC localization and classification 两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。"></a>VGG是Visual Geometry Group 这个实验室发明的，VGG是在2014年的 ILSVRC localization and classification 两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。</h4><h4 id="选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。"><a href="#选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。" class="headerlink" title="选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。"></a>选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。</h4><h4 id="CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。"><a href="#CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。" class="headerlink" title="CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。"></a>CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。</h4><p>注： 预训练好的VGG19模型可以从<a href="http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat" target="_blank" rel="noopener">http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat</a>下载，下载较慢的话，网盘<a href="https://pan.baidu.com/s/1uFinsEArbrgYRc2FWY9zVw" target="_blank" rel="noopener">https://pan.baidu.com/s/1uFinsEArbrgYRc2FWY9zVw</a>：。</p><h2 id="2-模型修改"><a href="#2-模型修改" class="headerlink" title="2.模型修改"></a>2.模型修改</h2><h4 id="这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。"><a href="#这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。" class="headerlink" title="这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。"></a>这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。</h4><h4 id="现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。"><a href="#现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。" class="headerlink" title="现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。"></a>现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。</h4><h4 id="风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。"><a href="#风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。" class="headerlink" title="风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。"></a>风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。</h4><p>理论上简单理解了，开始操作。</p><h4 id="这里建立py文件-models-py，下面内容我会写在注释里。"><a href="#这里建立py文件-models-py，下面内容我会写在注释里。" class="headerlink" title="这里建立py文件 models.py，下面内容我会写在注释里。"></a>这里建立py文件 models.py，下面内容我会写在注释里。</h4><pre><code># 导入必须的包import tensorflow as tfimport numpy as npimport settingsimport scipy.ioimport scipy.miscclass Model(object):    def __init__(self, content_path, style_path):        self.content = self.loadimg(content_path)  # 加载内容图片        self.style = self.loadimg(style_path)  # 加载风格图片        self.random_img = self.get_random_img()  # 生成噪音内容图片        self.net = self.vggnet()  # 建立vgg网络    def vggnet(self):            # 读取预训练的vgg模型            # 这里装的是misc，安装opencv的也亦可以使用opencv等其他方法        vgg = scipy.io.loadmat(settings.VGG_MODEL_PATH)         vgg_layers = vgg[&apos;layers&apos;][0]        net = {}            # 使用预训练的模型参数构建vgg网络的卷积层和池化层        # 全连接层不需要        # 注意，除了input之外，这里参数都为常量，不训练vgg的参数（权重比），这个以及训练完不需调整。        # 需要进行训练的是input，它即是我们最终生成的图像        net[&apos;input&apos;] = tf.Variable(np.zeros([1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3]), dtype=tf.float32)        # 参数对应的层数可以参考vgg模型图        net[&apos;conv1_1&apos;] = self.conv_relu(net[&apos;input&apos;], self.get_wb(vgg_layers, 0))        net[&apos;conv1_2&apos;] = self.conv_relu(net[&apos;conv1_1&apos;], self.get_wb(vgg_layers, 2))        net[&apos;pool1&apos;] = self.pool(net[&apos;conv1_2&apos;])        net[&apos;conv2_1&apos;] = self.conv_relu(net[&apos;pool1&apos;], self.get_wb(vgg_layers, 5))        net[&apos;conv2_2&apos;] = self.conv_relu(net[&apos;conv2_1&apos;], self.get_wb(vgg_layers, 7))        net[&apos;pool2&apos;] = self.pool(net[&apos;conv2_2&apos;])        net[&apos;conv3_1&apos;] = self.conv_relu(net[&apos;pool2&apos;], self.get_wb(vgg_layers, 10))        net[&apos;conv3_2&apos;] = self.conv_relu(net[&apos;conv3_1&apos;], self.get_wb(vgg_layers, 12))        net[&apos;conv3_3&apos;] = self.conv_relu(net[&apos;conv3_2&apos;], self.get_wb(vgg_layers, 14))        net[&apos;conv3_4&apos;] = self.conv_relu(net[&apos;conv3_3&apos;], self.get_wb(vgg_layers, 16))        net[&apos;pool3&apos;] = self.pool(net[&apos;conv3_4&apos;])        net[&apos;conv4_1&apos;] = self.conv_relu(net[&apos;pool3&apos;], self.get_wb(vgg_layers, 19))           net[&apos;conv4_2&apos;] = self.conv_relu(net[&apos;conv4_1&apos;], self.get_wb(vgg_layers, 21))        net[&apos;conv4_3&apos;] = self.conv_relu(net[&apos;conv4_2&apos;], self.get_wb(vgg_layers, 23))        net[&apos;conv4_4&apos;] = self.conv_relu(net[&apos;conv4_3&apos;], self.get_wb(vgg_layers, 25))        net[&apos;pool4&apos;] = self.pool(net[&apos;conv4_4&apos;])        net[&apos;conv5_1&apos;] = self.conv_relu(net[&apos;pool4&apos;], self.get_wb(vgg_layers, 28))        net[&apos;conv5_2&apos;] = self.conv_relu(net[&apos;conv5_1&apos;], self.get_wb(vgg_layers, 30))        net[&apos;conv5_3&apos;] = self.conv_relu(net[&apos;conv5_2&apos;], self.get_wb(vgg_layers, 32))        net[&apos;conv5_4&apos;] = self.conv_relu(net[&apos;conv5_3&apos;], self.get_wb(vgg_layers, 34))        net[&apos;pool5&apos;] = self.pool(net[&apos;conv5_4&apos;])        return net    def conv_relu(self, input, wb):        &quot;&quot;&quot;        进行先卷积、后relu的运算        :param input: 输入层        :param wb: wb[0],wb[1] == w,b        :return: relu后的结果        &quot;&quot;&quot;        conv = tf.nn.conv2d(input, wb[0], strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)        relu = tf.nn.relu(conv + wb[1])        return relu    def pool(self, input):        &quot;&quot;&quot;        进行max_pool操作        :param input: 输入层        :return: 池化后的结果        &quot;&quot;&quot;        return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)    def get_wb(self, layers, i):        &quot;&quot;&quot;        从预训练好的vgg模型中读取参数        :param layers: 训练好的vgg模型        :param i: vgg指定层数        :return: 该层的w,b        &quot;&quot;&quot;        w = tf.constant(layers[i][0][0][0][0][0])        bias = layers[i][0][0][0][0][1]        b = tf.constant(np.reshape(bias, (bias.size)))        return w, b    def get_random_img(self):        &quot;&quot;&quot;        根据噪音和内容图片，生成一张随机图片        :return:        &quot;&quot;&quot;        noise_image = np.random.uniform(-20, 20, [1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3])        random_img = noise_image * settings.NOISE + self.content * (1 - settings.NOISE)        return random_img    def loadimg(self, path):        &quot;&quot;&quot;        加载一张图片，将其转化为符合要求的格式        :param path:        :return:        &quot;&quot;&quot;        # 读取图片        image = scipy.misc.imread(path)        # 重新设定图片大小        image = scipy.misc.imresize(image, [settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH])        # 改变数组形状，其实就是把它变成一个batch_size=1的batch        image = np.reshape(image, (1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3))        # 减去均值，使其数据分布接近0        image = image - settings.IMAGE_MEAN_VALUE        return imageif __name__ == &apos;__main__&apos;:    Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE)</code></pre><h2 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3.模型训练"></a>3.模型训练</h2><h4 id="但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。"><a href="#但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。" class="headerlink" title="但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。"></a>但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。</h4><h4 id="就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。"><a href="#就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。" class="headerlink" title="就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。"></a>就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。</h4><h4 id="这里建立py文件-train-py，下面内容我会写在注释里。"><a href="#这里建立py文件-train-py，下面内容我会写在注释里。" class="headerlink" title="这里建立py文件 train.py，下面内容我会写在注释里。"></a>这里建立py文件 train.py，下面内容我会写在注释里。</h4><pre><code># -*- coding: utf-8 -*-import tensorflow as tfimport settingsimport modelsimport numpy as npimport scipy.miscdef loss(sess, model):    &quot;&quot;&quot;    定义模型的损失函数    :param sess: tf session    :param model: 神经网络模型    :return: 内容损失和风格损失的加权和损失    &quot;&quot;&quot;    # 先计算内容损失函数    # 获取定义内容损失的vgg层名称列表及权重    content_layers = settings.CONTENT_LOSS_LAYERS    # 将内容图片作为输入，方便后面提取内容图片在各层中的特征矩阵    sess.run(tf.assign(model.net[&apos;input&apos;], model.content))    # 内容损失累加量    content_loss = 0.0    # 逐个取出衡量内容损失的vgg层名称及对应权重    for layer_name, weight in content_layers:        # 提取内容图片在layer_name层中的特征矩阵        p = sess.run(model.net[layer_name])        # 提取噪音图片在layer_name层中的特征矩阵        x = model.net[layer_name]        # 长x宽        M = p.shape[1] * p.shape[2]        # 信道数        N = p.shape[3]        # 根据公式计算损失，并进行累加        content_loss += (1.0 / (2 * M * N)) * tf.reduce_sum(tf.pow(p - x, 2)) * weight    # 将损失对层数取平均    content_loss /= len(content_layers)    # 再计算风格损失函数    style_layers = settings.STYLE_LOSS_LAYERS    # 将风格图片作为输入，方便后面提取风格图片在各层中的特征矩阵    sess.run(tf.assign(model.net[&apos;input&apos;], model.style))    # 风格损失累加量    style_loss = 0.0    # 逐个取出衡量风格损失的vgg层名称及对应权重    for layer_name, weight in style_layers:        # 提取风格图片在layer_name层中的特征矩阵        a = sess.run(model.net[layer_name])        # 提取噪音图片在layer_name层中的特征矩阵        x = model.net[layer_name]        # 长x宽        M = a.shape[1] * a.shape[2]        # 信道数        N = a.shape[3]        # 求风格图片特征的gram矩阵        A = gram(a, M, N)        # 求噪音图片特征的gram矩阵        G = gram(x, M, N)        # 根据公式计算损失，并进行累加        style_loss += (1.0 / (4 * M * M * N * N)) * tf.reduce_sum(tf.pow(G - A, 2)) * weight    # 将损失对层数取平均    style_loss /= len(style_layers)    # 将内容损失和风格损失加权求和，构成总损失函数    loss = settings.ALPHA * content_loss + settings.BETA * style_loss    return lossdef gram(x, size, deep):    &quot;&quot;&quot;    创建给定矩阵的格莱姆矩阵，用来衡量风格    :param x:给定矩阵    :param size:矩阵的行数与列数的乘积    :param deep:矩阵信道数    :return:格莱姆矩阵    &quot;&quot;&quot;    # 改变shape为（size,deep）    x = tf.reshape(x, (size, deep))    # 求xTx    g = tf.matmul(tf.transpose(x), x)    return gdef train():    # 创建一个模型    model = models.Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE)    # 创建session    with tf.Session() as sess:        # 全局初始化        sess.run(tf.global_variables_initializer())        # 定义损失函数        cost = loss(sess, model)        # 创建优化器        optimizer = tf.train.AdamOptimizer(1.0).minimize(cost)        # 再初始化一次（主要针对于第一次初始化后又定义的运算，不然可能会报错）        sess.run(tf.global_variables_initializer())        # 使用噪声图片进行训练        sess.run(tf.assign(model.net[&apos;input&apos;], model.random_img))        # 迭代指定次数        for step in range(settings.TRAIN_STEPS):            # 进行一次反向传播            sess.run(optimizer)            # 每隔一定次数，输出一下进度，并保存当前训练结果            if step % 50 == 0:                print(&apos;step {} is down.&apos;.format(step))                # 取出input的内容，这是生成的图片                img = sess.run(model.net[&apos;input&apos;])                # 训练过程是减去均值的，这里要加上                img += settings.IMAGE_MEAN_VALUE                # 这里是一个batch_size=1的batch，所以img[0]才是图片内容                img = img[0]                # 将像素值限定在0-255，并转为整型                img = np.clip(img, 0, 255).astype(np.uint8)                # 保存图片                scipy.misc.imsave(&apos;{}-{}.jpg&apos;.format(settings.OUTPUT_IMAGE,step), img)        # 保存最终训练结果        img = sess.run(model.net[&apos;input&apos;])        img += settings.IMAGE_MEAN_VALUE        img = img[0]        img = np.clip(img, 0, 255).astype(np.uint8)        scipy.misc.imsave(&apos;{}.jpg&apos;.format(settings.OUTPUT_IMAGE), img)if __name__ == &apos;__main__&apos;:    train()</code></pre><h2 id="4-系统文件配置这里建立py文件-setting-py。"><a href="#4-系统文件配置这里建立py文件-setting-py。" class="headerlink" title="4.系统文件配置这里建立py文件 setting.py。"></a>4.系统文件配置这里建立py文件 setting.py。</h2><pre><code># -*- coding: utf-8 -*-# 内容图片路径CONTENT_IMAGE = &apos;content/qd.jpg&apos; # 路径/图片 自己在工程文件夹下建立。# 风格图片路径STYLE_IMAGE = &apos;style/painting.jpg&apos; # 路径/图片 自己在工程文件夹下建立。# 输出图片路径OUTPUT_IMAGE = &apos;output/output&apos; # 路径/图片开始名 自己在工程文件夹下建立。# 预训练的vgg模型路径VGG_MODEL_PATH = &apos;imagenet-vgg-verydeep-19.mat&apos; # 直接置于工程文件夹即可。# 图片宽度IMAGE_WIDTH = 450# 图片高度IMAGE_HEIGHT = 300# 定义计算内容损失的vgg层名称及对应权重的列表CONTENT_LOSS_LAYERS = [(&apos;conv4_2&apos;, 0.5),(&apos;conv5_2&apos;,0.5)]# 定义计算风格损失的vgg层名称及对应权重的列表STYLE_LOSS_LAYERS = [(&apos;conv1_1&apos;, 0.2), (&apos;conv2_1&apos;, 0.2), (&apos;conv3_1&apos;, 0.2), (&apos;conv4_1&apos;, 0.2), (&apos;conv5_1&apos;, 0.2)]# 噪音比率NOISE = 0.5# 图片RGB均值IMAGE_MEAN_VALUE = [128.0, 128.0, 128.0]# 内容损失权重ALPHA = 1# 风格损失权重BETA = 500# 训练次数TRAIN_STEPS = 3000  # 这里推荐几百次就行，确实时间太长。</code></pre><h2 id="5-总结："><a href="#5-总结：" class="headerlink" title="5.总结："></a>5.总结：</h2><p>这是第一次写博客，有不正确的地方还望指点，做出来的效果还是一般。之前做过网页版，但是因为跑的时间太长，效果不好。后期有时间出一篇网页版的风格迁移。源码数据 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络：（三）风格迁移——代码部分&quot;&gt;&lt;a href=&quot;#卷积神经网络：（三）风格迁移——代码部分&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络：（三）风格迁移——代码部分&quot;&gt;&lt;/a&gt;卷积神经网络：（三）风格迁移——代码部分&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>卷积神经网络：（二）风格迁移——原理部分</title>
    <link href="http://ailous.top/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%BA%8C%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/"/>
    <id>http://ailous.top/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%BA%8C%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/</id>
    <published>2020-04-09T04:27:42.211Z</published>
    <updated>2020-04-09T04:27:44.425Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络：（二）风格迁移——原理部分"><a href="#卷积神经网络：（二）风格迁移——原理部分" class="headerlink" title="卷积神经网络：（二）风格迁移——原理部分"></a>卷积神经网络：（二）风格迁移——原理部分</h1><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h4 id="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。"><a href="#本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。" class="headerlink" title="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。"></a>本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击<a href="https://blog.csdn.net/weixin_41108515/article/details/103636284" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103636284</a>，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。</h4><p>&nbsp;<br>转载请注明出处：<a href="https://blog.csdn.net/weixin_41108515/article/details/103650964" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103650964</a><br>&nbsp;<br>这里引用的是：<br><a href="http://zh.gluon.ai/chapter_computer-vision/neural-style.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_computer-vision/neural-style.html</a><br><a href="https://blog.csdn.net/aaronjny/article/details/79681080" target="_blank" rel="noopener">https://blog.csdn.net/aaronjny/article/details/79681080</a><br>这两篇都非常详细，并且经调试可以使用。</p><h1 id="涉及到的相关原理："><a href="#涉及到的相关原理：" class="headerlink" title="涉及到的相关原理："></a>涉及到的相关原理：</h1><h1 id="1、神经网络部分原理："><a href="#1、神经网络部分原理：" class="headerlink" title="1、神经网络部分原理："></a>1、神经网络部分原理：</h1><h2 id="1-1-神经网络基础介绍"><a href="#1-1-神经网络基础介绍" class="headerlink" title="1.1 神经网络基础介绍"></a>1.1 神经网络基础介绍</h2><h4 id="神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。"><a href="#神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。" class="headerlink" title="神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。"></a>神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。</h4><h4 id="生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。"><a href="#生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。" class="headerlink" title="生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。"></a>生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。<img src="https://img-blog.csdnimg.cn/2019122210122665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。</h4><h4 id="神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面："><a href="#神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：" class="headerlink" title="神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面："></a>神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：</h4><h5 id="（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。"><a href="#（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。" class="headerlink" title="（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。"></a>（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。</h5><h5 id="（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。"><a href="#（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。" class="headerlink" title="（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。"></a>（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。</h5><h5 id="（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。"><a href="#（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。" class="headerlink" title="（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。"></a>（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。</h5><h5 id="（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。"><a href="#（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。" class="headerlink" title="（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。"></a>（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。</h5><h4 id="纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。"><a href="#纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。" class="headerlink" title="纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。"></a>纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。<img src="https://img-blog.csdnimg.cn/20191222101416373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h2 id="1-2-卷积神经网络基本结构"><a href="#1-2-卷积神经网络基本结构" class="headerlink" title="1.2 卷积神经网络基本结构"></a>1.2 卷积神经网络基本结构</h2><h4 id="卷积神经网络-Convolutional-Neural-Network，CNN-是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注-16-。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络-Convolutional-Neural-Networks简称CNN-的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。"><a href="#卷积神经网络-Convolutional-Neural-Network，CNN-是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注-16-。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络-Convolutional-Neural-Networks简称CNN-的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。" class="headerlink" title="卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。"></a>卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。</h4><h3 id="1-2-1-输入层"><a href="#1-2-1-输入层" class="headerlink" title="1.2.1 输入层"></a>1.2.1 输入层</h3><h4 id="卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。"><a href="#卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。" class="headerlink" title="卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。"></a>卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。</h4><h3 id="1-2-2-隐含层"><a href="#1-2-2-隐含层" class="headerlink" title="1.2.2 隐含层"></a>1.2.2 隐含层</h3><h4 id="1-卷积层"><a href="#1-卷积层" class="headerlink" title="1.卷积层"></a>1.卷积层</h4><h5 id="（1）卷积层"><a href="#（1）卷积层" class="headerlink" title="（1）卷积层"></a>（1）卷积层</h5><h4 id="利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度-即颜色的三原色，以RGB表示-。"><a href="#利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度-即颜色的三原色，以RGB表示-。" class="headerlink" title="利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。"></a>利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。<img src="https://img-blog.csdnimg.cn/20191222102217444.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。"><a href="#如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。" class="headerlink" title="如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。"></a>如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。</h4><p><img src="https://img-blog.csdnimg.cn/20191222102503552.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="（2）卷积层参数"><a href="#（2）卷积层参数" class="headerlink" title="（2）卷积层参数"></a>（2）卷积层参数</h5><h4 id="卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。"><a href="#卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。" class="headerlink" title="卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。"></a>卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。</h4><h4 id="卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。"><a href="#卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。" class="headerlink" title="卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。"></a>卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。</h4><h4 id="由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类："><a href="#由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：" class="headerlink" title="由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类："></a>由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：</h4><h5 id="有效填充（valid-padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为-L-f-s-1。"><a href="#有效填充（valid-padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为-L-f-s-1。" class="headerlink" title="有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。"></a>有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。</h5><h5 id="相同填充-半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。"><a href="#相同填充-半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。" class="headerlink" title="相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。"></a>相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。</h5><h5 id="全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L-f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。"><a href="#全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L-f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。" class="headerlink" title="全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。"></a>全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。</h5><h5 id="任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。"><a href="#任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。" class="headerlink" title="任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。"></a>任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。</h5><h5 id="（3）激励函数"><a href="#（3）激励函数" class="headerlink" title="（3）激励函数"></a>（3）激励函数</h5><h4 id="一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质："><a href="#一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：" class="headerlink" title="一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质："></a>一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：</h4><h5 id="1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。"><a href="#1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。" class="headerlink" title="1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。"></a>1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。</h5><h5 id="2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。"><a href="#2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。" class="headerlink" title="2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。"></a>2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。</h5><h5 id="3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。"><a href="#3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。" class="headerlink" title="3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。"></a>3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。</h5><h4 id="经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层-比较常见，后者ReLU常见于卷积层。"><a href="#经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层-比较常见，后者ReLU常见于卷积层。" class="headerlink" title="经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。"></a>经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。</h4><h4 id="2-池化层"><a href="#2-池化层" class="headerlink" title="2.池化层"></a>2.池化层</h4><h4 id="池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化："><a href="#池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：" class="headerlink" title="池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化："></a>池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：</h4><h5 id="（1）一般池化-General-Pooling"><a href="#（1）一般池化-General-Pooling" class="headerlink" title="（1）一般池化(General Pooling)"></a>（1）一般池化(General Pooling)</h5><h5 id="1）mean-pooling，即只要求邻域中特征点的平均值；"><a href="#1）mean-pooling，即只要求邻域中特征点的平均值；" class="headerlink" title="1）mean-pooling，即只要求邻域中特征点的平均值；"></a>1）mean-pooling，即只要求邻域中特征点的平均值；</h5><h5 id="2）max-pooling，即在邻域中提取最大特征点；"><a href="#2）max-pooling，即在邻域中提取最大特征点；" class="headerlink" title="2）max-pooling，即在邻域中提取最大特征点；"></a>2）max-pooling，即在邻域中提取最大特征点；</h5><h5 id="3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。"><a href="#3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。" class="headerlink" title="3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。"></a>3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。</h5><h4 id="特征提取的误差主要来自两个方面："><a href="#特征提取的误差主要来自两个方面：" class="headerlink" title="特征提取的误差主要来自两个方面："></a>特征提取的误差主要来自两个方面：</h4><h5 id="1）邻域大小受限造成的估计值方差增大；"><a href="#1）邻域大小受限造成的估计值方差增大；" class="headerlink" title="1）邻域大小受限造成的估计值方差增大；"></a>1）邻域大小受限造成的估计值方差增大；</h5><h5 id="2）卷积层参数误差导致估计均值的偏移。"><a href="#2）卷积层参数误差导致估计均值的偏移。" class="headerlink" title="2）卷积层参数误差导致估计均值的偏移。"></a>2）卷积层参数误差导致估计均值的偏移。</h5><h4 id="一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean"><a href="#一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean" class="headerlink" title="一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-"></a>一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-</h4><h4 id="pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，"><a href="#pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，" class="headerlink" title="pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，"></a>pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，</h4><p><img src="https://img-blog.csdnimg.cn/20191222102557759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="（2）空间金字塔池化-Spatial-pyramid-pooling"><a href="#（2）空间金字塔池化-Spatial-pyramid-pooling" class="headerlink" title="（2）空间金字塔池化(Spatial pyramid pooling)"></a>（2）空间金字塔池化(Spatial pyramid pooling)</h5><h4 id="一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。"><a href="#一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。" class="headerlink" title="一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。"></a>一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。</h4><h4 id="空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。"><a href="#空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。" class="headerlink" title="空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。"></a>空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。</h4><h4 id="3-全连接层"><a href="#3-全连接层" class="headerlink" title="3.全连接层"></a>3.全连接层</h4><h4 id="卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。"><a href="#卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。" class="headerlink" title="卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。"></a>卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。</h4><h3 id="1-2-3-输出层"><a href="#1-2-3-输出层" class="headerlink" title="1.2.3 输出层"></a>1.2.3 输出层</h3><h4 id="卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。"><a href="#卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。" class="headerlink" title="卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。"></a>卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。</h4><h2 id="1-3-卷积神经网络的卷积过程"><a href="#1-3-卷积神经网络的卷积过程" class="headerlink" title="1.3 卷积神经网络的卷积过程"></a>1.3 卷积神经网络的卷积过程</h2><h4 id="卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层-convolutional-layer-、池化层-pooling-layer-、全连接层-fully-connected-layer-。"><a href="#卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层-convolutional-layer-、池化层-pooling-layer-、全连接层-fully-connected-layer-。" class="headerlink" title="卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。"></a>卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。</h4><p><img src="https://img-blog.csdnimg.cn/20191222102628441.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="图中的卷积网络工作流程如下，-输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述："><a href="#图中的卷积网络工作流程如下，-输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：" class="headerlink" title="图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述："></a>图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：</h4><h4 id="第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5-的接受域。"><a href="#第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5-的接受域。" class="headerlink" title="第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。"></a>第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。</h4><h4 id="第二隐藏层实现子采样和局部平均，它同样由-6个特征图组成，但其每个特征图由14×14-个神经元组成。每个神经元具有2×2-的接受域。"><a href="#第二隐藏层实现子采样和局部平均，它同样由-6个特征图组成，但其每个特征图由14×14-个神经元组成。每个神经元具有2×2-的接受域。" class="headerlink" title="第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。"></a>第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。</h4><h4 id="第三隐藏层进行第二次卷积，它由-16个特征图组成，每个特征图由-10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。"><a href="#第三隐藏层进行第二次卷积，它由-16个特征图组成，每个特征图由-10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。" class="headerlink" title="第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。"></a>第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。</h4><h4 id="第四个隐藏层进行第二次子采样和局部平均计算。它由-16个特征图组成，但每个特征图由-5×5个神经元构成，它以与第一次采样相同的方式进行工作。"><a href="#第四个隐藏层进行第二次子采样和局部平均计算。它由-16个特征图组成，但每个特征图由-5×5个神经元构成，它以与第一次采样相同的方式进行工作。" class="headerlink" title="第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。"></a>第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。</h4><h4 id="第五个隐藏层实现了卷积的最后阶段，它由-120个神经元组成，每个神经元指定5×5-的接受域。"><a href="#第五个隐藏层实现了卷积的最后阶段，它由-120个神经元组成，每个神经元指定5×5-的接受域。" class="headerlink" title="第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。"></a>第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。</h4><h4 id="端部是个全连接层，得到输出向量。"><a href="#端部是个全连接层，得到输出向量。" class="headerlink" title="端部是个全连接层，得到输出向量。"></a>端部是个全连接层，得到输出向量。</h4><h4 id="卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加-17-。"><a href="#卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加-17-。" class="headerlink" title="卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。"></a>卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。</h4><h4 id="卷积层研究输入数据的特征。卷积层由卷积核-convolutional-kernel-组成，卷积核用来计算不同的特征图；激励函数-activation-function-给卷积神经网络引入了非线性，常用的有sigmid、tanh、-ReLU函数；池化层减少了卷积层输出的特征向量，改良结果-使结构不易过拟合-，典型应用有average-pooling-和-max-pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。"><a href="#卷积层研究输入数据的特征。卷积层由卷积核-convolutional-kernel-组成，卷积核用来计算不同的特征图；激励函数-activation-function-给卷积神经网络引入了非线性，常用的有sigmid、tanh、-ReLU函数；池化层减少了卷积层输出的特征向量，改良结果-使结构不易过拟合-，典型应用有average-pooling-和-max-pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。" class="headerlink" title="卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。"></a>卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。</h4><h1 id="2、迁移学习相关原理"><a href="#2、迁移学习相关原理" class="headerlink" title="2、迁移学习相关原理"></a>2、迁移学习相关原理</h1><h2 id="2-1-迁移学习"><a href="#2-1-迁移学习" class="headerlink" title="2.1 迁移学习"></a>2.1 迁移学习</h2><h4 id="在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception-V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。"><a href="#在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception-V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。" class="headerlink" title="在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。"></a>在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。</h4><h2 id="2-2TensorFlow"><a href="#2-2TensorFlow" class="headerlink" title="2.2TensorFlow"></a>2.2TensorFlow</h2><h4 id="TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor-张量-意味着N维数组，Flow-流-意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。"><a href="#TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor-张量-意味着N维数组，Flow-流-意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。" class="headerlink" title="TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。"></a>TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。</h4><h4 id="TensorFlow-表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU-GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。"><a href="#TensorFlow-表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU-GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。" class="headerlink" title="TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。"></a>TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。</h4><h4 id="TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。"><a href="#TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。" class="headerlink" title="TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。"></a>TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。</h4><h5 id="（1）支持多种硬件的平台"><a href="#（1）支持多种硬件的平台" class="headerlink" title="（1）支持多种硬件的平台"></a>（1）支持多种硬件的平台</h5><h4 id="例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。"><a href="#例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。" class="headerlink" title="例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。"></a>例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。</h4><h5 id="（2）支持多种开发环境"><a href="#（2）支持多种开发环境" class="headerlink" title="（2）支持多种开发环境"></a>（2）支持多种开发环境</h5><h4 id="支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。"><a href="#支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。" class="headerlink" title="支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。"></a>支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。</h4><h4 id="目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。"><a href="#目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。" class="headerlink" title="目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。"></a>目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。</h4><h2 id="2-3VGG卷积神经网络模型"><a href="#2-3VGG卷积神经网络模型" class="headerlink" title="2.3VGG卷积神经网络模型"></a>2.3VGG卷积神经网络模型</h2><h4 id="VGG全称是Visual-Geometry-Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19-20-。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为-GG-Very-Deep-16-CNN-，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层-全连接层总数目的不同可以从VGG11-～-VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层-3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11-～GVV19的结构图："><a href="#VGG全称是Visual-Geometry-Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19-20-。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为-GG-Very-Deep-16-CNN-，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层-全连接层总数目的不同可以从VGG11-～-VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层-3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11-～GVV19的结构图：" class="headerlink" title="VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图："></a>VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图：</h4><p><img src="https://img-blog.csdnimg.cn/20191222102708425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3-stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。"><a href="#在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3-stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。" class="headerlink" title="在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。"></a>在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层3<em>3卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层1</em>1的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层3<em>3的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加3</em>3的卷积层，一共19层。模型E就是VGG19网络。</h4><h1 id="3、通过VGG实现风格迁移"><a href="#3、通过VGG实现风格迁移" class="headerlink" title="3、通过VGG实现风格迁移"></a>3、通过VGG实现风格迁移</h1><h2 id="3-1-图像风格迁移的原理"><a href="#3-1-图像风格迁移的原理" class="headerlink" title="3.1 图像风格迁移的原理"></a>3.1 图像风格迁移的原理</h2><h4 id="VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1-1-conv1-2-），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5-1-conv5-2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。"><a href="#VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1-1-conv1-2-），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5-1-conv5-2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。" class="headerlink" title="VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。"></a>VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。</h4><h4 id="VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示："><a href="#VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：" class="headerlink" title="VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示："></a>VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：</h4><p><img src="https://img-blog.csdnimg.cn/20191222104202202.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像-研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。"><a href="#具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像-研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。" class="headerlink" title="具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。"></a>具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。</h4><h2 id="3-2-代价函数"><a href="#3-2-代价函数" class="headerlink" title="3.2 代价函数"></a>3.2 代价函数</h2><h4 id="要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J-G-，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。"><a href="#要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J-G-，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。" class="headerlink" title="要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。"></a>要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。</h4><h4 id="Jcontent-C-G"><a href="#Jcontent-C-G" class="headerlink" title="Jcontent(C,G)"></a>Jcontent(C,G)</h4><h4 id="第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。"><a href="#第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。" class="headerlink" title="第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。"></a>第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。</h4><h4 id="Jstyle-S-G"><a href="#Jstyle-S-G" class="headerlink" title="Jstyle(S,G)"></a>Jstyle(S,G)</h4><h4 id="然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。"><a href="#然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。" class="headerlink" title="然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。"></a>然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。</h4><h4 id="J-G-αJcontent-C-G-βJstyle-S-G"><a href="#J-G-αJcontent-C-G-βJstyle-S-G" class="headerlink" title="J(G)=αJcontent(C,G)+βJstyle(S,G)"></a>J(G)=αJcontent(C,G)+βJstyle(S,G)</h4><h4 id="最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。"><a href="#最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。" class="headerlink" title="最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。"></a>最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。</h4><h2 id="3-3-内容代价函数"><a href="#3-3-内容代价函数" class="headerlink" title="3.3 内容代价函数"></a>3.3 内容代价函数</h2><h4 id="风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。"><a href="#风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。" class="headerlink" title="风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。"></a>风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。</h4><h4 id="首先定义内容代价部分。"><a href="#首先定义内容代价部分。" class="headerlink" title="首先定义内容代价部分。"></a>首先定义内容代价部分。</h4><h4 id="用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层-1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是-VGG-网络。"><a href="#用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层-1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是-VGG-网络。" class="headerlink" title="用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。"></a>用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。</h4><h4 id="之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α-L-C-和α-L-G-的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。"><a href="#之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α-L-C-和α-L-G-的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。" class="headerlink" title="之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。"></a>之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。</h4><p> <img src="https://img-blog.csdnimg.cn/20191222110732731.png" alt="在这里插入图片描述"></p><h4 id="为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1-2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。"><a href="#为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1-2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。" class="headerlink" title="为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。"></a>为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。</h4><h2 id="3-4-风格代价函数"><a href="#3-4-风格代价函数" class="headerlink" title="3.4 风格代价函数"></a>3.4 风格代价函数</h2><h4 id="图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。"><a href="#图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。" class="headerlink" title="图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。"></a>图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。</h4><h4 id="风格矩阵是一组向量的内积对称矩阵，比如向量组"><a href="#风格矩阵是一组向量的内积对称矩阵，比如向量组" class="headerlink" title="风格矩阵是一组向量的内积对称矩阵，比如向量组"></a>风格矩阵是一组向量的内积对称矩阵，比如向量组<img src="https://img-blog.csdnimg.cn/20191222110804515.png" alt="在这里插入图片描述"></h4><h4 id="的Gram矩阵是"><a href="#的Gram矩阵是" class="headerlink" title="的Gram矩阵是"></a>的Gram矩阵是</h4><p><img src="https://img-blog.csdnimg.cn/20191222111053849.png" alt="在这里插入图片描述"></p><h4 id="取内积即欧几里得空间上的标准内积，即"><a href="#取内积即欧几里得空间上的标准内积，即" class="headerlink" title="取内积即欧几里得空间上的标准内积，即"></a>取内积即欧几里得空间上的标准内积，即</h4><p> <img src="https://img-blog.csdnimg.cn/20191222111102313.png" alt="在这里插入图片描述"></p><h4 id="假设卷积层的输出为F-ij-l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为"><a href="#假设卷积层的输出为F-ij-l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为" class="headerlink" title="假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为"></a>假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为</h4><p><img src="https://img-blog.csdnimg.cn/20191222111118616.png" alt="在这里插入图片描述"></p><h4 id="设在第l层中，卷积特征的通道数为N-l-卷积的高、宽乘积数为M-l-那么F-ij-l满足"><a href="#设在第l层中，卷积特征的通道数为N-l-卷积的高、宽乘积数为M-l-那么F-ij-l满足" class="headerlink" title="设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足"></a>设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足</h4><h4 id="l≤i≤N-l，l≤j≤M-l"><a href="#l≤i≤N-l，l≤j≤M-l" class="headerlink" title="l≤i≤N_l，l≤j≤M_l"></a>l≤i≤N_l，l≤j≤M_l</h4><h4 id="Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。"><a href="#Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。" class="headerlink" title="Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。"></a>Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。</h4><h2 id="3-5-模型训练过程"><a href="#3-5-模型训练过程" class="headerlink" title="3.5 模型训练过程"></a>3.5 模型训练过程</h2><h4 id="首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。"><a href="#首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。" class="headerlink" title="首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。"></a>首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。</h4><h4 id="使用-‘conv4-2’-’conv5-2’-表示内容特征，使用-‘conv1-1’-’conv2-1’-’conv3-1’-’conv4-1’-表示风格特征。"><a href="#使用-‘conv4-2’-’conv5-2’-表示内容特征，使用-‘conv1-1’-’conv2-1’-’conv3-1’-’conv4-1’-表示风格特征。" class="headerlink" title="使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。"></a>使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。</h4><h4 id="将内容图片输入网络，计算内容图片在网络指定层上的输出值。"><a href="#将内容图片输入网络，计算内容图片在网络指定层上的输出值。" class="headerlink" title="将内容图片输入网络，计算内容图片在网络指定层上的输出值。"></a>将内容图片输入网络，计算内容图片在网络指定层上的输出值。</h4><h4 id="计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。"><a href="#计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。" class="headerlink" title="计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。"></a>计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。</h4><h4 id="对应每一层的内容损失函数："><a href="#对应每一层的内容损失函数：" class="headerlink" title="对应每一层的内容损失函数："></a>对应每一层的内容损失函数：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111244537.png" alt="在这里插入图片描述"></p><h4 id="其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长-宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。"><a href="#其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长-宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。" class="headerlink" title="其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。"></a>其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。</h4><h4 id="将风格图片输入网络，计算风格图片在网络指定层上的输出值。"><a href="#将风格图片输入网络，计算风格图片在网络指定层上的输出值。" class="headerlink" title="将风格图片输入网络，计算风格图片在网络指定层上的输出值。"></a>将风格图片输入网络，计算风格图片在网络指定层上的输出值。</h4><h4 id="计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。"><a href="#计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。" class="headerlink" title="计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。"></a>计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。</h4><h4 id="对于每一层的风格损失函数："><a href="#对于每一层的风格损失函数：" class="headerlink" title="对于每一层的风格损失函数："></a>对于每一层的风格损失函数：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111337543.png" alt="在这里插入图片描述"></p><h4 id="其中M是特征矩阵的长-宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。"><a href="#其中M是特征矩阵的长-宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。" class="headerlink" title="其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。"></a>其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。</h4><h4 id="最终的风格损失为，每一层的风格损失加权和，再对层数取平均。"><a href="#最终的风格损失为，每一层的风格损失加权和，再对层数取平均。" class="headerlink" title="最终的风格损失为，每一层的风格损失加权和，再对层数取平均。"></a>最终的风格损失为，每一层的风格损失加权和，再对层数取平均。</h4><h4 id="函数为内容损失和风格损失的加权和："><a href="#函数为内容损失和风格损失的加权和：" class="headerlink" title="函数为内容损失和风格损失的加权和："></a>函数为内容损失和风格损失的加权和：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111355281.png" alt="在这里插入图片描述"></p><h4 id="当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。"><a href="#当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。" class="headerlink" title="当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。"></a>当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。</h4><p><img src="https://img-blog.csdnimg.cn/20191222104239449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！"><a href="#原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！" class="headerlink" title="原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！"></a>原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！</h2><p><a href="https://blog.csdn.net/weixin_41108515/article/details/103651784" target="_blank" rel="noopener">卷积神经网络：（三）风格迁移——代码部分</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络：（二）风格迁移——原理部分&quot;&gt;&lt;a href=&quot;#卷积神经网络：（二）风格迁移——原理部分&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络：（二）风格迁移——原理部分&quot;&gt;&lt;/a&gt;卷积神经网络：（二）风格迁移——原理部分&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>卷积神经网络：（一）风格迁移——环境配置 (1)</title>
    <link href="http://ailous.top/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%80%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%20(1)/"/>
    <id>http://ailous.top/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%80%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%20(1)/</id>
    <published>2020-04-09T04:27:19.563Z</published>
    <updated>2020-04-09T04:27:20.070Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络：（一）风格迁移——环境配置"><a href="#卷积神经网络：（一）风格迁移——环境配置" class="headerlink" title="卷积神经网络：（一）风格迁移——环境配置"></a>卷积神经网络：（一）风格迁移——环境配置</h1><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h4 id="emsp-emsp-本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。"><a href="#emsp-emsp-本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。" class="headerlink" title="&emsp;&emsp;本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。"></a>&emsp;&emsp;本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。</h4><p>&nbsp;<br>转载请注明出处：<a href="https://blog.csdn.net/weixin_41108515/article/details/103636284" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103636284</a><br>&nbsp;</p><h1 id="第一步：搭建python环境："><a href="#第一步：搭建python环境：" class="headerlink" title="第一步：搭建python环境："></a>第一步：搭建python环境：</h1><h2 id="方法一-直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）"><a href="#方法一-直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）" class="headerlink" title="方法一 : 直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）"></a>方法一 : 直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）</h2><h4 id="emsp-emsp-直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3-6版本，3-7及以上版本目前不支持）"><a href="#emsp-emsp-直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3-6版本，3-7及以上版本目前不支持）" class="headerlink" title="&emsp;&emsp;直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3.6版本，3.7及以上版本目前不支持）"></a>&emsp;&emsp;直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3.6版本，3.7及以上版本目前不支持）</h4><h4 id="下载地址：https-www-python-org-ftp-python-3-6-8-python-3-6-8-amd64-exe"><a href="#下载地址：https-www-python-org-ftp-python-3-6-8-python-3-6-8-amd64-exe" class="headerlink" title="下载地址：https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe"></a>下载地址：<a href="https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe" target="_blank" rel="noopener">https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe</a></h4><h4 id="emsp-emsp-如图示安装install-Now即可-注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。"><a href="#emsp-emsp-如图示安装install-Now即可-注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。" class="headerlink" title="&emsp;&emsp;如图示安装install Now即可,注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。"></a>&emsp;&emsp;如图示安装install Now即可,注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。<img src="https://img-blog.csdnimg.cn/20191220183312800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h6 id="注：默认路径C-users-用户名-Appdata-local-Programs-Python-Python36-也可以使用Customize-installation来更换路径。"><a href="#注：默认路径C-users-用户名-Appdata-local-Programs-Python-Python36-也可以使用Customize-installation来更换路径。" class="headerlink" title="注：默认路径C:\users\用户名\Appdata\local\Programs\Python\Python36 也可以使用Customize installation来更换路径。"></a>注：默认路径C:\users\用户名\Appdata\local\Programs\Python\Python36 也可以使用Customize installation来更换路径。</h6><h4 id="安装完成后，进入cmd界面，输入python，如图"><a href="#安装完成后，进入cmd界面，输入python，如图" class="headerlink" title="安装完成后，进入cmd界面，输入python，如图"></a>安装完成后，进入cmd界面，输入python，如图</h4><p><img src="https://img-blog.csdnimg.cn/20191220183344749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="出现此界面即可。"><a href="#出现此界面即可。" class="headerlink" title="出现此界面即可。"></a>出现此界面即可。</h4><p>&nbsp;</p><h2 id="方法二-：使用anaconda搭建环境"><a href="#方法二-：使用anaconda搭建环境" class="headerlink" title="方法二 ：使用anaconda搭建环境:"></a>方法二 ：使用anaconda搭建环境:</h2><h4 id="emsp-emsp-因为anaconda官方网站下下载较慢，这里推荐使用-清华镜像来下载。"><a href="#emsp-emsp-因为anaconda官方网站下下载较慢，这里推荐使用-清华镜像来下载。" class="headerlink" title="&emsp;&emsp;因为anaconda官方网站下下载较慢，这里推荐使用    清华镜像来下载。"></a>&emsp;&emsp;因为<a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">anaconda官方网站</a>下下载较慢，这里推荐使用    <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">清华镜像</a>来下载。</h4><h6 id="注：anaconda官方当前最新版为基于python3-7version，这个并不意味着搭建的python环境版本就固定为3-7，但是这里还是不推荐使用下面会说明原因。"><a href="#注：anaconda官方当前最新版为基于python3-7version，这个并不意味着搭建的python环境版本就固定为3-7，但是这里还是不推荐使用下面会说明原因。" class="headerlink" title="注：anaconda官方当前最新版为基于python3.7version，这个并不意味着搭建的python环境版本就固定为3.7，但是这里还是不推荐使用下面会说明原因。"></a>注：anaconda官方当前最新版为基于python3.7version，这个并不意味着搭建的python环境版本就固定为3.7，但是这里还是不推荐使用下面会说明原因。</h6><h4 id="清华镜像下载地址https-mirrors-tuna-tsinghua-edu-cn-anaconda-archive-Anaconda3-5-2-0-Windows-x86-64-exe"><a href="#清华镜像下载地址https-mirrors-tuna-tsinghua-edu-cn-anaconda-archive-Anaconda3-5-2-0-Windows-x86-64-exe" class="headerlink" title="清华镜像下载地址https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe"></a>清华镜像下载地址<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe</a></h4><h4 id="如图所示安装anaconda，这里先用的是官方3-7的包："><a href="#如图所示安装anaconda，这里先用的是官方3-7的包：" class="headerlink" title="如图所示安装anaconda，这里先用的是官方3.7的包："></a>如图所示安装anaconda，这里先用的是官方3.7的包：</h4><h6 id="注：存在问题：1、下载太慢。2、3-7—version需额外对环境更改，这里只对3-7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。"><a href="#注：存在问题：1、下载太慢。2、3-7—version需额外对环境更改，这里只对3-7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。" class="headerlink" title="注：存在问题：1、下载太慢。2、3.7—version需额外对环境更改，这里只对3.7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。"></a>注：存在问题：1、下载太慢。2、3.7—version需额外对环境更改，这里只对3.7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。</h6><h4 id="安装步骤："><a href="#安装步骤：" class="headerlink" title="安装步骤："></a>安装步骤：</h4><p>&nbsp;<img src="https://img-blog.csdnimg.cn/20191220222414658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20191220222510588.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。"><a href="#目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。" class="headerlink" title="目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。"></a>目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。</h4><p><img src="https://img-blog.csdnimg.cn/20191220222529582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="这里是指是否将anaconda里的python作为电脑默认python环境。"><a href="#这里是指是否将anaconda里的python作为电脑默认python环境。" class="headerlink" title="这里是指是否将anaconda里的python作为电脑默认python环境。"></a>这里是指是否将anaconda里的python作为电脑默认python环境。</h4><h4 id="emsp-emsp-安装完成后，选择Anaconda-Navigator-打开，接下来选择Environments-gt-create，-生成如下界面："><a href="#emsp-emsp-安装完成后，选择Anaconda-Navigator-打开，接下来选择Environments-gt-create，-生成如下界面：" class="headerlink" title="&emsp;&emsp;安装完成后，选择Anaconda Navigator 打开，接下来选择Environments -&gt;create，:生成如下界面："></a>&emsp;&emsp;安装完成后，选择Anaconda Navigator 打开，接下来选择Environments -&gt;create，:生成如下界面：</h4><p><img src="https://img-blog.csdnimg.cn/2019122022253567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="emsp-emsp-这里是3-7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装："><a href="#emsp-emsp-这里是3-7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装：" class="headerlink" title="&emsp;&emsp;这里是3.7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装："></a>&emsp;&emsp;这里是3.7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装：</h4><p><img src="https://img-blog.csdnimg.cn/2019122022312263.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="emsp-emsp-接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3-6。"><a href="#emsp-emsp-接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3-6。" class="headerlink" title="&emsp;&emsp;接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3.6。"></a>&emsp;&emsp;接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3.6。</h4><h2 id="第一步，总结"><a href="#第一步，总结" class="headerlink" title="第一步，总结"></a>第一步，总结</h2><h4 id="emsp-emsp-anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。"><a href="#emsp-emsp-anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。" class="headerlink" title="&emsp;&emsp;anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。"></a>&emsp;&emsp;anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。</h4><p>&nbsp;</p><h1 id="第二步，安装pycharm"><a href="#第二步，安装pycharm" class="headerlink" title="第二步，安装pycharm"></a>第二步，安装pycharm</h1><h4 id="这个在pycharm官网上下载即可-如图示安装即可。"><a href="#这个在pycharm官网上下载即可-如图示安装即可。" class="headerlink" title="这个在pycharm官网上下载即可,如图示安装即可。 "></a>这个在<a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">pycharm官网</a>上下载即可,如图示安装即可。 <img src="https://img-blog.csdnimg.cn/20191221095541168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="pycharm_1"></h4><p><img src="https://img-blog.csdnimg.cn/20191221095624451.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="pycharm_2">)<img src="https://img-blog.csdnimg.cn/2019122316264677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191221095701293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="pycharm_3"><br>安装完成。<br>&nbsp;</p><h1 id="第三步，安装必需包"><a href="#第三步，安装必需包" class="headerlink" title="第三步，安装必需包"></a>第三步，安装必需包</h1><h4 id="emsp-emsp-这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm-中的【ALT-SHIFT-ENTER】即可安装，如还有缺失请自行百度。"><a href="#emsp-emsp-这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm-中的【ALT-SHIFT-ENTER】即可安装，如还有缺失请自行百度。" class="headerlink" title="&emsp;&emsp;这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm 中的【ALT+SHIFT+ENTER】即可安装，如还有缺失请自行百度。"></a>&emsp;&emsp;这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm 中的【ALT+SHIFT+ENTER】即可安装，如还有缺失请自行百度。</h4><h2 id="方法一：直装python环境"><a href="#方法一：直装python环境" class="headerlink" title="方法一：直装python环境"></a>方法一：直装python环境</h2><h4 id="emsp-emsp-默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python："><a href="#emsp-emsp-默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python：" class="headerlink" title="&emsp;&emsp;默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python："></a>&emsp;&emsp;默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python：</h4><h4 id="若为下图：第二行中出现anaconda。"><a href="#若为下图：第二行中出现anaconda。" class="headerlink" title="若为下图：第二行中出现anaconda。"></a>若为下图：第二行中出现anaconda。<img src="https://img-blog.csdnimg.cn/20191221211939424.png" alt="在这里插入图片描述"></h4><h4 id="则需修改默认python环境。"><a href="#则需修改默认python环境。" class="headerlink" title="则需修改默认python环境。"></a>则需修改默认python环境。</h4><h4 id="若为下图，第二行中不是anaconda，则继续操作即可。"><a href="#若为下图，第二行中不是anaconda，则继续操作即可。" class="headerlink" title="若为下图，第二行中不是anaconda，则继续操作即可。"></a>若为下图，第二行中不是anaconda，则继续操作即可。<img src="https://img-blog.csdnimg.cn/20191221213513434.png" alt="在这里插入图片描述"></h4><h3 id="3-1-1-安装tensorflow包"><a href="#3-1-1-安装tensorflow包" class="headerlink" title="3.1.1 安装tensorflow包"></a>3.1.1 安装tensorflow包</h3><h4 id="emsp-emsp-tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https-github-com-fo40225-t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https-github-com-fo40225-tensorflow-windows-wheel-raw-master-1-4-0-py36-CPU-avx2-tensorflow-1-4-0-cp36-cp36m-win-amd64-whl"><a href="#emsp-emsp-tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https-github-com-fo40225-t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https-github-com-fo40225-tensorflow-windows-wheel-raw-master-1-4-0-py36-CPU-avx2-tensorflow-1-4-0-cp36-cp36m-win-amd64-whl" class="headerlink" title="&emsp;&emsp;tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https://github.com/fo40225/t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl"></a>&emsp;&emsp;tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：<a href="https://github.com/fo40225/tensorflow-windows-wheel" target="_blank" rel="noopener">https://github.com/fo40225/t与ensorflow-windows-wheel</a>上下载，对于不同版本的python环境有不同的whl文件可以下载。<a href="https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl" target="_blank" rel="noopener">https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl</a></h4><h4 id="emsp-emsp-下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。"><a href="#emsp-emsp-下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。" class="headerlink" title="&emsp;&emsp;下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。"></a>&emsp;&emsp;下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。<img src="https://img-blog.csdnimg.cn/20191221184310679.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="PowerShell"></h4><h4 id="输入-："><a href="#输入-：" class="headerlink" title="输入    ："></a>输入    ：</h4><pre><code>pip install .\tensorflow-1.4.0-cp36-cp36m-win_amd64.whl</code></pre><p><img src="https://img-blog.csdnimg.cn/20191221185322803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="运行完即可，若是提示pip版本有更新，更不更新都可。"><a href="#运行完即可，若是提示pip版本有更新，更不更新都可。" class="headerlink" title="运行完即可，若是提示pip版本有更新，更不更新都可。"></a>运行完即可，若是提示pip版本有更新，更不更新都可。</h5><pre><code>python -m pip install --upgrade pip</code></pre><h3 id="3-1-2-安装opencv包"><a href="#3-1-2-安装opencv包" class="headerlink" title="3.1.2 安装opencv包"></a>3.1.2 安装opencv包</h3><h4 id="opencv在代码中使用时：import-cv2"><a href="#opencv在代码中使用时：import-cv2" class="headerlink" title="opencv在代码中使用时：import cv2"></a>opencv在代码中使用时：import cv2</h4><h4 id="opencv较tensorflow简单，只需输入代码即可运行："><a href="#opencv较tensorflow简单，只需输入代码即可运行：" class="headerlink" title="opencv较tensorflow简单，只需输入代码即可运行："></a>opencv较tensorflow简单，只需输入代码即可运行：</h4><pre><code>pip install opencv-python</code></pre><p><img src="https://img-blog.csdnimg.cn/20191221191736729.png" alt="opencv"></p><h3 id="3-1-2-安装PIL（Python-Imaging-Library）包"><a href="#3-1-2-安装PIL（Python-Imaging-Library）包" class="headerlink" title="3.1.2 安装PIL（Python Imaging Library）包"></a>3.1.2 安装PIL（Python Imaging Library）包</h3><h4 id="PIL安装并不是通过直接键入pip-install-PIL-而是通过："><a href="#PIL安装并不是通过直接键入pip-install-PIL-而是通过：" class="headerlink" title="PIL安装并不是通过直接键入pip install PIL 而是通过："></a>PIL安装并不是通过直接键入pip install PIL 而是通过：</h4><pre><code>pip install pillow</code></pre><p><img src="https://img-blog.csdnimg.cn/20191221193010925.png" alt="在这里插入图片描述"></p><h3 id="方法一总结："><a href="#方法一总结：" class="headerlink" title="方法一总结："></a>方法一总结：</h3><h4 id="emsp-emsp-这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。"><a href="#emsp-emsp-这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。" class="headerlink" title="&emsp;&emsp;这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。"></a>&emsp;&emsp;这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。</h4><p>&nbsp;</p><h2 id="方法二：anaconda环境"><a href="#方法二：anaconda环境" class="headerlink" title="方法二：anaconda环境"></a>方法二：anaconda环境</h2><h4 id="emsp-emsp-这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。"><a href="#emsp-emsp-这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。" class="headerlink" title="&emsp;&emsp;这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。"></a>&emsp;&emsp;这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。<img src="https://img-blog.csdnimg.cn/20191221235217581.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="点击上方的选择框设置为all，然后进行搜索tensorflw"><a href="#点击上方的选择框设置为all，然后进行搜索tensorflw" class="headerlink" title="点击上方的选择框设置为all，然后进行搜索tensorflw"></a>点击上方的选择框设置为all，然后进行搜索tensorflw<img src="https://img-blog.csdnimg.cn/20191221235345841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="选择tensorflow，点击apply"><a href="#选择tensorflow，点击apply" class="headerlink" title="选择tensorflow，点击apply"></a>选择tensorflow，点击apply</h4><p><img src="https://img-blog.csdnimg.cn/20191222000202665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="emsp-emsp-会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。"><a href="#emsp-emsp-会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。" class="headerlink" title="&emsp;&emsp;会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。"></a>&emsp;&emsp;会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。</h4><p>&nbsp;</p><h1 id="第四步，建立风格迁移工程"><a href="#第四步，建立风格迁移工程" class="headerlink" title="第四步，建立风格迁移工程"></a>第四步，建立风格迁移工程</h1><h4 id="在pycharm中选择：file-gt-New-project"><a href="#在pycharm中选择：file-gt-New-project" class="headerlink" title="在pycharm中选择：file-&gt;New project "></a>在pycharm中选择：file-&gt;New project <img src="https://img-blog.csdnimg.cn/20191221210833416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="NewProject"></h4><h4 id="emsp-emsp-这里要注意的是Base-interpreter-，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。"><a href="#emsp-emsp-这里要注意的是Base-interpreter-，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。" class="headerlink" title="&emsp;&emsp;这里要注意的是Base interpreter ，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。"></a>&emsp;&emsp;这里要注意的是Base interpreter ，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。</h4><h2 id="方法一：在创建项目是直接选择anaconda下的python："><a href="#方法一：在创建项目是直接选择anaconda下的python：" class="headerlink" title="方法一：在创建项目是直接选择anaconda下的python："></a>方法一：在创建项目是直接选择anaconda下的python：</h2><p><img src="https://img-blog.csdnimg.cn/20191222004253634.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="方法二：在项目创建完成后再选择路径为：File-gt-Settings-gt-project：项目名，如图点击齿轮——Add。"><a href="#方法二：在项目创建完成后再选择路径为：File-gt-Settings-gt-project：项目名，如图点击齿轮——Add。" class="headerlink" title="方法二：在项目创建完成后再选择路径为：File-&gt;Settings-&gt;project：项目名，如图点击齿轮——Add。"></a>方法二：在项目创建完成后再选择路径为：File-&gt;Settings-&gt;project：项目名，如图点击齿轮——Add。</h2><p><img src="https://img-blog.csdnimg.cn/20191222003319372.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="在Base-interpreter下选择anaconda中python路径即可。"><a href="#在Base-interpreter下选择anaconda中python路径即可。" class="headerlink" title="在Base interpreter下选择anaconda中python路径即可。"></a>在Base interpreter下选择anaconda中python路径即可。</h4><p><img src="https://img-blog.csdnimg.cn/20191222003949287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="至此所有环境配置完毕。"><a href="#至此所有环境配置完毕。" class="headerlink" title="至此所有环境配置完毕。"></a>至此所有环境配置完毕。</h4><p>&nbsp;</p><h1 id="第五步，写风格迁移代码"><a href="#第五步，写风格迁移代码" class="headerlink" title="第五步，写风格迁移代码"></a>第五步，写风格迁移代码</h1><h4 id="emsp-emsp-这里主要参网址的为http-zh-gluon-ai-chapter-computer-vision-neural-style-html会在下一篇博客https-blog-csdn-net-weixin-41108515-article-details-103650964里详细解释过程。"><a href="#emsp-emsp-这里主要参网址的为http-zh-gluon-ai-chapter-computer-vision-neural-style-html会在下一篇博客https-blog-csdn-net-weixin-41108515-article-details-103650964里详细解释过程。" class="headerlink" title="&emsp;&emsp;这里主要参网址的为http://zh.gluon.ai/chapter_computer-vision/neural-style.html会在下一篇博客https://blog.csdn.net/weixin_41108515/article/details/103650964里详细解释过程。"></a>&emsp;&emsp;这里主要参网址的为<a href="http://zh.gluon.ai/chapter_computer-vision/neural-style.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_computer-vision/neural-style.html</a>会在下一篇博客<a href="https://blog.csdn.net/weixin_41108515/article/details/103650964" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103650964</a>里详细解释过程。</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络：（一）风格迁移——环境配置&quot;&gt;&lt;a href=&quot;#卷积神经网络：（一）风格迁移——环境配置&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络：（一）风格迁移——环境配置&quot;&gt;&lt;/a&gt;卷积神经网络：（一）风格迁移——环境配置&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://ailous.top/2020/03/23/hello-world/"/>
    <id>http://ailous.top/2020/03/23/hello-world/</id>
    <published>2020-03-23T04:33:07.371Z</published>
    <updated>2020-03-23T04:33:07.371Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
