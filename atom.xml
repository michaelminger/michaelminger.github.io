<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Michaelming&#39;s Blog</title>
  
  <subtitle>不见你如我一般</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ailous.top/"/>
  <updated>2020-05-10T02:33:38.354Z</updated>
  <id>http://ailous.top/</id>
  
  <author>
    <name>Michaelming</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>网络爬虫——古诗文网中验证码（超级鹰）</title>
    <link href="http://ailous.top/2020/05/10/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%8F%A4%E8%AF%97%E6%96%87%E7%BD%91%E4%B8%AD%E9%AA%8C%E8%AF%81%E7%A0%81%EF%BC%88%E8%B6%85%E7%BA%A7%E9%B9%B0%EF%BC%89/"/>
    <id>http://ailous.top/2020/05/10/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%8F%A4%E8%AF%97%E6%96%87%E7%BD%91%E4%B8%AD%E9%AA%8C%E8%AF%81%E7%A0%81%EF%BC%88%E8%B6%85%E7%BA%A7%E9%B9%B0%EF%BC%89/</id>
    <published>2020-05-10T02:33:38.166Z</published>
    <updated>2020-05-10T02:33:38.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——古诗文网中验证码（超级鹰）"><a href="#网络爬虫——古诗文网中验证码（超级鹰）" class="headerlink" title="网络爬虫——古诗文网中验证码（超级鹰）"></a>网络爬虫——古诗文网中验证码（超级鹰）</h2><h4 id="目标网址-古诗文网"><a href="#目标网址-古诗文网" class="headerlink" title="目标网址: 古诗文网"></a>目标网址: 古诗文网</h4><h6 id="目标网址：https-so-gushiwen-org-user-login-aspx-from-http-so-gushiwen-org-user-collect-aspx"><a href="#目标网址：https-so-gushiwen-org-user-login-aspx-from-http-so-gushiwen-org-user-collect-aspx" class="headerlink" title="目标网址：https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx"></a>目标网址：<a href="https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx" target="_blank" rel="noopener">https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx</a></h6><p><img src="https://img-blog.csdnimg.cn/20200510095336662.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h6 id="任务要求："><a href="#任务要求：" class="headerlink" title="任务要求："></a>任务要求：</h6><p>（1）通过selenium的方式模拟该网站的登录，并成功输入用户名和密码；</p><p>（2）保存验证码图片，并使用输入式验证码识别的方式识别验证码的文字，获取后输入到输入框中，</p><p>（3）验证登录是否成功。</p><h4 id="源码："><a href="#源码：" class="headerlink" title="源码："></a>源码：</h4><h6 id="超级鹰源码："><a href="#超级鹰源码：" class="headerlink" title="超级鹰源码："></a>超级鹰源码：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from hashlib import md5</span><br><span class="line"></span><br><span class="line">class Chaojiying_Client(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, username, password, soft_id):</span><br><span class="line">        self.username &#x3D; username</span><br><span class="line">        # todo:更改点一</span><br><span class="line">        self.password &#x3D; md5(password.encode(&quot;utf-8&quot;)).hexdigest()</span><br><span class="line">        self.soft_id &#x3D; soft_id</span><br><span class="line">        self.base_params &#x3D; &#123;</span><br><span class="line">            &#39;user&#39;: self.username,</span><br><span class="line">            &#39;pass2&#39;: self.password,</span><br><span class="line">            &#39;softid&#39;: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers &#x3D; &#123;</span><br><span class="line">            &#39;Connection&#39;: &#39;Keep-Alive&#39;,</span><br><span class="line">            &#39;User-Agent&#39;: &#39;Mozilla&#x2F;4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident&#x2F;4.0)&#39;,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    def PostPic(self, im, codetype):</span><br><span class="line">        params &#x3D; &#123;</span><br><span class="line">            &#39;codetype&#39;: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files &#x3D; &#123;&#39;userfile&#39;: (&#39;ccc.jpg&#39;, im)&#125;</span><br><span class="line">        r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;upload.chaojiying.net&#x2F;Upload&#x2F;Processing.php&#39;, data&#x3D;params, files&#x3D;files, headers&#x3D;self.headers)</span><br><span class="line">        return r.json()</span><br><span class="line"></span><br><span class="line">    def ReportError(self, im_id):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        im_id:报错题目的图片ID</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        params &#x3D; &#123;</span><br><span class="line">            &#39;id&#39;: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;upload.chaojiying.net&#x2F;Upload&#x2F;ReportError.php&#39;, data&#x3D;params, headers&#x3D;self.headers)</span><br><span class="line">        return r.json()</span><br></pre></td></tr></table></figure><h6 id="识别源码："><a href="#识别源码：" class="headerlink" title="识别源码："></a>识别源码：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver </span><br><span class="line">from selenium.common.exceptions import TimeoutException , NoSuchElementException</span><br><span class="line">import time </span><br><span class="line">from PIL import Image</span><br><span class="line">import pytesseract</span><br><span class="line">import chaojiying</span><br><span class="line"></span><br><span class="line">browser &#x3D; webdriver.Edge(&#39;E:\\anaconda\\Scripts\\msedgedriver.exe&#39;)</span><br><span class="line"># browser &#x3D; webdriver.Chrome()</span><br><span class="line">try:</span><br><span class="line">    browser.get(&#39;https:&#x2F;&#x2F;so.gushiwen.org&#x2F;user&#x2F;login.aspx?from&#x3D;http:&#x2F;&#x2F;so.gushiwen.org&#x2F;user&#x2F;collect.aspx&#39;)</span><br><span class="line">except TimeoutException:</span><br><span class="line">    print(&#39;Time Out&#39;)</span><br><span class="line"></span><br><span class="line">try:   </span><br><span class="line">    username &#x3D; browser.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;email&quot;]&#39;)</span><br><span class="line">    username.send_keys(&#39;自己账号&#39;)</span><br><span class="line">    time.sleep(1)</span><br><span class="line">    </span><br><span class="line">    password &#x3D; browser.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;pwd&quot;]&#39;)</span><br><span class="line">    password.send_keys(&#39;自己密码&#39;)</span><br><span class="line">    time.sleep(1)</span><br><span class="line">    </span><br><span class="line">    pictureN &#x3D; browser.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;imgCode&quot;]&#39;)</span><br><span class="line">    </span><br><span class="line">    browser.save_screenshot(&#39;login.png&#39;)</span><br><span class="line"></span><br><span class="line">    loc &#x3D; pictureN.location</span><br><span class="line">    size &#x3D; pictureN.size</span><br><span class="line"> </span><br><span class="line">    left &#x3D; loc[&#39;x&#39;]</span><br><span class="line">    top &#x3D; loc[&#39;y&#39;]</span><br><span class="line">    bottom &#x3D; top+size[&#39;height&#39;]</span><br><span class="line">    right &#x3D; left+size[&#39;width&#39;]</span><br><span class="line"> </span><br><span class="line">    page &#x3D; Image.open(&#39;login.png&#39;)</span><br><span class="line">    Code &#x3D; page.crop((left,top,right,bottom))</span><br><span class="line">    Code.save(&#39;code.png&#39;)</span><br><span class="line">   </span><br><span class="line">    chaojiying &#x3D; Chaojiying_Client(&#39;超级鹰账号&#39;, &#39;密码&#39;, &#39;ID&#39;)#ID 具体看软件ID。</span><br><span class="line">    im &#x3D; open(&#39;code.png&#39;, &#39;rb&#39;).read()</span><br><span class="line">    text &#x3D; chaojiying.PostPic(im,2004)[&#39;pic_str&#39;]</span><br><span class="line">    print(text)</span><br><span class="line">       </span><br><span class="line">#     text &#x3D; pytesseract.image_to_string(Image.open(&#39;code.png&#39;))</span><br><span class="line">#     print(text)    </span><br><span class="line"></span><br><span class="line">    CodeWhere &#x3D; browser.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;code&quot;]&#39;)</span><br><span class="line">    CodeWhere.send_keys(text)</span><br><span class="line">    time.sleep(5)</span><br><span class="line">                     </span><br><span class="line">    Submit &#x3D; browser.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;denglu&quot;]&#39;)</span><br><span class="line">    Submit.click()</span><br><span class="line">    time.sleep(5)</span><br><span class="line">except NoSuchElementException:</span><br><span class="line">    print(&#39;No Element&#39;)</span><br><span class="line">finally:</span><br><span class="line">    browser.close()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——古诗文网中验证码（超级鹰）&quot;&gt;&lt;a href=&quot;#网络爬虫——古诗文网中验证码（超级鹰）&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——古诗文网中验证码（超级鹰）&quot;&gt;&lt;/a&gt;网络爬虫——古诗文网中验证码（超级鹰）&lt;/h2&gt;&lt;h4 i
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>网络爬虫——超级鹰源码下载</title>
    <link href="http://ailous.top/2020/04/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%B6%85%E7%BA%A7%E9%B9%B0%E6%BA%90%E7%A0%81%E4%B8%8B%E8%BD%BD/"/>
    <id>http://ailous.top/2020/04/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%B6%85%E7%BA%A7%E9%B9%B0%E6%BA%90%E7%A0%81%E4%B8%8B%E8%BD%BD/</id>
    <published>2020-04-27T05:47:40.000Z</published>
    <updated>2020-05-10T02:26:14.860Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——超级鹰源码下载"><a href="#网络爬虫——超级鹰源码下载" class="headerlink" title="网络爬虫——超级鹰源码下载"></a>网络爬虫——超级鹰源码下载</h2><p>超级鹰官方网址：<a href="https://www.chaojiying.com/" target="_blank" rel="noopener">https://www.chaojiying.com/</a><br><img src="https://img-blog.csdnimg.cn/20200510100745760.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200510100745748.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200510100745748.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>将文件下载再解压，这里使用源码：<br><img src="https://img-blog.csdnimg.cn/20200510100952775.jpg#pic_center" alt="在这里插入图片描述"></p><h4 id="源码："><a href="#源码：" class="headerlink" title="源码："></a>源码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line">import requests</span><br><span class="line">from hashlib import md5</span><br><span class="line"></span><br><span class="line">class Chaojiying_Client(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, username, password, soft_id):</span><br><span class="line">        self.username &#x3D; username</span><br><span class="line">password &#x3D;  password.encode(&#39;utf8&#39;)</span><br><span class="line">        self.password &#x3D; md5(password).hexdigest()</span><br><span class="line">        self.soft_id &#x3D; soft_id</span><br><span class="line">        self.base_params &#x3D; &#123;</span><br><span class="line">            &#39;user&#39;: self.username,</span><br><span class="line">            &#39;pass2&#39;: self.password,</span><br><span class="line">            &#39;softid&#39;: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers &#x3D; &#123;</span><br><span class="line">            &#39;Connection&#39;: &#39;Keep-Alive&#39;,</span><br><span class="line">            &#39;User-Agent&#39;: &#39;Mozilla&#x2F;4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident&#x2F;4.0)&#39;,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    def PostPic(self, im, codetype):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        im: 图片字节</span><br><span class="line">        codetype: 题目类型 参考 http:&#x2F;&#x2F;www.chaojiying.com&#x2F;price.html</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        params &#x3D; &#123;</span><br><span class="line">            &#39;codetype&#39;: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files &#x3D; &#123;&#39;userfile&#39;: (&#39;ccc.jpg&#39;, im)&#125;</span><br><span class="line">        r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;upload.chaojiying.net&#x2F;Upload&#x2F;Processing.php&#39;, data&#x3D;params, files&#x3D;files, headers&#x3D;self.headers)</span><br><span class="line">        return r.json()</span><br><span class="line"></span><br><span class="line">    def ReportError(self, im_id):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        im_id:报错题目的图片ID</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        params &#x3D; &#123;</span><br><span class="line">            &#39;id&#39;: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;upload.chaojiying.net&#x2F;Upload&#x2F;ReportError.php&#39;, data&#x3D;params, headers&#x3D;self.headers)</span><br><span class="line">        return r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">chaojiying &#x3D; Chaojiying_Client(&#39;超级鹰用户名&#39;, &#39;超级鹰用户名的密码&#39;, &#39;96001&#39;)#用户中心&gt;&gt;软件ID 生成一个替换 96001</span><br><span class="line">im &#x3D; open(&#39;a.jpg&#39;, &#39;rb&#39;).read()#本地图片文件路径 来替换 a.jpg 有时WIN系统须要&#x2F;&#x2F;</span><br><span class="line">print chaojiying.PostPic(im, 1902)#1902 验证码类型  官方网站&gt;&gt;价格体系 3.4+版 print 后要加()</span><br></pre></td></tr></table></figure><ul><li><h6 id="a-jpg-为自己目标图片"><a href="#a-jpg-为自己目标图片" class="headerlink" title="a.jpg 为自己目标图片"></a>a.jpg 为自己目标图片</h6></li><li><h6 id="超级鹰用户名及密码。需要自己去官网注册使用。"><a href="#超级鹰用户名及密码。需要自己去官网注册使用。" class="headerlink" title="超级鹰用户名及密码。需要自己去官网注册使用。"></a>超级鹰用户名及密码。需要自己去官网注册使用。</h6></li></ul><p>具体使用方法到我的后继博客中查找。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——超级鹰源码下载&quot;&gt;&lt;a href=&quot;#网络爬虫——超级鹰源码下载&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——超级鹰源码下载&quot;&gt;&lt;/a&gt;网络爬虫——超级鹰源码下载&lt;/h2&gt;&lt;p&gt;超级鹰官方网址：&lt;a href=&quot;https://w
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="超级鹰" scheme="http://ailous.top/tags/%E8%B6%85%E7%BA%A7%E9%B9%B0/"/>
    
  </entry>
  
  <entry>
    <title>算法——最长公共子序列</title>
    <link href="http://ailous.top/2020/04/24/%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/"/>
    <id>http://ailous.top/2020/04/24/%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/</id>
    <published>2020-04-23T23:33:47.063Z</published>
    <updated>2020-04-23T23:33:47.324Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法——最长公共子序列"><a href="#算法——最长公共子序列" class="headerlink" title="算法——最长公共子序列"></a>算法——最长公共子序列</h2><p>链接：<a href="https://ac.nowcoder.com/acm/problem/19978" target="_blank" rel="noopener">https://ac.nowcoder.com/acm/problem/19978</a></p><p>来源：牛客网</p><h4 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h4><p>字符序列的子序列是指从给定字符序列中随意地（不一定连续）去掉若干个字符（可能一个也不去掉）后所形成的字符序列。<br>令给定的字符序列X=“x0，x1，…，xm-1”，序列Y=“y0，y1，…，yk-1”是X的子序列，存在X的一个严格递增下标序列 &lt; i0，i1，…，ik-1 &gt; ，使得对所有的j=0，1，…，k-1，有xij = yj。例如，X=“ABCBDAB”，Y=“BCDB”是X的一个子序列。对给定的两个字符序列，求出他们最长的公共子序列长度，以及最长公共子序列个数。</p><h4 id="输入描述"><a href="#输入描述" class="headerlink" title="输入描述:"></a>输入描述:</h4><p>第1行为第1个字符序列，都是大写字母组成，以”.”结束。长度小于5000。<br>第2行为第2个字符序列，都是大写字母组成，以”.”结束，长度小于5000。</p><h4 id="输出描述"><a href="#输出描述" class="headerlink" title="输出描述:"></a>输出描述:</h4><p>第1行输出上述两个最长公共子序列的长度。<br>第2行输出所有可能出现的最长公共子序列个数，答案可能很大，只要将答案对100,000,000求余即可。</p><p><img src="https://img-blog.csdnimg.cn/20200424073104637.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="源码："><a href="#源码：" class="headerlink" title="源码："></a>源码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;cstdio&gt;</span><br><span class="line">#include&lt;cstring&gt;</span><br><span class="line">#define N 5010</span><br><span class="line">#define mod 100000000</span><br><span class="line">int f[2][N],g[2][N];</span><br><span class="line">char A[N],B[N];</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int n,m,i,j,d;</span><br><span class="line">    scanf(&quot;%s%s&quot;,A+1,B+1);</span><br><span class="line">    n&#x3D;strlen(A+1)-1,m&#x3D;strlen(B+1)-1;</span><br><span class="line">    for(i&#x3D;0;i&lt;&#x3D;m;i++) f[0][i]&#x3D;0,g[0][i]&#x3D;1;</span><br><span class="line">    for(d&#x3D;i&#x3D;1;i&lt;&#x3D;n;i++,d^&#x3D;1)</span><br><span class="line">    &#123;</span><br><span class="line">        f[d][0]&#x3D;0,g[d][0]&#x3D;1;</span><br><span class="line">        for(j&#x3D;1;j&lt;&#x3D;m;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(f[d^1][j]&gt;f[d][j-1]) f[d][j]&#x3D;f[d^1][j],</span><br><span class="line">            g[d][j]&#x3D;g[d^1][j];</span><br><span class="line">            else if(f[d^1][j]&lt;f[d][j-1])</span><br><span class="line">            f[d][j]&#x3D;f[d][j-1],g[d][j]&#x3D;g[d][j-1];</span><br><span class="line">            else</span><br><span class="line">            &#123;</span><br><span class="line">                f[d][j]&#x3D;f[d^1][j],g[d][j]&#x3D;(g[d^1][j]+g[d][j-1])%mod;</span><br><span class="line">                if(f[d][j]&#x3D;&#x3D;f[d^1][j-1]) g[d][j]&#x3D;(g[d][j]-g[d^1][j-1]+mod)%mod;</span><br><span class="line">            &#125;</span><br><span class="line">            if(A[i]&#x3D;&#x3D;B[j])</span><br><span class="line">            &#123;</span><br><span class="line">                if(f[d^1][j-1]+1&gt;f[d][j]) f[d][j]&#x3D;f[d^1][j-1]+1,g[d][j]&#x3D;g[d^1][j-1];</span><br><span class="line">                else if(f[d^1][j-1]+1&#x3D;&#x3D;f[d][j])</span><br><span class="line">                g[d][j]&#x3D;(g[d][j]+g[d^1][j-1])%mod;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;%d\n%d\n&quot;,f[n&amp;1][m],g[n&amp;1][m]);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;算法——最长公共子序列&quot;&gt;&lt;a href=&quot;#算法——最长公共子序列&quot; class=&quot;headerlink&quot; title=&quot;算法——最长公共子序列&quot;&gt;&lt;/a&gt;算法——最长公共子序列&lt;/h2&gt;&lt;p&gt;链接：&lt;a href=&quot;https://ac.nowcoder.co
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>汇编题——CJNE</title>
    <link href="http://ailous.top/2020/02/20/%E6%B1%87%E7%BC%96%E9%A2%98%E2%80%94%E2%80%94CJNE/"/>
    <id>http://ailous.top/2020/02/20/%E6%B1%87%E7%BC%96%E9%A2%98%E2%80%94%E2%80%94CJNE/</id>
    <published>2020-02-20T05:47:40.000Z</published>
    <updated>2020-04-23T14:11:59.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="汇编题——CJNE"><a href="#汇编题——CJNE" class="headerlink" title="汇编题——CJNE"></a>汇编题——CJNE</h2><p><img src="https://img-blog.csdnimg.cn/20200423220905984.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;汇编题——CJNE&quot;&gt;&lt;a href=&quot;#汇编题——CJNE&quot; class=&quot;headerlink&quot; title=&quot;汇编题——CJNE&quot;&gt;&lt;/a&gt;汇编题——CJNE&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2020
      
    
    </summary>
    
    
      <category term="汇编" scheme="http://ailous.top/categories/%E6%B1%87%E7%BC%96/"/>
    
    
      <category term="CJNE" scheme="http://ailous.top/tags/CJNE/"/>
    
      <category term="DJNE" scheme="http://ailous.top/tags/DJNE/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——前程无忧网数据获取及存储（高级）</title>
    <link href="http://ailous.top/2019/09/03/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E7%BD%91%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%8F%8A%E5%AD%98%E5%82%A8(%E9%AB%98%E7%BA%A7)/"/>
    <id>http://ailous.top/2019/09/03/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E7%BD%91%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%8F%8A%E5%AD%98%E5%82%A8(%E9%AB%98%E7%BA%A7)/</id>
    <published>2019-09-03T05:47:40.000Z</published>
    <updated>2020-04-14T06:07:53.533Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——前程无忧网数据获取及存储（高级）"><a href="#网络爬虫——前程无忧网数据获取及存储（高级）" class="headerlink" title="网络爬虫——前程无忧网数据获取及存储（高级）"></a>网络爬虫——前程无忧网数据获取及存储（高级）</h2><p>实验内容1<br>目标网站：前程无忧招聘网</p><p>目标网址：<a href="https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html" target="_blank" rel="noopener">https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html</a></p><p>目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间</p><p><img src="https://img-blog.csdnimg.cn/20200413221530218.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>要求</p><p>（1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存；</p><p>（2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）定义函数，将获取的目标数据保存到txt，csv文件中。</p><p>（4）使用框架式结构，通过参数传递实现整个特定数据的爬取。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import csv</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def parsePage(html):</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    clist &#x3D; []</span><br><span class="line">    rlist &#x3D; []</span><br><span class="line">    newhtml &#x3D;etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    result&#x3D;newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;resultList&quot;]&#x2F;div[@class&#x3D;&quot;el&quot;]&#x2F;&#x2F;text()&#39;)</span><br><span class="line">    </span><br><span class="line">    for i in range(len(result)):</span><br><span class="line">        ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\r&#39;,&quot;&quot;).replace(&quot;\n&quot;,&#39;&#39;))</span><br><span class="line"></span><br><span class="line">    while &#39;&#39; in ulist:</span><br><span class="line">        ulist.remove(&#39;&#39;)</span><br><span class="line">    </span><br><span class="line">    length &#x3D; len(ulist)</span><br><span class="line">    weight &#x3D; int(length &#x2F; 5 )</span><br><span class="line">    </span><br><span class="line">    for i in range(weight):</span><br><span class="line">        for j in range(5):</span><br><span class="line">            clist.append(ulist[i*5+j])</span><br><span class="line">        rlist.append(clist)</span><br><span class="line">        clist &#x3D; []</span><br><span class="line">    </span><br><span class="line">    return rlist</span><br><span class="line">        </span><br><span class="line"># def txtdata(data):</span><br><span class="line">#     with open(&#39;top20.txt&#39;,&#39;w&#39;)as file:</span><br><span class="line">#         for i in data:</span><br><span class="line">#             for j in i:</span><br><span class="line">#                 print(j)</span><br><span class="line">#         print(&#39;successful&#39;)</span><br><span class="line"></span><br><span class="line">def storedata(data):</span><br><span class="line">    with open(&#39;top20.txt&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;)as file:</span><br><span class="line">        for i in data:</span><br><span class="line">            file.write(json.dumps(i,ensure_ascii&#x3D;False)+&#39;\n&#39;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">        </span><br><span class="line">def csvdata(data):</span><br><span class="line">    with open(&#39;top20.csv&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;,newline&#x3D;&#39;&#39;)as csvfile:</span><br><span class="line">        fieldnames &#x3D; [&#39;职位名&#39;,&#39;公司名&#39;,&#39;工作地点&#39;,&#39;薪资&#39;,&#39;工作时间&#39;]</span><br><span class="line">        writer &#x3D; csv.DictWriter(csvfile,fieldnames&#x3D;fieldnames)</span><br><span class="line">        writer.writeheader()</span><br><span class="line">        for i in data:</span><br><span class="line">            writer.writerow(&#123;&#39;职位名&#39;:i[0],&#39;公司名&#39;:i[1],&#39;工作地点&#39;:i[2],&#39;薪资&#39;:i[3],&#39;工作时间&#39;:i[4]&#125;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">            </span><br><span class="line">def main():</span><br><span class="line">    url&#x3D;&quot;https:&#x2F;&#x2F;search.51job.com&#x2F;list&#x2F;120000,000000,0000,00,9,99,Python,2,1.html&quot;</span><br><span class="line">    html&#x3D;getHtmlText(url)</span><br><span class="line">    rlist&#x3D;parsePage(html)</span><br><span class="line">    </span><br><span class="line">#     txtdata(data)</span><br><span class="line">    storedata(rlist)</span><br><span class="line">    csvdata(rlist)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="结果输出："><a href="#结果输出：" class="headerlink" title="结果输出："></a>结果输出：</h3><p><img src="https://img-blog.csdnimg.cn/20200413222149472.jpg#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200413222120905.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200413222132312.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——前程无忧网数据获取及存储（高级）&quot;&gt;&lt;a href=&quot;#网络爬虫——前程无忧网数据获取及存储（高级）&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——前程无忧网数据获取及存储（高级）&quot;&gt;&lt;/a&gt;网络爬虫——前程无忧网数据获取及存储（高
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="CSV" scheme="http://ailous.top/tags/CSV/"/>
    
      <category term="Xpath" scheme="http://ailous.top/tags/Xpath/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫常见问题汇总</title>
    <link href="http://ailous.top/2019/06/02/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    <id>http://ailous.top/2019/06/02/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</id>
    <published>2019-06-02T05:47:40.000Z</published>
    <updated>2020-04-14T06:05:21.521Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫常见问题汇总"><a href="#网络爬虫常见问题汇总" class="headerlink" title="网络爬虫常见问题汇总"></a>网络爬虫常见问题汇总</h2><h3 id="问题一：使用requests库或者urllib库获取源代码时无法正常显示中文"><a href="#问题一：使用requests库或者urllib库获取源代码时无法正常显示中文" class="headerlink" title="问题一：使用requests库或者urllib库获取源代码时无法正常显示中文"></a>问题一：使用requests库或者urllib库获取源代码时无法正常显示中文</h3><p>解决方法：</p><p>（1）requests库的文本中有两种类型，一种是文本类型，使用text属性，一种是针对音频、视频、图片等二进制数据类型，使用content属性；一般返回的是text属性时会出现中文乱码现象，因此在输出返回之前需要显示的修改属性encoding，将其赋值为“utf-8”或者是apparent_encoding即可。</p><p>（2）urllib库的文本只有一种就是使用read()方法进行读取。因此要解决中文问题，一定要在读取后加入.decode(“utf-8”)，进行显示的转码之后便不会出现乱码问题了。</p><h3 id="问题二：文本节点"><a href="#问题二：文本节点" class="headerlink" title="问题二：文本节点"></a>问题二：文本节点</h3><p>首先看两个HTML代码:</p><p>这是你眼中的HTML代码<br><img src="https://img-blog.csdnimg.cn/202004132128304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这是计算机眼中的HTML代码:<br><img src="https://img-blog.csdnimg.cn/20200413212840829.png#pic_center" alt="在这里插入图片描述"><br>解决方法：</p><p> 在BS4中, 我们在HTML中看到的换行符以及空格都是NavigableString 也就是文本节点.</p><h3 id="问题三：滥用遍历文档树的方法"><a href="#问题三：滥用遍历文档树的方法" class="headerlink" title="问题三：滥用遍历文档树的方法"></a>问题三：滥用遍历文档树的方法</h3><p>常见的方法有:</p><blockquote><p>contents<br>descendants<br>parent<br>next_sibling<br>next_element</p></blockquote><p>这些方法都会遍历文档树中的所有节点, 包括文本节点. 也就是说: 只要你使用这些方法, 你就一定会选择出许多文本节点, 因为文本节点无处不在: 换行, 空格等.</p><p>解决方法：</p><p>使用过滤器find等方法:</p><p>soup.find(name=’tagname’)<br>当我们一旦在过滤器中指定了name关键字, 那么返回的结果就一定是tag对象, 因为文档节点没有name属性.</p><p>结论: 大多数情况下, 你需要的是find 这一类过滤器, 而不是遍历所有节点.</p><h3 id="问题四：html-parser"><a href="#问题四：html-parser" class="headerlink" title="问题四：html.parser"></a>问题四：html.parser</h3><p>html.parser是个令人又恨又爱的解析器, 它是Python内置的解析器, 开箱即用. 但是在一些情况下, 它解析出来的文档会丢失信息. </p><p>解决方法：</p><p>如果你发现你的文档信息缺少了, 那么试着换其他解析器,例如: lxml</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫常见问题汇总&quot;&gt;&lt;a href=&quot;#网络爬虫常见问题汇总&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫常见问题汇总&quot;&gt;&lt;/a&gt;网络爬虫常见问题汇总&lt;/h2&gt;&lt;h3 id=&quot;问题一：使用requests库或者urllib库获取源代码时无法正
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="问题" scheme="http://ailous.top/tags/%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——二手房数据抓取及MYSQL存储</title>
    <link href="http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8AMYSQL%E5%AD%98%E5%82%A8/"/>
    <id>http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8AMYSQL%E5%AD%98%E5%82%A8/</id>
    <published>2019-05-27T05:47:40.000Z</published>
    <updated>2020-04-23T14:07:12.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——二手房数据抓取及MYSQL存储"><a href="#网络爬虫——二手房数据抓取及MYSQL存储" class="headerlink" title="网络爬虫——二手房数据抓取及MYSQL存储"></a>网络爬虫——二手房数据抓取及MYSQL存储</h2><h4 id="目标网址："><a href="#目标网址：" class="headerlink" title="目标网址："></a>目标网址：</h4><p><a href="https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar" target="_blank" rel="noopener">https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar</a></p><h4 id="目标数据："><a href="#目标数据：" class="headerlink" title="目标数据："></a>目标数据：</h4><p> 标题 + 链接地址 + 厅室+ 面积+ 层数+建造时间 + 地址 + 单价（或总价）</p><p>要求：</p><p>（1）自选请求库和解析库获取目标数据；</p><p>（2）第一个存储至txt或者csv中，第二个源码存储至Mysql中。<br><img src="https://img-blog.csdnimg.cn/20200423220000472.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="源码（1）：csv-txt"><a href="#源码（1）：csv-txt" class="headerlink" title="源码（1）：csv,txt"></a>源码（1）：csv,txt</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import csv</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def cleanData(clist):</span><br><span class="line">    olist &#x3D; []</span><br><span class="line">    for i in range(len(clist)):</span><br><span class="line">        olist.append(clist[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\r&#39;,&quot;&quot;).replace(&quot;\n&quot;,&#39;&#39;).replace(&quot;\xa0\xa0&quot;,&#39;,&#39;))</span><br><span class="line">        </span><br><span class="line">    return olist</span><br><span class="line">    </span><br><span class="line">def parsePage(html):</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    clist &#x3D; []</span><br><span class="line">    </span><br><span class="line">    newhtml &#x3D;etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    titles &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div&#x2F;a&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    hrefs &#x3D; cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div&#x2F;a&#x2F;&#x2F;@href&#39;, stream&#x3D;True))</span><br><span class="line">    others &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div[2]&#x2F;span&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    addresss &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div[3]&#x2F;span&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    prices &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[3]&#x2F;span[2]&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    </span><br><span class="line">    length &#x3D; len(titles)   </span><br><span class="line">    </span><br><span class="line">    for i in range(length):</span><br><span class="line">        ulist.append(titles[i])     </span><br><span class="line">        ulist.append(hrefs[i])</span><br><span class="line">        ulist.append(others[i*4+0])</span><br><span class="line">        ulist.append(others[i*4+1])</span><br><span class="line">        ulist.append(others[i*4+2])</span><br><span class="line">        ulist.append(others[i*4+3])</span><br><span class="line">        ulist.append(addresss[i])</span><br><span class="line">        ulist.append(prices[i])</span><br><span class="line">        clist.append(ulist)</span><br><span class="line">        ulist &#x3D; []</span><br><span class="line">   </span><br><span class="line">    return clist</span><br><span class="line"></span><br><span class="line">def txtdata(data):</span><br><span class="line">    with open(&#39;data.txt&#39;,&#39;w&#39;)as file:</span><br><span class="line">        for i in data:</span><br><span class="line">            for j in i:</span><br><span class="line">                print(j)</span><br><span class="line">        print(&#39;successful&#39;)</span><br><span class="line"></span><br><span class="line">def storedata(data):</span><br><span class="line">    with open(&#39;data.txt&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;)as file:</span><br><span class="line">        for i in data:</span><br><span class="line">            file.write(json.dumps(i,ensure_ascii&#x3D;False)+&#39;\n&#39;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">        </span><br><span class="line">def csvdata(data): </span><br><span class="line">    with open(&#39;data.csv&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;,newline&#x3D;&#39;&#39;)as csvfile:</span><br><span class="line">        fieldnames &#x3D; [&#39;标题&#39;,&#39;链接地址&#39;,&#39;厅室&#39;,&#39;面积&#39;,&#39;层数&#39;,&#39;建造时间&#39;,&#39;地址&#39;,&#39;单价&#39;]</span><br><span class="line">        writer &#x3D; csv.DictWriter(csvfile,fieldnames&#x3D;fieldnames)</span><br><span class="line">        writer.writeheader()</span><br><span class="line">        for i in data:</span><br><span class="line">            writer.writerow(&#123;&#39;标题&#39;:i[0],&#39;链接地址&#39;:i[1],&#39;厅室&#39;:i[2],&#39;面积&#39;:i[3],&#39;层数&#39;:i[4],&#39;建造时间&#39;:i[5],&#39;地址&#39;:i[6],&#39;单价&#39;:i[7]&#125;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">            </span><br><span class="line">def main():</span><br><span class="line">    url&#x3D;&quot;https:&#x2F;&#x2F;qd.anjuke.com&#x2F;sale&#x2F;jiaozhoushi&#x2F;?from&#x3D;SearchBar&quot;</span><br><span class="line">    html&#x3D;getHtmlText(url)</span><br><span class="line">    rlist&#x3D;parsePage(html)</span><br><span class="line">    </span><br><span class="line">    txtdata(rlist)</span><br><span class="line">    storedata(rlist)</span><br><span class="line">    csvdata(rlist)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="源码（1）：MYSQL"><a href="#源码（1）：MYSQL" class="headerlink" title="源码（1）：MYSQL"></a>源码（1）：MYSQL</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import csv</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line">import pymysql </span><br><span class="line">import pytesseract</span><br><span class="line">import traceback  </span><br><span class="line"></span><br><span class="line">def connectMysql():</span><br><span class="line">    return pymysql.connect(host&#x3D;&#39;localhost&#39;,user&#x3D;&#39;root&#39;,password&#x3D;&#39;123456&#39;,port&#x3D;3306,db&#x3D;&#39;spiders&#39;)</span><br><span class="line"></span><br><span class="line">def createMysqlTable():</span><br><span class="line">    db &#x3D; connectMysql()</span><br><span class="line">    cursor &#x3D; db.cursor()</span><br><span class="line">    sql &#x3D; &#39;create table if not exists data (\</span><br><span class="line">    标题 varchar(255) not null ,\</span><br><span class="line">    链接地址 varchar(255) not null ,\</span><br><span class="line">    厅室 varchar(255) not null,\</span><br><span class="line">    面积 varchar(255) not null,\</span><br><span class="line">    层数 varchar(255) not null,\</span><br><span class="line">    建造时间 varchar(255) not null,\</span><br><span class="line">    地址 varchar(255) not null,\</span><br><span class="line">    单价 varchar(255) not null,\</span><br><span class="line">    primary key(标题))&#39;</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line"></span><br><span class="line">    print(&#39;ok&#39;)</span><br><span class="line">    db.close()</span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def cleanData(clist):</span><br><span class="line">    olist &#x3D; []</span><br><span class="line">    for i in range(len(clist)):</span><br><span class="line">        olist.append(clist[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\r&#39;,&quot;&quot;).replace(&quot;\n&quot;,&#39;&#39;).replace(&quot;\xa0\xa0&quot;,&#39;,&#39;))</span><br><span class="line">        </span><br><span class="line">    return olist</span><br><span class="line">    </span><br><span class="line">def parsePage(html):</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    clist &#x3D; []</span><br><span class="line">    </span><br><span class="line">    newhtml &#x3D;etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    titles &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div&#x2F;a&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    hrefs &#x3D; cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div&#x2F;a&#x2F;&#x2F;@href&#39;, stream&#x3D;True))</span><br><span class="line">    others &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div[2]&#x2F;span&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    addresss &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[2]&#x2F;div[3]&#x2F;span&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    prices &#x3D;cleanData(newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;houselist-mod-new&quot;]&#x2F;li&#x2F;div[3]&#x2F;span[2]&#x2F;&#x2F;text()&#39;))</span><br><span class="line">    </span><br><span class="line">    length &#x3D; len(titles)   </span><br><span class="line">    </span><br><span class="line">    for i in range(length):</span><br><span class="line">        ulist.append(titles[i])     </span><br><span class="line">        ulist.append(hrefs[i][0:100])</span><br><span class="line">        ulist.append(others[i*4+0])</span><br><span class="line">        ulist.append(others[i*4+1])</span><br><span class="line">        ulist.append(others[i*4+2])</span><br><span class="line">        ulist.append(others[i*4+3])</span><br><span class="line">        ulist.append(addresss[i])</span><br><span class="line">        ulist.append(prices[i])</span><br><span class="line">        clist.append(ulist)</span><br><span class="line">        ulist &#x3D; []</span><br><span class="line">   </span><br><span class="line">    return clist</span><br><span class="line"></span><br><span class="line">def txtdata(data):</span><br><span class="line">    with open(&#39;data.txt&#39;,&#39;w&#39;)as file:</span><br><span class="line">        for i in data:</span><br><span class="line">            for j in i:</span><br><span class="line">                print(j)</span><br><span class="line">        print(&#39;successful&#39;)</span><br><span class="line"></span><br><span class="line">def storedata(data):</span><br><span class="line">    with open(&#39;data.txt&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;)as file:</span><br><span class="line">        for i in data:</span><br><span class="line">            file.write(json.dumps(i,ensure_ascii&#x3D;False)+&#39;\n&#39;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">        </span><br><span class="line">def csvdata(data): </span><br><span class="line">    with open(&#39;data.csv&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;,newline&#x3D;&#39;&#39;)as csvfile:</span><br><span class="line">        fieldnames &#x3D; [&#39;标题&#39;,&#39;链接地址&#39;,&#39;厅室&#39;,&#39;面积&#39;,&#39;层数&#39;,&#39;建造时间&#39;,&#39;地址&#39;,&#39;单价&#39;]</span><br><span class="line">        writer &#x3D; csv.DictWriter(csvfile,fieldnames&#x3D;fieldnames)</span><br><span class="line">        writer.writeheader()</span><br><span class="line">        for i in data:</span><br><span class="line">            writer.writerow(&#123;&#39;标题&#39;:i[0],&#39;链接地址&#39;:i[1],&#39;厅室&#39;:i[2],&#39;面积&#39;:i[3],&#39;层数&#39;:i[4],&#39;建造时间&#39;:i[5],&#39;地址&#39;:i[6],&#39;单价&#39;:i[7]&#125;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">def mysqlData(datas):</span><br><span class="line">    </span><br><span class="line">    table &#x3D; &#39;data&#39;</span><br><span class="line">    keys &#x3D; &#39;标题,链接地址,厅室,面积,层数,建造时间,地址,单价&#39;</span><br><span class="line">    </span><br><span class="line">    db &#x3D; connectMysql()</span><br><span class="line">    cursor &#x3D; db.cursor()</span><br><span class="line">    for data in datas:</span><br><span class="line">        values &#x3D; &#39;,&#39;.join([&#39;%s&#39;]*len(data))</span><br><span class="line">        sql &#x3D; &#39;INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES(&#123;values&#125;)&#39;.format(table&#x3D;table,keys &#x3D; keys ,values &#x3D; values)</span><br><span class="line">        print(sql)</span><br><span class="line">        print(tuple(data))</span><br><span class="line">        try :</span><br><span class="line">            if cursor.execute(sql, tuple(data)):</span><br><span class="line">                print(&quot;Succcessful&quot;)</span><br><span class="line">                db.commit()</span><br><span class="line">        except:</span><br><span class="line">            traceback.print_exc()</span><br><span class="line">            print(&quot;Failed&quot;)</span><br><span class="line">            db.rollback()</span><br><span class="line">    </span><br><span class="line">    db.close()</span><br><span class="line">                </span><br><span class="line">def main():</span><br><span class="line">#     createMysqlTable()</span><br><span class="line">    url&#x3D;&quot;https:&#x2F;&#x2F;qd.anjuke.com&#x2F;sale&#x2F;jiaozhoushi&#x2F;?from&#x3D;SearchBar&quot;</span><br><span class="line">    html&#x3D;getHtmlText(url)</span><br><span class="line">    rlist&#x3D;parsePage(html)</span><br><span class="line">    </span><br><span class="line">#     txtdata(rlist)</span><br><span class="line">    storedata(rlist)</span><br><span class="line">    csvdata(rlist)</span><br><span class="line">    mysqlData(rlist)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200423220435815.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200423220435749.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200423220435717.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200423220435486.jpg" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——二手房数据抓取及MYSQL存储&quot;&gt;&lt;a href=&quot;#网络爬虫——二手房数据抓取及MYSQL存储&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——二手房数据抓取及MYSQL存储&quot;&gt;&lt;/a&gt;网络爬虫——二手房数据抓取及MYSQL存储&lt;/
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="二手房" scheme="http://ailous.top/tags/%E4%BA%8C%E6%89%8B%E6%88%BF/"/>
    
      <category term="MYSQL" scheme="http://ailous.top/tags/MYSQL/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——前程无忧网数据获取及MYSQL存储</title>
    <link href="http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E7%BD%91%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%8F%8AMYSQL%E5%AD%98%E5%82%A8/"/>
    <id>http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E7%BD%91%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%8F%8AMYSQL%E5%AD%98%E5%82%A8/</id>
    <published>2019-05-27T05:47:40.000Z</published>
    <updated>2020-04-23T13:41:19.059Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——前程无忧网数据获取及MYSQL存储"><a href="#网络爬虫——前程无忧网数据获取及MYSQL存储" class="headerlink" title="网络爬虫——前程无忧网数据获取及MYSQL存储"></a>网络爬虫——前程无忧网数据获取及MYSQL存储</h2><p>实验内容1<br>目标网站：前程无忧招聘网</p><p>目标网址：<a href="https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html" target="_blank" rel="noopener">https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html</a></p><p>目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间</p><p><img src="https://img-blog.csdnimg.cn/20200413221530218.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>要求</p><p>（1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存；</p><p>（2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）定义函数，将获取的目标数据保存到MYSQL库文件中。</p><p>（4）使用框架式结构，通过参数传递实现整个特定数据的爬取。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import csv</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line">import pymysql </span><br><span class="line">from PIL import Image</span><br><span class="line">import pytesseract</span><br><span class="line">import traceback  </span><br><span class="line"></span><br><span class="line">def connectMysql():</span><br><span class="line">    return pymysql.connect(host&#x3D;&#39;localhost&#39;,user&#x3D;&#39;root&#39;,password&#x3D;&#39;123456&#39;,port&#x3D;3306,db&#x3D;&#39;spiders&#39;)</span><br><span class="line"></span><br><span class="line">def createMysqlTable():</span><br><span class="line">    db &#x3D; connectMysql()</span><br><span class="line">    cursor &#x3D; db.cursor()</span><br><span class="line"></span><br><span class="line"># （1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间</span><br><span class="line">    sql &#x3D; &#39;create table if not exists proStr (\</span><br><span class="line">    职位名 varchar(255) not null ,\</span><br><span class="line">    公司名 varchar(255) not null,\</span><br><span class="line">    工作地点 varchar(255) not null,\</span><br><span class="line">    薪资 varchar(255) not null,\</span><br><span class="line">    发布时间 varchar(255) not null,\</span><br><span class="line">    primary key(职位名))&#39;</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line"></span><br><span class="line">    print(&#39;ok&#39;)</span><br><span class="line">    db.close()</span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def parsePage(html):</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    clist &#x3D; []</span><br><span class="line">    rlist &#x3D; []</span><br><span class="line">    ilist &#x3D; []</span><br><span class="line">    </span><br><span class="line">    newhtml &#x3D;etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    result&#x3D;newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;content&quot;]&#x2F;div[2]&#x2F;table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;&#x2F;text()&#39;)</span><br><span class="line">    imgs &#x3D; newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;content&quot;]&#x2F;div[2]&#x2F;table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;a&#x2F;img&#x2F;@src&#39;, stream&#x3D;True)</span><br><span class="line">    j &#x3D; 0 </span><br><span class="line">    </span><br><span class="line">    for img in imgs:</span><br><span class="line">        j&#x3D;j+1</span><br><span class="line">        with open(str(j)+&#39;.png&#39;, &#39;wb&#39;) as fd:</span><br><span class="line">            picture&#x3D;requests.get(img).content</span><br><span class="line">            fd.write(picture)  </span><br><span class="line">      </span><br><span class="line">    for i in range(len(imgs)):  </span><br><span class="line">            str_ &#x3D; str(i+1)+&#39;.png&#39;</span><br><span class="line">            text &#x3D; pytesseract.image_to_string(Image.open(str_))</span><br><span class="line">            ilist.append(text.replace(&quot; &quot;,&quot;.&quot;).replace(&quot;M&quot;,&quot;亿&quot;).replace(&quot;a&quot;,&quot;亿&quot;))</span><br><span class="line">#     print(ilist)        </span><br><span class="line">    for i in range(len(result)):</span><br><span class="line">        ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\r&#39;,&quot;&quot;).replace(&quot;\n&quot;,&#39;&#39;))</span><br><span class="line"></span><br><span class="line">    while &#39;&#39; in ulist:</span><br><span class="line">        ulist.remove(&#39;&#39;)</span><br><span class="line">    </span><br><span class="line">    length &#x3D; len(ulist)</span><br><span class="line">    weight &#x3D; int(length &#x2F; 8 )</span><br><span class="line">    </span><br><span class="line">    for i in range(weight):</span><br><span class="line">        for j in range(8):</span><br><span class="line">            clist.append(ulist[i*8+j])</span><br><span class="line">        clist.append(ilist[i])</span><br><span class="line">        rlist.append(clist)</span><br><span class="line">        clist &#x3D; []</span><br><span class="line">        </span><br><span class="line">    return rlist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mysqlData(datas):</span><br><span class="line">    </span><br><span class="line">    table &#x3D; &#39;movies&#39;</span><br><span class="line">    keys &#x3D; &#39;名次,电影名称,日期,总场次,废场,人次,上座率,票价,票房&#39;</span><br><span class="line">    </span><br><span class="line">    db &#x3D; connectMysql()</span><br><span class="line">    cursor &#x3D; db.cursor()</span><br><span class="line">    </span><br><span class="line">    for data in datas:</span><br><span class="line">        values &#x3D; &#39;,&#39;.join([&#39;%s&#39;]*len(data))</span><br><span class="line">        sql &#x3D; &#39;INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES(&#123;values&#125;)&#39;.format(table&#x3D;table,keys &#x3D; keys ,values &#x3D; values)</span><br><span class="line">        print(sql)</span><br><span class="line">        print(tuple(data))</span><br><span class="line">        try :</span><br><span class="line">            if cursor.execute(sql, tuple(data)):</span><br><span class="line">                print(&quot;Succcessful&quot;)</span><br><span class="line">                db.commit()</span><br><span class="line">        except:</span><br><span class="line">            traceback.print_exc()</span><br><span class="line">            print(&quot;Failed&quot;)</span><br><span class="line">            db.rollback()</span><br><span class="line">    </span><br><span class="line">    db.close()</span><br><span class="line">    </span><br><span class="line">def main():</span><br><span class="line">    createMysqlTable()</span><br><span class="line"></span><br><span class="line">    url&#x3D;&quot;http:&#x2F;&#x2F;58921.com&#x2F;daily&#x2F;wangpiao&quot;</span><br><span class="line">    html&#x3D;getHtmlText(url)</span><br><span class="line">    rlist&#x3D;parsePage(html)</span><br><span class="line">    mysqlData(rlist)</span><br><span class="line">                          </span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="结果输出效果："><a href="#结果输出效果：" class="headerlink" title="结果输出效果："></a>结果输出效果：</h3><p><img src="https://img-blog.csdnimg.cn/20200423213948381.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——前程无忧网数据获取及MYSQL存储&quot;&gt;&lt;a href=&quot;#网络爬虫——前程无忧网数据获取及MYSQL存储&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——前程无忧网数据获取及MYSQL存储&quot;&gt;&lt;/a&gt;网络爬虫——前程无忧网数据获取及M
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="MYSQL" scheme="http://ailous.top/tags/MYSQL/"/>
    
      <category term="前程无忧" scheme="http://ailous.top/tags/%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——票房网数据抓取及存储（初级）</title>
    <link href="http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E7%A5%A8%E6%88%BF%E7%BD%91%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8A%E5%AD%98%E5%82%A8/"/>
    <id>http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E7%A5%A8%E6%88%BF%E7%BD%91%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8A%E5%AD%98%E5%82%A8/</id>
    <published>2019-05-27T05:47:40.000Z</published>
    <updated>2020-04-23T13:51:58.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——票房网数据抓取及存储"><a href="#网络爬虫——票房网数据抓取及存储" class="headerlink" title="网络爬虫——票房网数据抓取及存储"></a>网络爬虫——票房网数据抓取及存储</h2><h4 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h4><p>目标网站：电影票房网</p><p>目标网址：<a href="http://58921.com/daily/wangpiao" target="_blank" rel="noopener">http://58921.com/daily/wangpiao</a></p><h4 id="任务要求"><a href="#任务要求" class="headerlink" title="任务要求"></a>任务要求</h4><p>目标数据：（1）名次（2）电影名称 （3）日期（4）票房 （5）总场次（6）废场（7）人次（8）上座率（9）票价</p><p><img src="https://img-blog.csdnimg.cn/20200423214801795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>（1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存；</p><p>（2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）定义函数，将获取的目标数据保存到csv文件中。</p><p>（4）使用框架式结构，通过参数传递实现整个特定数据的爬取。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import csv</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def parsePage(html):</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    clist &#x3D; []</span><br><span class="line">    rlist &#x3D; []</span><br><span class="line">    ilist &#x3D; []</span><br><span class="line">    </span><br><span class="line">    newhtml &#x3D;etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    result&#x3D;newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;content&quot;]&#x2F;div[2]&#x2F;table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;&#x2F;text()&#39;)</span><br><span class="line">    imgs &#x3D; newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;content&quot;]&#x2F;div[2]&#x2F;table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;a&#x2F;img&#x2F;@src&#39;, stream&#x3D;True)</span><br><span class="line">    j &#x3D; 0 </span><br><span class="line">    </span><br><span class="line">    for img in imgs:</span><br><span class="line">        j&#x3D;j+1</span><br><span class="line">        with open(str(j)+&#39;.png&#39;, &#39;wb&#39;) as fd:</span><br><span class="line">            picture&#x3D;requests.get(img).content</span><br><span class="line">            fd.write(picture)  </span><br><span class="line">      </span><br><span class="line">    for i in range(len(imgs)):  </span><br><span class="line">            str_ &#x3D; str(i+1)+&#39;.png&#39;</span><br><span class="line">            text &#x3D; pytesseract.image_to_string(Image.open(str_))</span><br><span class="line">            ilist.append(text.replace(&quot; &quot;,&quot;.&quot;).replace(&quot;M&quot;,&quot;亿&quot;).replace(&quot;a&quot;,&quot;亿&quot;))</span><br><span class="line">#     print(ilist)        </span><br><span class="line">    for i in range(len(result)):</span><br><span class="line">        ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\r&#39;,&quot;&quot;).replace(&quot;\n&quot;,&#39;&#39;))</span><br><span class="line"></span><br><span class="line">    while &#39;&#39; in ulist:</span><br><span class="line">        ulist.remove(&#39;&#39;)</span><br><span class="line">    </span><br><span class="line">    length &#x3D; len(ulist)</span><br><span class="line">    weight &#x3D; int(length &#x2F; 8 )</span><br><span class="line">    </span><br><span class="line">    for i in range(weight):</span><br><span class="line">        for j in range(8):</span><br><span class="line">            clist.append(ulist[i*8+j])</span><br><span class="line">        clist.append(ilist[i])</span><br><span class="line">        rlist.append(clist)</span><br><span class="line">        clist &#x3D; []</span><br><span class="line">        </span><br><span class="line">    return rlist</span><br><span class="line"></span><br><span class="line"># def txtdata(data):</span><br><span class="line">#     with open(&#39;top20.txt&#39;,&#39;w&#39;)as file:</span><br><span class="line">#         for i in data:</span><br><span class="line">#             for j in i:</span><br><span class="line">#                 print(j)</span><br><span class="line">#         print(&#39;successful&#39;)</span><br><span class="line"></span><br><span class="line">def storedata(data):</span><br><span class="line">    with open(&#39;top20.txt&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;)as file:</span><br><span class="line">        for i in data:</span><br><span class="line">            file.write(json.dumps(i,ensure_ascii&#x3D;False)+&#39;\n&#39;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">        </span><br><span class="line">def csvdata(data):</span><br><span class="line">    with open(&#39;top20.csv&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;,newline&#x3D;&#39;&#39;)as csvfile:</span><br><span class="line">        fieldnames &#x3D; [&#39;名次&#39;,&#39;电影名称&#39;,&#39;日期&#39;,&#39;票房&#39;,&#39;总场次&#39;,&#39;废场&#39;,&#39;人次&#39;,&#39;上座率&#39;,&#39;票价（元）&#39;]</span><br><span class="line">        writer &#x3D; csv.DictWriter(csvfile,fieldnames&#x3D;fieldnames)</span><br><span class="line">        writer.writeheader()</span><br><span class="line">        for i in data:</span><br><span class="line">            writer.writerow(&#123;&#39;名次&#39;:i[0],&#39;电影名称&#39;:i[1],&#39;日期&#39;:i[2],&#39;票房&#39;:i[8],&#39;总场次&#39;:i[3],&#39;废场&#39;:i[4],&#39;人次&#39;:i[5],&#39;上座率&#39;:i[6],&#39;票价（元）&#39;:i[7]&#125;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">            </span><br><span class="line">def main():</span><br><span class="line">    url&#x3D;&quot;http:&#x2F;&#x2F;58921.com&#x2F;daily&#x2F;wangpiao&quot;</span><br><span class="line">    html&#x3D;getHtmlText(url)</span><br><span class="line">    rlist&#x3D;parsePage(html)</span><br><span class="line">    </span><br><span class="line">#     txtdata(rlist)</span><br><span class="line">    storedata(rlist)</span><br><span class="line">    csvdata(rlist)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="结果输出："><a href="#结果输出：" class="headerlink" title="结果输出："></a>结果输出：</h3><p><img src="https://img-blog.csdnimg.cn/20200413222149472.jpg#pic_center" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20200423215045899.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200423215045907.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——票房网数据抓取及存储&quot;&gt;&lt;a href=&quot;#网络爬虫——票房网数据抓取及存储&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——票房网数据抓取及存储&quot;&gt;&lt;/a&gt;网络爬虫——票房网数据抓取及存储&lt;/h2&gt;&lt;h4 id=&quot;实验内容&quot;&gt;&lt;a 
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="票房" scheme="http://ailous.top/tags/%E7%A5%A8%E6%88%BF/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——票房网数据抓取及MYSQL存储</title>
    <link href="http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E7%A5%A8%E6%88%BF%E7%BD%91%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8AMYSQL%E5%AD%98%E5%82%A8/"/>
    <id>http://ailous.top/2019/05/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E7%A5%A8%E6%88%BF%E7%BD%91%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8AMYSQL%E5%AD%98%E5%82%A8/</id>
    <published>2019-05-27T05:47:40.000Z</published>
    <updated>2020-04-23T13:55:45.835Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——票房网数据抓取及MYSQL存储"><a href="#网络爬虫——票房网数据抓取及MYSQL存储" class="headerlink" title="网络爬虫——票房网数据抓取及MYSQL存储"></a>网络爬虫——票房网数据抓取及MYSQL存储</h2><h4 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h4><p>目标网站：电影票房网</p><p>目标网址：<a href="http://58921.com/daily/wangpiao" target="_blank" rel="noopener">http://58921.com/daily/wangpiao</a></p><h4 id="任务要求"><a href="#任务要求" class="headerlink" title="任务要求"></a>任务要求</h4><p>目标数据：（1）名次（2）电影名称 （3）日期（4）票房 （5）总场次（6）废场（7）人次（8）上座率（9）票价<br><img src="https://img-blog.csdnimg.cn/20200423214801795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>要求</p><p>（1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存；</p><p>（2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）定义函数，将获取的目标数据保存到MYSQL库文件中。</p><p>（4）使用框架式结构，通过参数传递实现整个特定数据的爬取。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import csv</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line">import pymysql </span><br><span class="line">from PIL import Image</span><br><span class="line">import pytesseract</span><br><span class="line">import traceback  </span><br><span class="line"></span><br><span class="line">def connectMysql():</span><br><span class="line">    return pymysql.connect(host&#x3D;&#39;localhost&#39;,user&#x3D;&#39;root&#39;,password&#x3D;&#39;123456&#39;,port&#x3D;3306,db&#x3D;&#39;spiders&#39;)</span><br><span class="line"></span><br><span class="line">def createMysqlTable():</span><br><span class="line">    db &#x3D; connectMysql()</span><br><span class="line">    cursor &#x3D; db.cursor()</span><br><span class="line"></span><br><span class="line"># fieldnames &#x3D; [&#39;名次&#39;,&#39;电影名称&#39;,&#39;日期&#39;,&#39;票房&#39;,&#39;总场次&#39;,&#39;废场&#39;,&#39;人次&#39;,&#39;上座率&#39;,&#39;票价&#39;]</span><br><span class="line">    sql &#x3D; &#39;create table if not exists movies (\</span><br><span class="line">    名次 int not null ,\</span><br><span class="line">    电影名称 varchar(255) not null ,\</span><br><span class="line">    日期 varchar(255) not null,\</span><br><span class="line">    总场次 varchar(255) not null,\</span><br><span class="line">    废场 varchar(255) not null,\</span><br><span class="line">    人次 varchar(255) not null,\</span><br><span class="line">    上座率 varchar(255) not null,\</span><br><span class="line">    票价 varchar(255) not null,\</span><br><span class="line">    票房 varchar(255) not null,\</span><br><span class="line">    primary key(名次))&#39;</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line"></span><br><span class="line">    print(&#39;ok&#39;)</span><br><span class="line">    db.close()</span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def parsePage(html):</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    clist &#x3D; []</span><br><span class="line">    rlist &#x3D; []</span><br><span class="line">    ilist &#x3D; []</span><br><span class="line">    </span><br><span class="line">    newhtml &#x3D;etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    result&#x3D;newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;content&quot;]&#x2F;div[2]&#x2F;table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;&#x2F;text()&#39;)</span><br><span class="line">    imgs &#x3D; newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;content&quot;]&#x2F;div[2]&#x2F;table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;a&#x2F;img&#x2F;@src&#39;, stream&#x3D;True)</span><br><span class="line">    j &#x3D; 0 </span><br><span class="line">    </span><br><span class="line">    for img in imgs:</span><br><span class="line">        j&#x3D;j+1</span><br><span class="line">        with open(str(j)+&#39;.png&#39;, &#39;wb&#39;) as fd:</span><br><span class="line">            picture&#x3D;requests.get(img).content</span><br><span class="line">            fd.write(picture)  </span><br><span class="line">      </span><br><span class="line">    for i in range(len(imgs)):  </span><br><span class="line">            str_ &#x3D; str(i+1)+&#39;.png&#39;</span><br><span class="line">            text &#x3D; pytesseract.image_to_string(Image.open(str_))</span><br><span class="line">            ilist.append(text.replace(&quot; &quot;,&quot;.&quot;).replace(&quot;M&quot;,&quot;亿&quot;).replace(&quot;a&quot;,&quot;亿&quot;))</span><br><span class="line">#     print(ilist)        </span><br><span class="line">    for i in range(len(result)):</span><br><span class="line">        ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\r&#39;,&quot;&quot;).replace(&quot;\n&quot;,&#39;&#39;))</span><br><span class="line"></span><br><span class="line">    while &#39;&#39; in ulist:</span><br><span class="line">        ulist.remove(&#39;&#39;)</span><br><span class="line">    </span><br><span class="line">    length &#x3D; len(ulist)</span><br><span class="line">    weight &#x3D; int(length &#x2F; 8 )</span><br><span class="line">    </span><br><span class="line">    for i in range(weight):</span><br><span class="line">        for j in range(8):</span><br><span class="line">            clist.append(ulist[i*8+j])</span><br><span class="line">        clist.append(ilist[i])</span><br><span class="line">        rlist.append(clist)</span><br><span class="line">        clist &#x3D; []</span><br><span class="line">        </span><br><span class="line">    return rlist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mysqlData(datas):</span><br><span class="line">    </span><br><span class="line">    table &#x3D; &#39;movies&#39;</span><br><span class="line">    keys &#x3D; &#39;名次,电影名称,日期,总场次,废场,人次,上座率,票价,票房&#39;</span><br><span class="line">    </span><br><span class="line">    db &#x3D; connectMysql()</span><br><span class="line">    cursor &#x3D; db.cursor()</span><br><span class="line">    </span><br><span class="line">    for data in datas:</span><br><span class="line">        values &#x3D; &#39;,&#39;.join([&#39;%s&#39;]*len(data))</span><br><span class="line">        sql &#x3D; &#39;INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES(&#123;values&#125;)&#39;.format(table&#x3D;table,keys &#x3D; keys ,values &#x3D; values)</span><br><span class="line">        print(sql)</span><br><span class="line">        print(tuple(data))</span><br><span class="line">        try :</span><br><span class="line">            if cursor.execute(sql, tuple(data)):</span><br><span class="line">                print(&quot;Succcessful&quot;)</span><br><span class="line">                db.commit()</span><br><span class="line">        except:</span><br><span class="line">            traceback.print_exc()</span><br><span class="line">            print(&quot;Failed&quot;)</span><br><span class="line">            db.rollback()</span><br><span class="line">    </span><br><span class="line">    db.close()</span><br><span class="line">    </span><br><span class="line">def main():</span><br><span class="line">    createMysqlTable()</span><br><span class="line"></span><br><span class="line">    url&#x3D;&quot;http:&#x2F;&#x2F;58921.com&#x2F;daily&#x2F;wangpiao&quot;</span><br><span class="line">    html&#x3D;getHtmlText(url)</span><br><span class="line">    rlist&#x3D;parsePage(html)</span><br><span class="line">    mysqlData(rlist)</span><br><span class="line">                          </span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="结果输出效果："><a href="#结果输出效果：" class="headerlink" title="结果输出效果："></a>结果输出效果：</h3><p><img src="https://img-blog.csdnimg.cn/20200423215438570.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——票房网数据抓取及MYSQL存储&quot;&gt;&lt;a href=&quot;#网络爬虫——票房网数据抓取及MYSQL存储&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——票房网数据抓取及MYSQL存储&quot;&gt;&lt;/a&gt;网络爬虫——票房网数据抓取及MYSQL存储&lt;/
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="MYSQL" scheme="http://ailous.top/tags/MYSQL/"/>
    
      <category term="票房" scheme="http://ailous.top/tags/%E7%A5%A8%E6%88%BF/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——搜狐最新时政新闻数据爬取——BS4</title>
    <link href="http://ailous.top/2019/05/21/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E6%90%9C%E7%8B%90%E6%9C%80%E6%96%B0%E6%97%B6%E6%94%BF%E6%96%B0%E9%97%BB%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/"/>
    <id>http://ailous.top/2019/05/21/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E6%90%9C%E7%8B%90%E6%9C%80%E6%96%B0%E6%97%B6%E6%94%BF%E6%96%B0%E9%97%BB%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/</id>
    <published>2019-05-21T05:47:40.000Z</published>
    <updated>2020-04-14T06:05:05.658Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——搜狐最新时政新闻数据爬取"><a href="#网络爬虫——搜狐最新时政新闻数据爬取" class="headerlink" title="网络爬虫——搜狐最新时政新闻数据爬取"></a>网络爬虫——搜狐最新时政新闻数据爬取</h2><p>目标网址：<a href="https://www.sohu.com/c/8/1460?spm=smpc.null.side-nav.14.1584869506422WxyU9iK" target="_blank" rel="noopener">https://www.sohu.com/c/8/1460?spm=smpc.null.side-nav.14.1584869506422WxyU9iK</a></p><p>目标数据描述：（1）标题 （2）链接地址<br><img src="https://img-blog.csdnimg.cn/20200413211744198.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>要求：</p><p>（1）使用urllib库或者requests抓取网页源代码；</p><p>（2）使用BeautifulSoup的CSS选择器方法对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）利用框架结构，通过函数调用，参数传递，实现目标数据抓取，并尝试将结果写入文本文件中。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import bs4 </span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;A&quot;)</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def findUniverse(ulist , html):</span><br><span class="line">    soup &#x3D; BeautifulSoup(html,&quot;html.parser&quot;)</span><br><span class="line">    </span><br><span class="line">    for div in soup.find(attrs&#x3D;[&#39;class&#39;,&#39;news-list clearfix&#39;]).children:</span><br><span class="line">        if isinstance(div ,bs4.element.Tag):</span><br><span class="line">            list_0 &#x3D; div.find(&#39;h4&#39;).find(&#39;a&#39;).get(&#39;href&#39;)</span><br><span class="line">            list_1 &#x3D; div.find(&#39;h4&#39;).string.replace(&quot; &quot;,&#39;&#39;).replace(&quot;\n&quot;,&#39;&#39;)</span><br><span class="line">            ulist.append([list_0,list_1])</span><br><span class="line">    </span><br><span class="line">def findSame(ulist,html):</span><br><span class="line">    soup &#x3D; BeautifulSoup(html,&quot;html.parser&quot;)</span><br><span class="line">    </span><br><span class="line">    for div in soup.find(attrs&#x3D;[&#39;class&#39;,&#39;second-nav&#39;]).children:</span><br><span class="line">        if isinstance(div ,bs4.element.Tag):</span><br><span class="line">            ulist.append(div.find(&#39;a&#39;).get(&#39;href&#39;))</span><br><span class="line">    return ulist</span><br><span class="line"></span><br><span class="line">def printUniverse(ulist):</span><br><span class="line">    tplt &#x3D; &#39;&#123;0:30&#125;\t&#123;1:18&#125;&#39;</span><br><span class="line">    print(tplt.format(&quot;网址&quot;,&quot;名称&quot;,chr(12288)))</span><br><span class="line">    for i in range(len(ulist)):</span><br><span class="line">        u &#x3D; ulist[i]</span><br><span class="line">        print(tplt.format(u[0],u[1],chr(12288)))</span><br><span class="line">    </span><br><span class="line">def main():</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    ulist_Same &#x3D; []</span><br><span class="line">    </span><br><span class="line">    url &#x3D; &#39;https:&#x2F;&#x2F;www.sohu.com&#x2F;c&#x2F;8&#x2F;1460?spm&#x3D;smpc.null.side-nav.14.1585491604691ZcX26aI&#39;    </span><br><span class="line">    html &#x3D; getHtmlText(url)</span><br><span class="line">    ulist_Same &#x3D; findSame(ulist_Same,html)</span><br><span class="line">    </span><br><span class="line">    for i in range(len(ulist_Same) - 2 ):</span><br><span class="line">        url &#x3D; &#39;https:&#x2F;&#x2F;www.sohu.com&#39; + ulist_Same[i+1]</span><br><span class="line">        html &#x3D; getHtmlText(url)    </span><br><span class="line">        findUniverse(ulist,html)</span><br><span class="line">    printUniverse(ulist)</span><br><span class="line">    </span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="输出如下："><a href="#输出如下：" class="headerlink" title="输出如下："></a>输出如下：</h3><p><img src="https://img-blog.csdnimg.cn/20200413212248457.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——搜狐最新时政新闻数据爬取&quot;&gt;&lt;a href=&quot;#网络爬虫——搜狐最新时政新闻数据爬取&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——搜狐最新时政新闻数据爬取&quot;&gt;&lt;/a&gt;网络爬虫——搜狐最新时政新闻数据爬取&lt;/h2&gt;&lt;p&gt;目标网址：&lt;
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="BS4" scheme="http://ailous.top/tags/BS4/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣电影排行榜数据抓取（高级）——BS4</title>
    <link href="http://ailous.top/2019/05/04/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C%E6%A6%9C%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%EF%BC%88%E9%AB%98%E7%BA%A7%EF%BC%89/"/>
    <id>http://ailous.top/2019/05/04/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C%E6%A6%9C%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%EF%BC%88%E9%AB%98%E7%BA%A7%EF%BC%89/</id>
    <published>2019-05-04T05:47:40.000Z</published>
    <updated>2020-04-14T06:04:42.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——豆瓣电影排行榜数据抓取（高级）"><a href="#网络爬虫——豆瓣电影排行榜数据抓取（高级）" class="headerlink" title="网络爬虫——豆瓣电影排行榜数据抓取（高级）"></a>网络爬虫——豆瓣电影排行榜数据抓取（高级）</h2><h3 id="目标网址："><a href="#目标网址：" class="headerlink" title="目标网址："></a>目标网址：</h3><p>豆瓣电影排行：<a href="https://movie.douban.com/top250?start=" target="_blank" rel="noopener">https://movie.douban.com/top250?start=</a></p><p>目标数据描述：排名、电影名称、导演、主演、评价人数等信息，将尽可能多的数据抓取保存<br><img src="https://img-blog.csdnimg.cn/20200413210123780.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>任务明细：</p><p>（1）使用requests库实现该网站网页源代码的获取；</p><p>（2）使用BeautifulSoup对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。</p><p>（4）使用框架式结构，通过参数传递实现整个特定数据的爬取。</p><p>可以选择定义全局列表，将目标数据获取后添加到列表中，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import bs4 </span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        result&#x3D;(result.text.replace(&#39;&lt;br&gt;&#39;,&#39;&#39;)).replace(&#39;&lt;br&#x2F;&gt;&#39;,&#39;&#39;)</span><br><span class="line">        return result</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line">            </span><br><span class="line">def findUniverse(ulist , html):</span><br><span class="line">    soup &#x3D; BeautifulSoup(html,&quot;html.parser&quot;)   </span><br><span class="line">    list_ &#x3D; [0,0,0,0,0,0,0]</span><br><span class="line">    </span><br><span class="line">    for li in soup.find(attrs&#x3D;[&#39;class&#39;,&#39;grid_view&#39;]).children:</span><br><span class="line">        if isinstance(li ,bs4.element.Tag):</span><br><span class="line">            list_[0] &#x3D; li.find(&#39;em&#39;).string</span><br><span class="line">            list_[1] &#x3D; li.find(attrs&#x3D;[&#39;class&#39;,&#39;title&#39;]).string</span><br><span class="line">            list_[2] &#x3D; li.find(attrs&#x3D;[&#39;class&#39;,&#39;bd&#39;]).find(attrs&#x3D;[&#39;class&#39;,&#39;&#39;]).string.strip().split(&quot; &quot;)[1].replace(&quot; &quot;,&#39;&#39;)</span><br><span class="line">            list_[3] &#x3D; li.find(attrs&#x3D;[&#39;class&#39;,&#39;bd&#39;]).find(attrs&#x3D;[&#39;class&#39;,&#39;&#39;]).string.strip().split(&quot; &quot;)[4].replace(&quot; &quot;,&#39;&#39;)      </span><br><span class="line">            list_[4] &#x3D; li.find(attrs&#x3D;[&#39;class&#39;,&#39;star&#39;]).span.find_next_sibling().string.strip()</span><br><span class="line">            list_[5] &#x3D; li.find(attrs&#x3D;[&#39;class&#39;,&#39;star&#39;]).span.find_next_sibling().find_next_sibling().find_next_sibling().string.strip()</span><br><span class="line">            if li.find(attrs&#x3D;[&#39;class&#39;,&#39;quote&#39;]) is  not None:</span><br><span class="line">                list_[6] &#x3D; li.find(attrs&#x3D;[&#39;class&#39;,&#39;quote&#39;]).span.string</span><br><span class="line">            else:</span><br><span class="line">                list_[6] &#x3D; None</span><br><span class="line">            </span><br><span class="line">            ulist.append([list_[0],list_[1],list_[2],list_[3],list_[4],list_[5],list_[6]])</span><br><span class="line">            </span><br><span class="line">def printUniverse(ulist):</span><br><span class="line">    tplt &#x3D; &#39;&#123;0:^4&#125;\t&#123;1:^10&#125;\t&#123;2:10&#125;\t&#123;3:10&#125;\t&#123;4:10&#125;\t&#123;5:10&#125;&#39;</span><br><span class="line">    print(tplt.format(&quot;排名&quot;,&quot;电影名称&quot;,&quot;导演&quot;,&quot;主演&quot;,&quot;评分&quot;,&quot;评价人数&quot;,chr(12288)))</span><br><span class="line">    for i in range(len(ulist)):</span><br><span class="line">        u &#x3D; ulist[i]</span><br><span class="line">        print(tplt.format(u[0],u[1],u[2],u[3],u[4],u[5],chr(12288)))</span><br><span class="line">        </span><br><span class="line">def main():</span><br><span class="line">    ulist &#x3D; [] </span><br><span class="line">    for i in range(10):</span><br><span class="line">        url &#x3D;  &#39;https:&#x2F;&#x2F;movie.douban.com&#x2F;top250?start&#x3D;&#39; + str( 25 * i ) </span><br><span class="line">        html &#x3D; getHtmlText(url)</span><br><span class="line">        findUniverse(ulist,html)</span><br><span class="line">        </span><br><span class="line">    printUniverse(ulist)</span><br><span class="line">        </span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>输出如下：<br><img src="https://img-blog.csdnimg.cn/2020041321124824.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——豆瓣电影排行榜数据抓取（高级）&quot;&gt;&lt;a href=&quot;#网络爬虫——豆瓣电影排行榜数据抓取（高级）&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——豆瓣电影排行榜数据抓取（高级）&quot;&gt;&lt;/a&gt;网络爬虫——豆瓣电影排行榜数据抓取（高级）&lt;/
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="BS4" scheme="http://ailous.top/tags/BS4/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣电影排行榜数据抓取（初级）——BS4</title>
    <link href="http://ailous.top/2019/05/03/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C%E6%A6%9C%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%EF%BC%88%E5%88%9D%E7%BA%A7%EF%BC%89/"/>
    <id>http://ailous.top/2019/05/03/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C%E6%A6%9C%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%EF%BC%88%E5%88%9D%E7%BA%A7%EF%BC%89/</id>
    <published>2019-05-03T05:47:40.000Z</published>
    <updated>2020-04-14T06:04:27.346Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——豆瓣电影排行榜数据抓取（初级）"><a href="#网络爬虫——豆瓣电影排行榜数据抓取（初级）" class="headerlink" title="网络爬虫——豆瓣电影排行榜数据抓取（初级）"></a>网络爬虫——豆瓣电影排行榜数据抓取（初级）</h2><h3 id="目标网址："><a href="#目标网址：" class="headerlink" title="目标网址："></a>目标网址：</h3><p>豆瓣电影排行：<a href="https://movie.douban.com/top250?start=" target="_blank" rel="noopener">https://movie.douban.com/top250?start=</a></p><p>目标数据描述：（1）排名（2）电影名称<br><img src="https://img-blog.csdnimg.cn/20200413210123780.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>任务明细：</p><p>（1）使用requests库实现该网站网页源代码的获取；</p><p>（2）使用BeautifulSoup对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。</p><p>（4）使用框架式结构，通过参数传递实现整个特定数据的爬取。</p><p>可以选择定义全局列表，将目标数据获取后添加到列表中，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。</p><p>下一阶段，目标数据增加导演、主演、评价人数等信息，将尽可能多的数据抓取保存。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import bs4 </span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        </span><br><span class="line">        result&#x3D;(result.text.replace(&#39;&lt;br&gt;&#39;,&#39;&#39;)).replace(&#39;&lt;br&#x2F;&gt;&#39;,&#39;&#39;)</span><br><span class="line">        return result</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def findUniverse(ulist , html):</span><br><span class="line">    soup &#x3D; BeautifulSoup(html,&quot;html.parser&quot;)</span><br><span class="line">    list &#x3D; [0,0]</span><br><span class="line">    for li in soup.find(attrs&#x3D;[&#39;class&#39;,&#39;grid_view&#39;]).children:</span><br><span class="line">        if isinstance(li ,bs4.element.Tag):</span><br><span class="line">            list[0] &#x3D; li.find(&#39;em&#39;).string</span><br><span class="line">            list[1] &#x3D; li.find(attrs&#x3D;[&#39;class&#39;,&#39;title&#39;]).string</span><br><span class="line">            ulist.append([list[0],list[1]])</span><br><span class="line"></span><br><span class="line">def printUniverse(ulist):</span><br><span class="line">    tplt &#x3D; &#39;&#123;0:^10&#125;\t&#123;1:^10&#125;&#39;</span><br><span class="line">    print(tplt.format(&quot;排名&quot;,&quot;电影名称&quot;,chr(12288)))</span><br><span class="line">    for i in range(len(ulist)):</span><br><span class="line">        u &#x3D; ulist[i]</span><br><span class="line">        print(tplt.format(u[0],u[1],chr(12288)))</span><br><span class="line">    </span><br><span class="line">def main():</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    url &#x3D; &#39;https:&#x2F;&#x2F;movie.douban.com&#x2F;top250?start&#x3D;&#39;</span><br><span class="line">    html &#x3D; getHtmlText(url)</span><br><span class="line">    </span><br><span class="line">    findUniverse(ulist,html)</span><br><span class="line">    printUniverse(ulist)</span><br><span class="line">    </span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>输出如下：<br><img src="https://img-blog.csdnimg.cn/20200413210616682.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——豆瓣电影排行榜数据抓取（初级）&quot;&gt;&lt;a href=&quot;#网络爬虫——豆瓣电影排行榜数据抓取（初级）&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——豆瓣电影排行榜数据抓取（初级）&quot;&gt;&lt;/a&gt;网络爬虫——豆瓣电影排行榜数据抓取（初级）&lt;/
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="BS4" scheme="http://ailous.top/tags/BS4/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——抓取TIOBE指数前20名排行开发语言</title>
    <link href="http://ailous.top/2019/04/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E6%8A%93%E5%8F%96TIOBE%E6%8C%87%E6%95%B0%E5%89%8D20%E5%90%8D%E6%8E%92%E8%A1%8C%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80/"/>
    <id>http://ailous.top/2019/04/27/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E6%8A%93%E5%8F%96TIOBE%E6%8C%87%E6%95%B0%E5%89%8D20%E5%90%8D%E6%8E%92%E8%A1%8C%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80/</id>
    <published>2019-04-27T05:47:40.000Z</published>
    <updated>2020-04-14T06:07:27.821Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——抓取TIOBE指数前20名排行开发语言"><a href="#网络爬虫——抓取TIOBE指数前20名排行开发语言" class="headerlink" title="网络爬虫——抓取TIOBE指数前20名排行开发语言"></a>网络爬虫——抓取TIOBE指数前20名排行开发语言</h2><p>目标网址<br>TIOBE指数前20名排行开发语言：<a href="https://www.tiobe.com/tiobe-index/" target="_blank" rel="noopener">https://www.tiobe.com/tiobe-index/</a></p><p>说明<br> TIOBE排行榜是根据互联网上有经验的程序员、课程和第三方厂商的数量，并使用搜索引擎（如Google、Bing、Yahoo!）以及Wikipedia、Amazon、YouTube统计出排名数据，只是反映某个编程语言的热门程度，并不能说明一门编程语言好不好，或者一门语言所编写的代码数量多少。</p><p> 该指数可以用来检阅开发者的编程技能能否跟上趋势，或是否有必要作出战略改变，以及什么编程语言是应该及时掌握的。观察认为，该指数反应的虽并非当前最流行或应用最广的语言，但对世界范围内开发语言的走势仍具有重要参考意义。</p><p><img src="https://img-blog.csdnimg.cn/20200413220913662.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>目标数据：（如上表所示）</p><p>（1）2020年3月的排名（2）2019年3月排名（3）编程语言（4）评分（5）变化率</p><p>明细：<br>（1）使用urllib或者requests库抓取目标网页中的网页源代码；</p><p>（2）使用lxml库中的xpath方法解析源代码，提取上面所示的目标数据，并打印输出；</p><p>（3）尝试着使用try..except方法及时捕获异常。</p><p>（4）可以尝试将获取的数据保存到文本文件中。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line">def one_to_page(url):</span><br><span class="line">    headers&#x3D;&#123;</span><br><span class="line">        &#39;user-agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; WOW64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;70.0.3538.25 Safari&#x2F;537.36 Core&#x2F;1.70.3756.400 QQBrowser&#x2F;10.5.4039.400&#39;</span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        response&#x3D;requests.get(url,headers&#x3D;headers)</span><br><span class="line">        body&#x3D;response.text</span><br><span class="line">        return body</span><br><span class="line">    except RequestException as e:</span><br><span class="line">        print(&#39;request is error!&#39;,e)</span><br><span class="line">def parsePage(html):</span><br><span class="line">    htmlNew &#x3D; etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    result &#x3D; htmlNew.xpath(&#39;&#x2F;&#x2F;table[contains(@class,&quot;table-top20&quot;)]&#x2F;tbody&#x2F;tr&#x2F;&#x2F;text()&#39;)</span><br><span class="line">    pos &#x3D; 0</span><br><span class="line">    for i in range(20):</span><br><span class="line">        if i &#x3D;&#x3D; 0:</span><br><span class="line">            yield result[i:5]</span><br><span class="line">        else:</span><br><span class="line">            yield result[pos:pos+5]</span><br><span class="line">        pos +&#x3D; 5</span><br><span class="line">def printRank(data):</span><br><span class="line">    for i in data:</span><br><span class="line">        rank &#x3D; &#123;</span><br><span class="line">            &quot;2020年3月&quot;:i[0],</span><br><span class="line">            &quot;2019年3月&quot;:i[1],</span><br><span class="line">            &quot;编程语言&quot;:i[2],</span><br><span class="line">            &quot;评分&quot;:i[3],</span><br><span class="line">            &quot;变化率&quot;:i[4],    </span><br><span class="line">        &#125;</span><br><span class="line">        print(rank)</span><br><span class="line">def printRankEasy(data):</span><br><span class="line">    tplt &#x3D; &quot;&#123;0:^10&#125;\t&#123;1:^10&#125;\t&#123;2:^20&#125;\t&#123;3:^10&#125;\t&#123;4:^10&#125;&quot;</span><br><span class="line">    print(tplt.format(&quot;2020年3月&quot;,&quot;2019年3月&quot;,&quot;编程语言&quot;,&quot;评分&quot;,&quot;变化率&quot;,chr(12288)))</span><br><span class="line">    for i in data:</span><br><span class="line">        print(tplt.format(i[0],i[1],i[2],i[3],i[4],chr(12288)))</span><br><span class="line">def main():</span><br><span class="line">    url &#x3D; &#39;https:&#x2F;&#x2F;www.tiobe.com&#x2F;tiobe-index&#x2F;&#39;</span><br><span class="line">    html &#x3D; one_to_page(url)</span><br><span class="line">    data &#x3D; parsePage(html)</span><br><span class="line">    printRankEasy(data)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>输出如下<br><img src="https://img-blog.csdnimg.cn/20200413221221131.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——抓取TIOBE指数前20名排行开发语言&quot;&gt;&lt;a href=&quot;#网络爬虫——抓取TIOBE指数前20名排行开发语言&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——抓取TIOBE指数前20名排行开发语言&quot;&gt;&lt;/a&gt;网络爬虫——抓取TIO
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Xpath" scheme="http://ailous.top/tags/Xpath/"/>
    
      <category term="TIOBE指数" scheme="http://ailous.top/tags/TIOBE%E6%8C%87%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>中国大学排名数据抓取</title>
    <link href="http://ailous.top/2019/04/20/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E6%8E%92%E5%90%8D%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96/"/>
    <id>http://ailous.top/2019/04/20/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E6%8E%92%E5%90%8D%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96/</id>
    <published>2019-04-20T05:47:40.000Z</published>
    <updated>2020-04-14T06:04:05.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——中国大学排名数据抓取"><a href="#网络爬虫——中国大学排名数据抓取" class="headerlink" title="网络爬虫——中国大学排名数据抓取"></a>网络爬虫——中国大学排名数据抓取</h2><h3 id="目标网址"><a href="#目标网址" class="headerlink" title="目标网址"></a>目标网址</h3><p>中国大学排名网：<a href="http://www.zuihaodaxue.com/zuihaodaxuepaiming2019.html" target="_blank" rel="noopener">http://www.zuihaodaxue.com/zuihaodaxuepaiming2019.html</a></p><p> 全球有很多份大学排名，这里以上海交通大学研发的“软科中国最好大学排名2019”为例，，编写“大学排名爬虫”，从网络上获取数据 。拟从该网址爬取该名单上310 所国内大学的排名数据，并将它们打印出来。<br><img src="https://img-blog.csdnimg.cn/20200413204633837.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>大学排名爬虫的构建需要三个重要步骤：</p><p>第一，从网络上获取网页内容；</p><p>第二，分析网页内容并提取有用数据到恰当的数据结构中；</p><p>第三，利用数据结构展示或进一步处理数据。</p><p>由于大学排名是一个典型的二维数据，因此，采用二维列表存储该排名所涉及的表单数据。具体来说，采用requests 库爬取网页内容，使用beautifulsoup4 库分析网页中数据，提取310 个学校的排名及相关数据，存储到二维列表中，最后采用用户偏好的方式打印出来。</p><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import bs4 </span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def findUniverse(ulist , html):</span><br><span class="line">    soup &#x3D; BeautifulSoup(html,&quot;html.parser&quot;)</span><br><span class="line"></span><br><span class="line">    for tr in soup.find(attrs&#x3D;[&#39;class&#39;,&#39;hidden_zhpm&#39;]).children:</span><br><span class="line">        if isinstance(tr ,bs4.element.Tag):</span><br><span class="line">            tds &#x3D; tr(&#39;td&#39;)</span><br><span class="line">            ulist.append([tds[0].string, tds[1].string,tds[3].string ])</span><br><span class="line">    </span><br><span class="line">def printUniverse(ulist):</span><br><span class="line">    tplt &#x3D; &#39;&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;&#39;</span><br><span class="line">    print(tplt.format(&quot;排名&quot;,&quot;学校名称&quot;,&quot;总分&quot;,chr(12288)))</span><br><span class="line">    for i in range(len(ulist)):</span><br><span class="line">        u &#x3D; ulist[i]</span><br><span class="line">        print(tplt.format(u[0],u[1],u[2],chr(12288)))</span><br><span class="line">    </span><br><span class="line">def main():</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    url &#x3D; &#39;http:&#x2F;&#x2F;www.zuihaodaxue.com&#x2F;zuihaodaxuepaiming2019.html&#39;</span><br><span class="line">    html &#x3D; getHtmlText(url)</span><br><span class="line">    </span><br><span class="line">    findUniverse(ulist,html)</span><br><span class="line">    printUniverse(ulist)</span><br><span class="line">    </span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>结果输出如下：<br><img src="https://img-blog.csdnimg.cn/20200413205104149.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——中国大学排名数据抓取&quot;&gt;&lt;a href=&quot;#网络爬虫——中国大学排名数据抓取&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——中国大学排名数据抓取&quot;&gt;&lt;/a&gt;网络爬虫——中国大学排名数据抓取&lt;/h2&gt;&lt;h3 id=&quot;目标网址&quot;&gt;&lt;a 
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="BS4" scheme="http://ailous.top/tags/BS4/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫——前程无忧网数据获取及存储（低级）</title>
    <link href="http://ailous.top/2019/04/10/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E7%BD%91%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%8F%8A%E5%AD%98%E5%82%A8%EF%BC%88%E4%BD%8E%E7%BA%A7%EF%BC%89/"/>
    <id>http://ailous.top/2019/04/10/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E7%BD%91%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%8F%8A%E5%AD%98%E5%82%A8%EF%BC%88%E4%BD%8E%E7%BA%A7%EF%BC%89/</id>
    <published>2019-04-10T05:47:40.000Z</published>
    <updated>2020-04-14T06:03:41.985Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫——前程无忧网数据获取及存储（低级）"><a href="#网络爬虫——前程无忧网数据获取及存储（低级）" class="headerlink" title="网络爬虫——前程无忧网数据获取及存储（低级）"></a>网络爬虫——前程无忧网数据获取及存储（低级）</h2><h3 id="目标网站：前程无忧招聘网"><a href="#目标网站：前程无忧招聘网" class="headerlink" title="目标网站：前程无忧招聘网"></a>目标网站：前程无忧招聘网</h3><p>目标网址：<a href="https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html" target="_blank" rel="noopener">https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html</a></p><p>目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间</p><p>任务要求</p><p>（1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存；</p><p>（2）通过Xpath解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析；</p><p>（3）定义函数，将获取的目标数据保存到txt文本文件中。</p><p>这里使用的是Xpath，相对于之前猫眼电影使用的，这个较为简单，但是数据处理上较为复杂。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import csv</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line">def getHtmlText(url):</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64)AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36 Edg&#x2F;80.0.361.69&#39;        </span><br><span class="line">    &#125;</span><br><span class="line">    try:</span><br><span class="line">        result &#x3D; requests.get(url,headers&#x3D;headers,timeout&#x3D;30)</span><br><span class="line">        result.raise_for_status()</span><br><span class="line">        result.encoding &#x3D; result.apparent_encoding</span><br><span class="line">        return result.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def parsePage(html):</span><br><span class="line">    ulist &#x3D; []</span><br><span class="line">    clist &#x3D; []</span><br><span class="line">    rlist &#x3D; []</span><br><span class="line">    newhtml &#x3D;etree.HTML(html,etree.HTMLParser())</span><br><span class="line">    result&#x3D;newhtml.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;resultList&quot;]&#x2F;div[@class&#x3D;&quot;el&quot;]&#x2F;&#x2F;text()&#39;)</span><br><span class="line">    </span><br><span class="line">    for i in range(len(result)):</span><br><span class="line">        ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\r&#39;,&quot;&quot;).replace(&quot;\n&quot;,&#39;&#39;))</span><br><span class="line"></span><br><span class="line">    while &#39;&#39; in ulist:</span><br><span class="line">        ulist.remove(&#39;&#39;)</span><br><span class="line">    </span><br><span class="line">    length &#x3D; len(ulist)</span><br><span class="line">    weight &#x3D; int(length &#x2F; 5 )</span><br><span class="line">    </span><br><span class="line">    for i in range(weight):</span><br><span class="line">        for j in range(5):</span><br><span class="line">            clist.append(ulist[i*5+j])</span><br><span class="line">        rlist.append(clist)</span><br><span class="line">        clist &#x3D; []</span><br><span class="line">    </span><br><span class="line">    return rlist</span><br><span class="line">        </span><br><span class="line"># def txtdata(data):</span><br><span class="line">#     with open(&#39;top20.txt&#39;,&#39;w&#39;)as file:</span><br><span class="line">#         for i in data:</span><br><span class="line">#             for j in i:</span><br><span class="line">#                 print(j)</span><br><span class="line">#         print(&#39;successful&#39;)</span><br><span class="line"></span><br><span class="line">def storedata(data):</span><br><span class="line">    with open(&#39;top20.txt&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;)as file:</span><br><span class="line">        for i in data:</span><br><span class="line">            file.write(json.dumps(i,ensure_ascii&#x3D;False)+&#39;\n&#39;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">        </span><br><span class="line">def csvdata(data):</span><br><span class="line">    with open(&#39;top20.csv&#39;,&#39;w&#39;,encoding &#x3D; &#39;utf-8&#39;,newline&#x3D;&#39;&#39;)as csvfile:</span><br><span class="line">        fieldnames &#x3D; [&#39;职位名&#39;,&#39;公司名&#39;,&#39;工作地点&#39;,&#39;薪资&#39;,&#39;工作时间&#39;]</span><br><span class="line">        writer &#x3D; csv.DictWriter(csvfile,fieldnames&#x3D;fieldnames)</span><br><span class="line">        writer.writeheader()</span><br><span class="line">        for i in data:</span><br><span class="line">            writer.writerow(&#123;&#39;职位名&#39;:i[0],&#39;公司名&#39;:i[1],&#39;工作地点&#39;:i[2],&#39;薪资&#39;:i[3],&#39;工作时间&#39;:i[4]&#125;)</span><br><span class="line">        print(&#39;ok&#39;)</span><br><span class="line">            </span><br><span class="line">def main():</span><br><span class="line">    url&#x3D;&quot;https:&#x2F;&#x2F;search.51job.com&#x2F;list&#x2F;120000,000000,0000,00,9,99,Python,2,1.html&quot;</span><br><span class="line">    html&#x3D;getHtmlText(url)</span><br><span class="line">    rlist&#x3D;parsePage(html)</span><br><span class="line">    </span><br><span class="line">#     txtdata(data)</span><br><span class="line">    storedata(rlist)</span><br><span class="line">    csvdata(rlist)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络爬虫——前程无忧网数据获取及存储（低级）&quot;&gt;&lt;a href=&quot;#网络爬虫——前程无忧网数据获取及存储（低级）&quot; class=&quot;headerlink&quot; title=&quot;网络爬虫——前程无忧网数据获取及存储（低级）&quot;&gt;&lt;/a&gt;网络爬虫——前程无忧网数据获取及存储（低
      
    
    </summary>
    
    
      <category term="网络爬虫" scheme="http://ailous.top/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Xpath" scheme="http://ailous.top/tags/Xpath/"/>
    
  </entry>
  
  <entry>
    <title>人工智能——mnist（手写识别）</title>
    <link href="http://ailous.top/2018/12/09/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E2%80%94%E2%80%94mnist%EF%BC%88%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB%EF%BC%89/"/>
    <id>http://ailous.top/2018/12/09/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E2%80%94%E2%80%94mnist%EF%BC%88%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB%EF%BC%89/</id>
    <published>2018-12-09T05:47:40.000Z</published>
    <updated>2020-04-14T07:57:08.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="人工智能——mnist（手写识别）"><a href="#人工智能——mnist（手写识别）" class="headerlink" title="人工智能——mnist（手写识别）"></a>人工智能——mnist（手写识别）</h2><p>因为时间较久了，就不详细介绍了。<br>简单的说，一开始对已有的大量数字图片进行训练生成模型。然后通过对一张手写的数字图片进行读取，进入模型匹配，输出结果。</p><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"># Python3</span><br><span class="line"></span><br><span class="line"># 使用LeNet5的七层卷积神经网络用于MNIST手写数字识别</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;, one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line"># 为输入图像和目标输出类别创建节点</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, 784]) # 训练所需数据  占位符</span><br><span class="line">y_ &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, 10]) # 训练所需标签数据  占位符</span><br><span class="line"></span><br><span class="line"># *************** 构建多层卷积网络 *************** #</span><br><span class="line"></span><br><span class="line"># 权重、偏置、卷积及池化操作初始化,以避免在建立模型的时候反复做初始化操作</span><br><span class="line">def weight_variable(shape):</span><br><span class="line">  initial &#x3D; tf.truncated_normal(shape, stddev&#x3D;0.1) # 取随机值，符合均值为0，标准差stddev为0.1</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">def bias_variable(shape):</span><br><span class="line">  initial &#x3D; tf.constant(0.1, shape&#x3D;shape)</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"># x 的第一个参数为图片的数量，第二、三个参数分别为图片高度和宽度，第四个参数为图片通道数。</span><br><span class="line"># W 的前两个参数为卷积核尺寸，第三个参数为图像通道数，第四个参数为卷积核数量</span><br><span class="line"># strides为卷积步长，其第一、四个参数必须为1，因为卷积层的步长只对矩阵的长和宽有效</span><br><span class="line"># padding表示卷积的形式，即是否考虑边界。&quot;SAME&quot;是考虑边界，不足的时候用0去填充周围，&quot;VALID&quot;则不考虑</span><br><span class="line">def conv2d(x, W):</span><br><span class="line">  return tf.nn.conv2d(x, W, strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line"></span><br><span class="line"># x 参数的格式同tf.nn.conv2d中的x，ksize为池化层过滤器的尺度，strides为过滤器步长</span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">  return tf.nn.max_pool(x, ksize&#x3D;[1, 2, 2, 1], strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line"></span><br><span class="line">#把x更改为4维张量，第1维代表样本数量，第2维和第3维代表图像长宽， 第4维代表图像通道数</span><br><span class="line">x_image &#x3D; tf.reshape(x, [-1,28,28,1]) # -1表示任意数量的样本数,大小为28x28，深度为1的张量</span><br><span class="line"></span><br><span class="line"># 第一层：卷积</span><br><span class="line">W_conv1 &#x3D; weight_variable([5, 5, 1, 32]) # 卷积在每个5x5的patch中算出32个特征。</span><br><span class="line">b_conv1 &#x3D; bias_variable([32])</span><br><span class="line"></span><br><span class="line">h_conv1 &#x3D; tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line"></span><br><span class="line"># 第二层：池化</span><br><span class="line">h_pool1 &#x3D; max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"># 第三层：卷积</span><br><span class="line">W_conv2 &#x3D; weight_variable([5, 5, 32, 64])</span><br><span class="line">b_conv2 &#x3D; bias_variable([64])</span><br><span class="line"></span><br><span class="line">h_conv2 &#x3D; tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line"># 第四层：池化</span><br><span class="line">h_pool2 &#x3D; max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"># 第五层：全连接层</span><br><span class="line">W_fc1 &#x3D; weight_variable([7 * 7 * 64, 1024])</span><br><span class="line">b_fc1 &#x3D; bias_variable([1024])</span><br><span class="line"></span><br><span class="line">h_pool2_flat &#x3D; tf.reshape(h_pool2, [-1, 7*7*64])</span><br><span class="line">h_fc1 &#x3D; tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"># 在输出层之前加入dropout以减少过拟合</span><br><span class="line">keep_prob &#x3D; tf.placeholder(&quot;float&quot;)</span><br><span class="line">h_fc1_drop &#x3D; tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line"># 第六层：全连接层</span><br><span class="line">W_fc2 &#x3D; weight_variable([1024, 10])</span><br><span class="line">b_fc2 &#x3D; bias_variable([10])</span><br><span class="line"></span><br><span class="line"># 第七层：输出层</span><br><span class="line">y_conv&#x3D;tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"># *************** 训练和评估模型 *************** #</span><br><span class="line"></span><br><span class="line"># 为训练过程指定最小化误差用的损失函数，即目标类别和预测类别之间的交叉熵</span><br><span class="line">cross_entropy &#x3D; -tf.reduce_sum(y_*tf.log(y_conv))</span><br><span class="line"></span><br><span class="line"># 使用反向传播，利用优化器使损失函数最小化</span><br><span class="line">train_step &#x3D; tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"># 检测我们的预测是否真实标签匹配(索引位置一样表示匹配)</span><br><span class="line"># tf.argmax(y_conv,dimension), 返回最大数值的下标 通常和tf.equal()一起使用，计算模型准确度</span><br><span class="line"># dimension&#x3D;0 按列找  dimension&#x3D;1 按行找</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))</span><br><span class="line"></span><br><span class="line"># 统计测试准确率， 将correct_prediction的布尔值转换为浮点数来代表对、错，并取平均值。</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span><br><span class="line"></span><br><span class="line">saver &#x3D; tf.train.Saver() # 定义saver</span><br><span class="line"></span><br><span class="line"># *************** 开始训练模型 *************** #</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    for i in range(1000):</span><br><span class="line">      batch &#x3D; mnist.train.next_batch(50)</span><br><span class="line">      if i%100 &#x3D;&#x3D; 0:</span><br><span class="line">        # 评估模型准确度，此阶段不使用Dropout</span><br><span class="line">        train_accuracy &#x3D; accuracy.eval(feed_dict&#x3D;&#123;x:batch[0], y_: batch[1], keep_prob: 1.0&#125;)</span><br><span class="line">        print(&quot;step %d, training accuracy %g&quot;%(i, train_accuracy))</span><br><span class="line"></span><br><span class="line">      # 训练模型，此阶段使用50%的Dropout</span><br><span class="line">      train_step.run(feed_dict&#x3D;&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)</span><br><span class="line"></span><br><span class="line">    saver.save(sess, &#39;.&#x2F;save&#x2F;model.ckpt&#39;) #模型储存位置</span><br><span class="line"></span><br><span class="line">    print(&quot;test accuracy %g&quot;%accuracy.eval(feed_dict&#x3D;&#123;x: mnist.test.images [0:2000], y_: mnist.test.labels [0:2000], keep_prob: 1.0&#125;))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;人工智能——mnist（手写识别）&quot;&gt;&lt;a href=&quot;#人工智能——mnist（手写识别）&quot; class=&quot;headerlink&quot; title=&quot;人工智能——mnist（手写识别）&quot;&gt;&lt;/a&gt;人工智能——mnist（手写识别）&lt;/h2&gt;&lt;p&gt;因为时间较久了，就不
      
    
    </summary>
    
    
      <category term="人工智能" scheme="http://ailous.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="手写识别" scheme="http://ailous.top/tags/%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB/"/>
    
      <category term="mnist" scheme="http://ailous.top/tags/mnist/"/>
    
  </entry>
  
  <entry>
    <title>风格迁移——代码部分</title>
    <link href="http://ailous.top/2018/10/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%89%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86%20(1)/"/>
    <id>http://ailous.top/2018/10/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%89%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86%20(1)/</id>
    <published>2018-10-29T05:47:40.000Z</published>
    <updated>2020-04-14T06:03:07.247Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络：（三）风格迁移——代码部分"><a href="#卷积神经网络：（三）风格迁移——代码部分" class="headerlink" title="卷积神经网络：（三）风格迁移——代码部分"></a>卷积神经网络：（三）风格迁移——代码部分</h1><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h4 id="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。"><a href="#本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。" class="headerlink" title="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。"></a>本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。</h4><h5 id="友情提示：风格迁移跑的时间会很长。有点耐心哦。"><a href="#友情提示：风格迁移跑的时间会很长。有点耐心哦。" class="headerlink" title="友情提示：风格迁移跑的时间会很长。有点耐心哦。"></a>友情提示：风格迁移跑的时间会很长。有点耐心哦。</h5><h5 id="若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，"><a href="#若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，" class="headerlink" title="若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，"></a>若想查看环境配置步骤，请点击<a href="https://blog.csdn.net/weixin_41108515/article/details/103636284" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103636284</a>，</h5><h5 id="想知道原理，请点击https-blog-csdn-net-weixin-41108515-article-details-103650964"><a href="#想知道原理，请点击https-blog-csdn-net-weixin-41108515-article-details-103650964" class="headerlink" title="想知道原理，请点击https://blog.csdn.net/weixin_41108515/article/details/103650964"></a>想知道原理，请点击<a href="https://blog.csdn.net/weixin_41108515/article/details/103650964" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103650964</a></h5><p>&nbsp;<br>转载请注明出处：<a href="https://blog.csdn.net/weixin_41108515/article/details/103651784" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103651784</a><br>&nbsp;<br>这里引用的是：<br><a href="https://blog.csdn.net/aaronjny/article/details/79681080" target="_blank" rel="noopener">https://blog.csdn.net/aaronjny/article/details/79681080</a><br><a href="http://zh.gluon.ai/chapter_computer-vision/neural-style.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_computer-vision/neural-style.html</a><br>这两篇都非常详细，并且经调试可以使用，但是第二个并未使用tensorflow。<br>以及理论帮助的一篇<a href="https://juejin.im/post/5d29e818e51d454f73356de0" target="_blank" rel="noopener">https://juejin.im/post/5d29e818e51d454f73356de0</a></p><h1 id="第一部分-：简介"><a href="#第一部分-：简介" class="headerlink" title="第一部分 ：简介"></a>第一部分 ：简介</h1><h4 id="主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。"><a href="#主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。" class="headerlink" title="主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。"></a>主要操作以这部分为主，这篇引用的是<a href="https://blog.csdn.net/aaronjny/article/details/79681080" target="_blank" rel="noopener">tensorflow练手项目三</a>。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。</h4><h4 id="所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画"><a href="#所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画" class="headerlink" title="所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画"></a>所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画<img src="https://img-blog.csdnimg.cn/20191222152716368.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting-jpg），数据集将存放在百度网盘。"><a href="#风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting-jpg），数据集将存放在百度网盘。" class="headerlink" title="风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting.jpg），数据集将存放在百度网盘。"></a>风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting.jpg），数据集将存放在<a href="https://pan.baidu.com/s/1lTWsIRpMCxkopcsqUfdMFw" target="_blank" rel="noopener">百度网盘</a>。</h4><h4 id="content文件选取的是qd-jpg，在content文件夹下。"><a href="#content文件选取的是qd-jpg，在content文件夹下。" class="headerlink" title="content文件选取的是qd.jpg，在content文件夹下。"></a>content文件选取的是qd.jpg，在content文件夹下。</h4><p><img src="https://img-blog.csdnimg.cn/20191222152726511.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>最终实现效果如下。</p><p><img src="https://img-blog.csdnimg.cn/20191222152856860.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="第二部分-：操作"><a href="#第二部分-：操作" class="headerlink" title="第二部分 ：操作"></a>第二部分 ：操作</h1><h2 id="1-获取模型"><a href="#1-获取模型" class="headerlink" title="1.获取模型"></a>1.获取模型</h2><h4 id="VGG是Visual-Geometry-Group-这个实验室发明的，VGG是在2014年的-ILSVRC-localization-and-classification-两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max-pooling。"><a href="#VGG是Visual-Geometry-Group-这个实验室发明的，VGG是在2014年的-ILSVRC-localization-and-classification-两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max-pooling。" class="headerlink" title="VGG是Visual Geometry Group 这个实验室发明的，VGG是在2014年的 ILSVRC localization and classification 两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。"></a>VGG是Visual Geometry Group 这个实验室发明的，VGG是在2014年的 ILSVRC localization and classification 两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。</h4><h4 id="选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。"><a href="#选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。" class="headerlink" title="选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。"></a>选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。</h4><h4 id="CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。"><a href="#CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。" class="headerlink" title="CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。"></a>CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。</h4><p>注： 预训练好的VGG19模型可以从<a href="http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat" target="_blank" rel="noopener">http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat</a>下载，下载较慢的话，网盘<a href="https://pan.baidu.com/s/1uFinsEArbrgYRc2FWY9zVw" target="_blank" rel="noopener">https://pan.baidu.com/s/1uFinsEArbrgYRc2FWY9zVw</a>：。</p><h2 id="2-模型修改"><a href="#2-模型修改" class="headerlink" title="2.模型修改"></a>2.模型修改</h2><h4 id="这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。"><a href="#这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。" class="headerlink" title="这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。"></a>这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。</h4><h4 id="现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。"><a href="#现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。" class="headerlink" title="现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。"></a>现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。</h4><h4 id="风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。"><a href="#风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。" class="headerlink" title="风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。"></a>风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。</h4><p>理论上简单理解了，开始操作。</p><h4 id="这里建立py文件-models-py，下面内容我会写在注释里。"><a href="#这里建立py文件-models-py，下面内容我会写在注释里。" class="headerlink" title="这里建立py文件 models.py，下面内容我会写在注释里。"></a>这里建立py文件 models.py，下面内容我会写在注释里。</h4><pre><code># 导入必须的包import tensorflow as tfimport numpy as npimport settingsimport scipy.ioimport scipy.miscclass Model(object):    def __init__(self, content_path, style_path):        self.content = self.loadimg(content_path)  # 加载内容图片        self.style = self.loadimg(style_path)  # 加载风格图片        self.random_img = self.get_random_img()  # 生成噪音内容图片        self.net = self.vggnet()  # 建立vgg网络    def vggnet(self):            # 读取预训练的vgg模型            # 这里装的是misc，安装opencv的也亦可以使用opencv等其他方法        vgg = scipy.io.loadmat(settings.VGG_MODEL_PATH)         vgg_layers = vgg[&apos;layers&apos;][0]        net = {}            # 使用预训练的模型参数构建vgg网络的卷积层和池化层        # 全连接层不需要        # 注意，除了input之外，这里参数都为常量，不训练vgg的参数（权重比），这个以及训练完不需调整。        # 需要进行训练的是input，它即是我们最终生成的图像        net[&apos;input&apos;] = tf.Variable(np.zeros([1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3]), dtype=tf.float32)        # 参数对应的层数可以参考vgg模型图        net[&apos;conv1_1&apos;] = self.conv_relu(net[&apos;input&apos;], self.get_wb(vgg_layers, 0))        net[&apos;conv1_2&apos;] = self.conv_relu(net[&apos;conv1_1&apos;], self.get_wb(vgg_layers, 2))        net[&apos;pool1&apos;] = self.pool(net[&apos;conv1_2&apos;])        net[&apos;conv2_1&apos;] = self.conv_relu(net[&apos;pool1&apos;], self.get_wb(vgg_layers, 5))        net[&apos;conv2_2&apos;] = self.conv_relu(net[&apos;conv2_1&apos;], self.get_wb(vgg_layers, 7))        net[&apos;pool2&apos;] = self.pool(net[&apos;conv2_2&apos;])        net[&apos;conv3_1&apos;] = self.conv_relu(net[&apos;pool2&apos;], self.get_wb(vgg_layers, 10))        net[&apos;conv3_2&apos;] = self.conv_relu(net[&apos;conv3_1&apos;], self.get_wb(vgg_layers, 12))        net[&apos;conv3_3&apos;] = self.conv_relu(net[&apos;conv3_2&apos;], self.get_wb(vgg_layers, 14))        net[&apos;conv3_4&apos;] = self.conv_relu(net[&apos;conv3_3&apos;], self.get_wb(vgg_layers, 16))        net[&apos;pool3&apos;] = self.pool(net[&apos;conv3_4&apos;])        net[&apos;conv4_1&apos;] = self.conv_relu(net[&apos;pool3&apos;], self.get_wb(vgg_layers, 19))           net[&apos;conv4_2&apos;] = self.conv_relu(net[&apos;conv4_1&apos;], self.get_wb(vgg_layers, 21))        net[&apos;conv4_3&apos;] = self.conv_relu(net[&apos;conv4_2&apos;], self.get_wb(vgg_layers, 23))        net[&apos;conv4_4&apos;] = self.conv_relu(net[&apos;conv4_3&apos;], self.get_wb(vgg_layers, 25))        net[&apos;pool4&apos;] = self.pool(net[&apos;conv4_4&apos;])        net[&apos;conv5_1&apos;] = self.conv_relu(net[&apos;pool4&apos;], self.get_wb(vgg_layers, 28))        net[&apos;conv5_2&apos;] = self.conv_relu(net[&apos;conv5_1&apos;], self.get_wb(vgg_layers, 30))        net[&apos;conv5_3&apos;] = self.conv_relu(net[&apos;conv5_2&apos;], self.get_wb(vgg_layers, 32))        net[&apos;conv5_4&apos;] = self.conv_relu(net[&apos;conv5_3&apos;], self.get_wb(vgg_layers, 34))        net[&apos;pool5&apos;] = self.pool(net[&apos;conv5_4&apos;])        return net    def conv_relu(self, input, wb):        &quot;&quot;&quot;        进行先卷积、后relu的运算        :param input: 输入层        :param wb: wb[0],wb[1] == w,b        :return: relu后的结果        &quot;&quot;&quot;        conv = tf.nn.conv2d(input, wb[0], strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)        relu = tf.nn.relu(conv + wb[1])        return relu    def pool(self, input):        &quot;&quot;&quot;        进行max_pool操作        :param input: 输入层        :return: 池化后的结果        &quot;&quot;&quot;        return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)    def get_wb(self, layers, i):        &quot;&quot;&quot;        从预训练好的vgg模型中读取参数        :param layers: 训练好的vgg模型        :param i: vgg指定层数        :return: 该层的w,b        &quot;&quot;&quot;        w = tf.constant(layers[i][0][0][0][0][0])        bias = layers[i][0][0][0][0][1]        b = tf.constant(np.reshape(bias, (bias.size)))        return w, b    def get_random_img(self):        &quot;&quot;&quot;        根据噪音和内容图片，生成一张随机图片        :return:        &quot;&quot;&quot;        noise_image = np.random.uniform(-20, 20, [1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3])        random_img = noise_image * settings.NOISE + self.content * (1 - settings.NOISE)        return random_img    def loadimg(self, path):        &quot;&quot;&quot;        加载一张图片，将其转化为符合要求的格式        :param path:        :return:        &quot;&quot;&quot;        # 读取图片        image = scipy.misc.imread(path)        # 重新设定图片大小        image = scipy.misc.imresize(image, [settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH])        # 改变数组形状，其实就是把它变成一个batch_size=1的batch        image = np.reshape(image, (1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3))        # 减去均值，使其数据分布接近0        image = image - settings.IMAGE_MEAN_VALUE        return imageif __name__ == &apos;__main__&apos;:    Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE)</code></pre><h2 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3.模型训练"></a>3.模型训练</h2><h4 id="但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。"><a href="#但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。" class="headerlink" title="但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。"></a>但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。</h4><h4 id="就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。"><a href="#就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。" class="headerlink" title="就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。"></a>就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。</h4><h4 id="这里建立py文件-train-py，下面内容我会写在注释里。"><a href="#这里建立py文件-train-py，下面内容我会写在注释里。" class="headerlink" title="这里建立py文件 train.py，下面内容我会写在注释里。"></a>这里建立py文件 train.py，下面内容我会写在注释里。</h4><pre><code># -*- coding: utf-8 -*-import tensorflow as tfimport settingsimport modelsimport numpy as npimport scipy.miscdef loss(sess, model):    &quot;&quot;&quot;    定义模型的损失函数    :param sess: tf session    :param model: 神经网络模型    :return: 内容损失和风格损失的加权和损失    &quot;&quot;&quot;    # 先计算内容损失函数    # 获取定义内容损失的vgg层名称列表及权重    content_layers = settings.CONTENT_LOSS_LAYERS    # 将内容图片作为输入，方便后面提取内容图片在各层中的特征矩阵    sess.run(tf.assign(model.net[&apos;input&apos;], model.content))    # 内容损失累加量    content_loss = 0.0    # 逐个取出衡量内容损失的vgg层名称及对应权重    for layer_name, weight in content_layers:        # 提取内容图片在layer_name层中的特征矩阵        p = sess.run(model.net[layer_name])        # 提取噪音图片在layer_name层中的特征矩阵        x = model.net[layer_name]        # 长x宽        M = p.shape[1] * p.shape[2]        # 信道数        N = p.shape[3]        # 根据公式计算损失，并进行累加        content_loss += (1.0 / (2 * M * N)) * tf.reduce_sum(tf.pow(p - x, 2)) * weight    # 将损失对层数取平均    content_loss /= len(content_layers)    # 再计算风格损失函数    style_layers = settings.STYLE_LOSS_LAYERS    # 将风格图片作为输入，方便后面提取风格图片在各层中的特征矩阵    sess.run(tf.assign(model.net[&apos;input&apos;], model.style))    # 风格损失累加量    style_loss = 0.0    # 逐个取出衡量风格损失的vgg层名称及对应权重    for layer_name, weight in style_layers:        # 提取风格图片在layer_name层中的特征矩阵        a = sess.run(model.net[layer_name])        # 提取噪音图片在layer_name层中的特征矩阵        x = model.net[layer_name]        # 长x宽        M = a.shape[1] * a.shape[2]        # 信道数        N = a.shape[3]        # 求风格图片特征的gram矩阵        A = gram(a, M, N)        # 求噪音图片特征的gram矩阵        G = gram(x, M, N)        # 根据公式计算损失，并进行累加        style_loss += (1.0 / (4 * M * M * N * N)) * tf.reduce_sum(tf.pow(G - A, 2)) * weight    # 将损失对层数取平均    style_loss /= len(style_layers)    # 将内容损失和风格损失加权求和，构成总损失函数    loss = settings.ALPHA * content_loss + settings.BETA * style_loss    return lossdef gram(x, size, deep):    &quot;&quot;&quot;    创建给定矩阵的格莱姆矩阵，用来衡量风格    :param x:给定矩阵    :param size:矩阵的行数与列数的乘积    :param deep:矩阵信道数    :return:格莱姆矩阵    &quot;&quot;&quot;    # 改变shape为（size,deep）    x = tf.reshape(x, (size, deep))    # 求xTx    g = tf.matmul(tf.transpose(x), x)    return gdef train():    # 创建一个模型    model = models.Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE)    # 创建session    with tf.Session() as sess:        # 全局初始化        sess.run(tf.global_variables_initializer())        # 定义损失函数        cost = loss(sess, model)        # 创建优化器        optimizer = tf.train.AdamOptimizer(1.0).minimize(cost)        # 再初始化一次（主要针对于第一次初始化后又定义的运算，不然可能会报错）        sess.run(tf.global_variables_initializer())        # 使用噪声图片进行训练        sess.run(tf.assign(model.net[&apos;input&apos;], model.random_img))        # 迭代指定次数        for step in range(settings.TRAIN_STEPS):            # 进行一次反向传播            sess.run(optimizer)            # 每隔一定次数，输出一下进度，并保存当前训练结果            if step % 50 == 0:                print(&apos;step {} is down.&apos;.format(step))                # 取出input的内容，这是生成的图片                img = sess.run(model.net[&apos;input&apos;])                # 训练过程是减去均值的，这里要加上                img += settings.IMAGE_MEAN_VALUE                # 这里是一个batch_size=1的batch，所以img[0]才是图片内容                img = img[0]                # 将像素值限定在0-255，并转为整型                img = np.clip(img, 0, 255).astype(np.uint8)                # 保存图片                scipy.misc.imsave(&apos;{}-{}.jpg&apos;.format(settings.OUTPUT_IMAGE,step), img)        # 保存最终训练结果        img = sess.run(model.net[&apos;input&apos;])        img += settings.IMAGE_MEAN_VALUE        img = img[0]        img = np.clip(img, 0, 255).astype(np.uint8)        scipy.misc.imsave(&apos;{}.jpg&apos;.format(settings.OUTPUT_IMAGE), img)if __name__ == &apos;__main__&apos;:    train()</code></pre><h2 id="4-系统文件配置这里建立py文件-setting-py。"><a href="#4-系统文件配置这里建立py文件-setting-py。" class="headerlink" title="4.系统文件配置这里建立py文件 setting.py。"></a>4.系统文件配置这里建立py文件 setting.py。</h2><pre><code># -*- coding: utf-8 -*-# 内容图片路径CONTENT_IMAGE = &apos;content/qd.jpg&apos; # 路径/图片 自己在工程文件夹下建立。# 风格图片路径STYLE_IMAGE = &apos;style/painting.jpg&apos; # 路径/图片 自己在工程文件夹下建立。# 输出图片路径OUTPUT_IMAGE = &apos;output/output&apos; # 路径/图片开始名 自己在工程文件夹下建立。# 预训练的vgg模型路径VGG_MODEL_PATH = &apos;imagenet-vgg-verydeep-19.mat&apos; # 直接置于工程文件夹即可。# 图片宽度IMAGE_WIDTH = 450# 图片高度IMAGE_HEIGHT = 300# 定义计算内容损失的vgg层名称及对应权重的列表CONTENT_LOSS_LAYERS = [(&apos;conv4_2&apos;, 0.5),(&apos;conv5_2&apos;,0.5)]# 定义计算风格损失的vgg层名称及对应权重的列表STYLE_LOSS_LAYERS = [(&apos;conv1_1&apos;, 0.2), (&apos;conv2_1&apos;, 0.2), (&apos;conv3_1&apos;, 0.2), (&apos;conv4_1&apos;, 0.2), (&apos;conv5_1&apos;, 0.2)]# 噪音比率NOISE = 0.5# 图片RGB均值IMAGE_MEAN_VALUE = [128.0, 128.0, 128.0]# 内容损失权重ALPHA = 1# 风格损失权重BETA = 500# 训练次数TRAIN_STEPS = 3000  # 这里推荐几百次就行，确实时间太长。</code></pre><h2 id="5-总结："><a href="#5-总结：" class="headerlink" title="5.总结："></a>5.总结：</h2><p>这是第一次写博客，有不正确的地方还望指点，做出来的效果还是一般。之前做过网页版，但是因为跑的时间太长，效果不好。后期有时间出一篇网页版的风格迁移。源码数据 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络：（三）风格迁移——代码部分&quot;&gt;&lt;a href=&quot;#卷积神经网络：（三）风格迁移——代码部分&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络：（三）风格迁移——代码部分&quot;&gt;&lt;/a&gt;卷积神经网络：（三）风格迁移——代码部分&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
      <category term="人工智能" scheme="http://ailous.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="卷积神经网络" scheme="http://ailous.top/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="风格迁移" scheme="http://ailous.top/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>风格迁移——原理部分</title>
    <link href="http://ailous.top/2018/10/28/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%BA%8C%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/"/>
    <id>http://ailous.top/2018/10/28/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%BA%8C%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/</id>
    <published>2018-10-28T05:47:40.000Z</published>
    <updated>2020-04-14T06:03:00.771Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络：（二）风格迁移——原理部分"><a href="#卷积神经网络：（二）风格迁移——原理部分" class="headerlink" title="卷积神经网络：（二）风格迁移——原理部分"></a>卷积神经网络：（二）风格迁移——原理部分</h1><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h4 id="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。"><a href="#本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https-blog-csdn-net-weixin-41108515-article-details-103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。" class="headerlink" title="本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。"></a>本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击<a href="https://blog.csdn.net/weixin_41108515/article/details/103636284" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103636284</a>，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。</h4><p>&nbsp;<br>转载请注明出处：<a href="https://blog.csdn.net/weixin_41108515/article/details/103650964" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103650964</a><br>&nbsp;<br>这里引用的是：<br><a href="http://zh.gluon.ai/chapter_computer-vision/neural-style.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_computer-vision/neural-style.html</a><br><a href="https://blog.csdn.net/aaronjny/article/details/79681080" target="_blank" rel="noopener">https://blog.csdn.net/aaronjny/article/details/79681080</a><br>这两篇都非常详细，并且经调试可以使用。</p><h1 id="涉及到的相关原理："><a href="#涉及到的相关原理：" class="headerlink" title="涉及到的相关原理："></a>涉及到的相关原理：</h1><h1 id="1、神经网络部分原理："><a href="#1、神经网络部分原理：" class="headerlink" title="1、神经网络部分原理："></a>1、神经网络部分原理：</h1><h2 id="1-1-神经网络基础介绍"><a href="#1-1-神经网络基础介绍" class="headerlink" title="1.1 神经网络基础介绍"></a>1.1 神经网络基础介绍</h2><h4 id="神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。"><a href="#神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。" class="headerlink" title="神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。"></a>神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。</h4><h4 id="生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。"><a href="#生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。" class="headerlink" title="生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。"></a>生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。<img src="https://img-blog.csdnimg.cn/2019122210122665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。</h4><h4 id="神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面："><a href="#神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：" class="headerlink" title="神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面："></a>神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：</h4><h5 id="（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。"><a href="#（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。" class="headerlink" title="（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。"></a>（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。</h5><h5 id="（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。"><a href="#（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。" class="headerlink" title="（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。"></a>（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。</h5><h5 id="（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。"><a href="#（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。" class="headerlink" title="（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。"></a>（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。</h5><h5 id="（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。"><a href="#（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。" class="headerlink" title="（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。"></a>（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。</h5><h4 id="纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。"><a href="#纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。" class="headerlink" title="纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。"></a>纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。<img src="https://img-blog.csdnimg.cn/20191222101416373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h2 id="1-2-卷积神经网络基本结构"><a href="#1-2-卷积神经网络基本结构" class="headerlink" title="1.2 卷积神经网络基本结构"></a>1.2 卷积神经网络基本结构</h2><h4 id="卷积神经网络-Convolutional-Neural-Network，CNN-是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注-16-。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络-Convolutional-Neural-Networks简称CNN-的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。"><a href="#卷积神经网络-Convolutional-Neural-Network，CNN-是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注-16-。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络-Convolutional-Neural-Networks简称CNN-的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。" class="headerlink" title="卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。"></a>卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。</h4><h3 id="1-2-1-输入层"><a href="#1-2-1-输入层" class="headerlink" title="1.2.1 输入层"></a>1.2.1 输入层</h3><h4 id="卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。"><a href="#卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。" class="headerlink" title="卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。"></a>卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。</h4><h3 id="1-2-2-隐含层"><a href="#1-2-2-隐含层" class="headerlink" title="1.2.2 隐含层"></a>1.2.2 隐含层</h3><h4 id="1-卷积层"><a href="#1-卷积层" class="headerlink" title="1.卷积层"></a>1.卷积层</h4><h5 id="（1）卷积层"><a href="#（1）卷积层" class="headerlink" title="（1）卷积层"></a>（1）卷积层</h5><h4 id="利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度-即颜色的三原色，以RGB表示-。"><a href="#利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度-即颜色的三原色，以RGB表示-。" class="headerlink" title="利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。"></a>利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。<img src="https://img-blog.csdnimg.cn/20191222102217444.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。"><a href="#如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。" class="headerlink" title="如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。"></a>如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。</h4><p><img src="https://img-blog.csdnimg.cn/20191222102503552.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="（2）卷积层参数"><a href="#（2）卷积层参数" class="headerlink" title="（2）卷积层参数"></a>（2）卷积层参数</h5><h4 id="卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。"><a href="#卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。" class="headerlink" title="卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。"></a>卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。</h4><h4 id="卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。"><a href="#卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。" class="headerlink" title="卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。"></a>卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。</h4><h4 id="由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类："><a href="#由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：" class="headerlink" title="由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类："></a>由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：</h4><h5 id="有效填充（valid-padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为-L-f-s-1。"><a href="#有效填充（valid-padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为-L-f-s-1。" class="headerlink" title="有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。"></a>有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。</h5><h5 id="相同填充-半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。"><a href="#相同填充-半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。" class="headerlink" title="相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。"></a>相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。</h5><h5 id="全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L-f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。"><a href="#全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L-f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。" class="headerlink" title="全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。"></a>全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。</h5><h5 id="任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。"><a href="#任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。" class="headerlink" title="任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。"></a>任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。</h5><h5 id="（3）激励函数"><a href="#（3）激励函数" class="headerlink" title="（3）激励函数"></a>（3）激励函数</h5><h4 id="一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质："><a href="#一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：" class="headerlink" title="一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质："></a>一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：</h4><h5 id="1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。"><a href="#1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。" class="headerlink" title="1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。"></a>1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。</h5><h5 id="2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。"><a href="#2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。" class="headerlink" title="2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。"></a>2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。</h5><h5 id="3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。"><a href="#3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。" class="headerlink" title="3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。"></a>3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。</h5><h4 id="经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层-比较常见，后者ReLU常见于卷积层。"><a href="#经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层-比较常见，后者ReLU常见于卷积层。" class="headerlink" title="经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。"></a>经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。</h4><h4 id="2-池化层"><a href="#2-池化层" class="headerlink" title="2.池化层"></a>2.池化层</h4><h4 id="池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化："><a href="#池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：" class="headerlink" title="池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化："></a>池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：</h4><h5 id="（1）一般池化-General-Pooling"><a href="#（1）一般池化-General-Pooling" class="headerlink" title="（1）一般池化(General Pooling)"></a>（1）一般池化(General Pooling)</h5><h5 id="1）mean-pooling，即只要求邻域中特征点的平均值；"><a href="#1）mean-pooling，即只要求邻域中特征点的平均值；" class="headerlink" title="1）mean-pooling，即只要求邻域中特征点的平均值；"></a>1）mean-pooling，即只要求邻域中特征点的平均值；</h5><h5 id="2）max-pooling，即在邻域中提取最大特征点；"><a href="#2）max-pooling，即在邻域中提取最大特征点；" class="headerlink" title="2）max-pooling，即在邻域中提取最大特征点；"></a>2）max-pooling，即在邻域中提取最大特征点；</h5><h5 id="3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。"><a href="#3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。" class="headerlink" title="3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。"></a>3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。</h5><h4 id="特征提取的误差主要来自两个方面："><a href="#特征提取的误差主要来自两个方面：" class="headerlink" title="特征提取的误差主要来自两个方面："></a>特征提取的误差主要来自两个方面：</h4><h5 id="1）邻域大小受限造成的估计值方差增大；"><a href="#1）邻域大小受限造成的估计值方差增大；" class="headerlink" title="1）邻域大小受限造成的估计值方差增大；"></a>1）邻域大小受限造成的估计值方差增大；</h5><h5 id="2）卷积层参数误差导致估计均值的偏移。"><a href="#2）卷积层参数误差导致估计均值的偏移。" class="headerlink" title="2）卷积层参数误差导致估计均值的偏移。"></a>2）卷积层参数误差导致估计均值的偏移。</h5><h4 id="一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean"><a href="#一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean" class="headerlink" title="一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-"></a>一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-</h4><h4 id="pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，"><a href="#pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，" class="headerlink" title="pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，"></a>pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示，</h4><p><img src="https://img-blog.csdnimg.cn/20191222102557759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="（2）空间金字塔池化-Spatial-pyramid-pooling"><a href="#（2）空间金字塔池化-Spatial-pyramid-pooling" class="headerlink" title="（2）空间金字塔池化(Spatial pyramid pooling)"></a>（2）空间金字塔池化(Spatial pyramid pooling)</h5><h4 id="一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。"><a href="#一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。" class="headerlink" title="一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。"></a>一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。</h4><h4 id="空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。"><a href="#空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。" class="headerlink" title="空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。"></a>空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。</h4><h4 id="3-全连接层"><a href="#3-全连接层" class="headerlink" title="3.全连接层"></a>3.全连接层</h4><h4 id="卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。"><a href="#卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。" class="headerlink" title="卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。"></a>卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。</h4><h3 id="1-2-3-输出层"><a href="#1-2-3-输出层" class="headerlink" title="1.2.3 输出层"></a>1.2.3 输出层</h3><h4 id="卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。"><a href="#卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。" class="headerlink" title="卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。"></a>卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。</h4><h2 id="1-3-卷积神经网络的卷积过程"><a href="#1-3-卷积神经网络的卷积过程" class="headerlink" title="1.3 卷积神经网络的卷积过程"></a>1.3 卷积神经网络的卷积过程</h2><h4 id="卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层-convolutional-layer-、池化层-pooling-layer-、全连接层-fully-connected-layer-。"><a href="#卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层-convolutional-layer-、池化层-pooling-layer-、全连接层-fully-connected-layer-。" class="headerlink" title="卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。"></a>卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。</h4><p><img src="https://img-blog.csdnimg.cn/20191222102628441.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="图中的卷积网络工作流程如下，-输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述："><a href="#图中的卷积网络工作流程如下，-输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：" class="headerlink" title="图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述："></a>图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：</h4><h4 id="第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5-的接受域。"><a href="#第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5-的接受域。" class="headerlink" title="第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。"></a>第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。</h4><h4 id="第二隐藏层实现子采样和局部平均，它同样由-6个特征图组成，但其每个特征图由14×14-个神经元组成。每个神经元具有2×2-的接受域。"><a href="#第二隐藏层实现子采样和局部平均，它同样由-6个特征图组成，但其每个特征图由14×14-个神经元组成。每个神经元具有2×2-的接受域。" class="headerlink" title="第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。"></a>第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。</h4><h4 id="第三隐藏层进行第二次卷积，它由-16个特征图组成，每个特征图由-10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。"><a href="#第三隐藏层进行第二次卷积，它由-16个特征图组成，每个特征图由-10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。" class="headerlink" title="第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。"></a>第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。</h4><h4 id="第四个隐藏层进行第二次子采样和局部平均计算。它由-16个特征图组成，但每个特征图由-5×5个神经元构成，它以与第一次采样相同的方式进行工作。"><a href="#第四个隐藏层进行第二次子采样和局部平均计算。它由-16个特征图组成，但每个特征图由-5×5个神经元构成，它以与第一次采样相同的方式进行工作。" class="headerlink" title="第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。"></a>第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。</h4><h4 id="第五个隐藏层实现了卷积的最后阶段，它由-120个神经元组成，每个神经元指定5×5-的接受域。"><a href="#第五个隐藏层实现了卷积的最后阶段，它由-120个神经元组成，每个神经元指定5×5-的接受域。" class="headerlink" title="第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。"></a>第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。</h4><h4 id="端部是个全连接层，得到输出向量。"><a href="#端部是个全连接层，得到输出向量。" class="headerlink" title="端部是个全连接层，得到输出向量。"></a>端部是个全连接层，得到输出向量。</h4><h4 id="卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加-17-。"><a href="#卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加-17-。" class="headerlink" title="卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。"></a>卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。</h4><h4 id="卷积层研究输入数据的特征。卷积层由卷积核-convolutional-kernel-组成，卷积核用来计算不同的特征图；激励函数-activation-function-给卷积神经网络引入了非线性，常用的有sigmid、tanh、-ReLU函数；池化层减少了卷积层输出的特征向量，改良结果-使结构不易过拟合-，典型应用有average-pooling-和-max-pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。"><a href="#卷积层研究输入数据的特征。卷积层由卷积核-convolutional-kernel-组成，卷积核用来计算不同的特征图；激励函数-activation-function-给卷积神经网络引入了非线性，常用的有sigmid、tanh、-ReLU函数；池化层减少了卷积层输出的特征向量，改良结果-使结构不易过拟合-，典型应用有average-pooling-和-max-pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。" class="headerlink" title="卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。"></a>卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。</h4><h1 id="2、迁移学习相关原理"><a href="#2、迁移学习相关原理" class="headerlink" title="2、迁移学习相关原理"></a>2、迁移学习相关原理</h1><h2 id="2-1-迁移学习"><a href="#2-1-迁移学习" class="headerlink" title="2.1 迁移学习"></a>2.1 迁移学习</h2><h4 id="在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception-V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。"><a href="#在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception-V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。" class="headerlink" title="在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。"></a>在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。</h4><h2 id="2-2TensorFlow"><a href="#2-2TensorFlow" class="headerlink" title="2.2TensorFlow"></a>2.2TensorFlow</h2><h4 id="TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor-张量-意味着N维数组，Flow-流-意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。"><a href="#TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor-张量-意味着N维数组，Flow-流-意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。" class="headerlink" title="TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。"></a>TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。</h4><h4 id="TensorFlow-表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU-GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。"><a href="#TensorFlow-表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU-GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。" class="headerlink" title="TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。"></a>TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。</h4><h4 id="TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。"><a href="#TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。" class="headerlink" title="TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。"></a>TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。</h4><h5 id="（1）支持多种硬件的平台"><a href="#（1）支持多种硬件的平台" class="headerlink" title="（1）支持多种硬件的平台"></a>（1）支持多种硬件的平台</h5><h4 id="例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。"><a href="#例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。" class="headerlink" title="例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。"></a>例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。</h4><h5 id="（2）支持多种开发环境"><a href="#（2）支持多种开发环境" class="headerlink" title="（2）支持多种开发环境"></a>（2）支持多种开发环境</h5><h4 id="支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。"><a href="#支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。" class="headerlink" title="支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。"></a>支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。</h4><h4 id="目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。"><a href="#目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。" class="headerlink" title="目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。"></a>目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。</h4><h2 id="2-3VGG卷积神经网络模型"><a href="#2-3VGG卷积神经网络模型" class="headerlink" title="2.3VGG卷积神经网络模型"></a>2.3VGG卷积神经网络模型</h2><h4 id="VGG全称是Visual-Geometry-Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19-20-。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为-GG-Very-Deep-16-CNN-，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层-全连接层总数目的不同可以从VGG11-～-VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层-3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11-～GVV19的结构图："><a href="#VGG全称是Visual-Geometry-Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19-20-。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为-GG-Very-Deep-16-CNN-，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层-全连接层总数目的不同可以从VGG11-～-VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层-3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11-～GVV19的结构图：" class="headerlink" title="VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图："></a>VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图：</h4><p><img src="https://img-blog.csdnimg.cn/20191222102708425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3-stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。"><a href="#在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3-stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3-stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。" class="headerlink" title="在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。"></a>在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层3<em>3卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层1</em>1的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层3<em>3的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加3</em>3的卷积层，一共19层。模型E就是VGG19网络。</h4><h1 id="3、通过VGG实现风格迁移"><a href="#3、通过VGG实现风格迁移" class="headerlink" title="3、通过VGG实现风格迁移"></a>3、通过VGG实现风格迁移</h1><h2 id="3-1-图像风格迁移的原理"><a href="#3-1-图像风格迁移的原理" class="headerlink" title="3.1 图像风格迁移的原理"></a>3.1 图像风格迁移的原理</h2><h4 id="VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1-1-conv1-2-），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5-1-conv5-2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。"><a href="#VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1-1-conv1-2-），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5-1-conv5-2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。" class="headerlink" title="VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。"></a>VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。</h4><h4 id="VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示："><a href="#VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：" class="headerlink" title="VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示："></a>VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：</h4><p><img src="https://img-blog.csdnimg.cn/20191222104202202.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像-研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。"><a href="#具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像-研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。" class="headerlink" title="具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。"></a>具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。</h4><h2 id="3-2-代价函数"><a href="#3-2-代价函数" class="headerlink" title="3.2 代价函数"></a>3.2 代价函数</h2><h4 id="要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J-G-，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。"><a href="#要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J-G-，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。" class="headerlink" title="要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。"></a>要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。</h4><h4 id="Jcontent-C-G"><a href="#Jcontent-C-G" class="headerlink" title="Jcontent(C,G)"></a>Jcontent(C,G)</h4><h4 id="第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。"><a href="#第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。" class="headerlink" title="第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。"></a>第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。</h4><h4 id="Jstyle-S-G"><a href="#Jstyle-S-G" class="headerlink" title="Jstyle(S,G)"></a>Jstyle(S,G)</h4><h4 id="然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。"><a href="#然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。" class="headerlink" title="然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。"></a>然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。</h4><h4 id="J-G-αJcontent-C-G-βJstyle-S-G"><a href="#J-G-αJcontent-C-G-βJstyle-S-G" class="headerlink" title="J(G)=αJcontent(C,G)+βJstyle(S,G)"></a>J(G)=αJcontent(C,G)+βJstyle(S,G)</h4><h4 id="最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。"><a href="#最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。" class="headerlink" title="最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。"></a>最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。</h4><h2 id="3-3-内容代价函数"><a href="#3-3-内容代价函数" class="headerlink" title="3.3 内容代价函数"></a>3.3 内容代价函数</h2><h4 id="风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。"><a href="#风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。" class="headerlink" title="风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。"></a>风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。</h4><h4 id="首先定义内容代价部分。"><a href="#首先定义内容代价部分。" class="headerlink" title="首先定义内容代价部分。"></a>首先定义内容代价部分。</h4><h4 id="用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层-1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是-VGG-网络。"><a href="#用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层-1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是-VGG-网络。" class="headerlink" title="用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。"></a>用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。</h4><h4 id="之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α-L-C-和α-L-G-的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。"><a href="#之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α-L-C-和α-L-G-的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。" class="headerlink" title="之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。"></a>之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。</h4><p> <img src="https://img-blog.csdnimg.cn/20191222110732731.png" alt="在这里插入图片描述"></p><h4 id="为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1-2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。"><a href="#为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1-2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。" class="headerlink" title="为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。"></a>为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。</h4><h2 id="3-4-风格代价函数"><a href="#3-4-风格代价函数" class="headerlink" title="3.4 风格代价函数"></a>3.4 风格代价函数</h2><h4 id="图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。"><a href="#图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。" class="headerlink" title="图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。"></a>图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。</h4><h4 id="风格矩阵是一组向量的内积对称矩阵，比如向量组"><a href="#风格矩阵是一组向量的内积对称矩阵，比如向量组" class="headerlink" title="风格矩阵是一组向量的内积对称矩阵，比如向量组"></a>风格矩阵是一组向量的内积对称矩阵，比如向量组<img src="https://img-blog.csdnimg.cn/20191222110804515.png" alt="在这里插入图片描述"></h4><h4 id="的Gram矩阵是"><a href="#的Gram矩阵是" class="headerlink" title="的Gram矩阵是"></a>的Gram矩阵是</h4><p><img src="https://img-blog.csdnimg.cn/20191222111053849.png" alt="在这里插入图片描述"></p><h4 id="取内积即欧几里得空间上的标准内积，即"><a href="#取内积即欧几里得空间上的标准内积，即" class="headerlink" title="取内积即欧几里得空间上的标准内积，即"></a>取内积即欧几里得空间上的标准内积，即</h4><p> <img src="https://img-blog.csdnimg.cn/20191222111102313.png" alt="在这里插入图片描述"></p><h4 id="假设卷积层的输出为F-ij-l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为"><a href="#假设卷积层的输出为F-ij-l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为" class="headerlink" title="假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为"></a>假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为</h4><p><img src="https://img-blog.csdnimg.cn/20191222111118616.png" alt="在这里插入图片描述"></p><h4 id="设在第l层中，卷积特征的通道数为N-l-卷积的高、宽乘积数为M-l-那么F-ij-l满足"><a href="#设在第l层中，卷积特征的通道数为N-l-卷积的高、宽乘积数为M-l-那么F-ij-l满足" class="headerlink" title="设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足"></a>设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足</h4><h4 id="l≤i≤N-l，l≤j≤M-l"><a href="#l≤i≤N-l，l≤j≤M-l" class="headerlink" title="l≤i≤N_l，l≤j≤M_l"></a>l≤i≤N_l，l≤j≤M_l</h4><h4 id="Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。"><a href="#Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。" class="headerlink" title="Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。"></a>Gram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。</h4><h2 id="3-5-模型训练过程"><a href="#3-5-模型训练过程" class="headerlink" title="3.5 模型训练过程"></a>3.5 模型训练过程</h2><h4 id="首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。"><a href="#首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。" class="headerlink" title="首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。"></a>首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。</h4><h4 id="使用-‘conv4-2’-’conv5-2’-表示内容特征，使用-‘conv1-1’-’conv2-1’-’conv3-1’-’conv4-1’-表示风格特征。"><a href="#使用-‘conv4-2’-’conv5-2’-表示内容特征，使用-‘conv1-1’-’conv2-1’-’conv3-1’-’conv4-1’-表示风格特征。" class="headerlink" title="使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。"></a>使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。</h4><h4 id="将内容图片输入网络，计算内容图片在网络指定层上的输出值。"><a href="#将内容图片输入网络，计算内容图片在网络指定层上的输出值。" class="headerlink" title="将内容图片输入网络，计算内容图片在网络指定层上的输出值。"></a>将内容图片输入网络，计算内容图片在网络指定层上的输出值。</h4><h4 id="计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。"><a href="#计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。" class="headerlink" title="计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。"></a>计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。</h4><h4 id="对应每一层的内容损失函数："><a href="#对应每一层的内容损失函数：" class="headerlink" title="对应每一层的内容损失函数："></a>对应每一层的内容损失函数：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111244537.png" alt="在这里插入图片描述"></p><h4 id="其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长-宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。"><a href="#其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长-宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。" class="headerlink" title="其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。"></a>其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。</h4><h4 id="将风格图片输入网络，计算风格图片在网络指定层上的输出值。"><a href="#将风格图片输入网络，计算风格图片在网络指定层上的输出值。" class="headerlink" title="将风格图片输入网络，计算风格图片在网络指定层上的输出值。"></a>将风格图片输入网络，计算风格图片在网络指定层上的输出值。</h4><h4 id="计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。"><a href="#计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。" class="headerlink" title="计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。"></a>计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。</h4><h4 id="对于每一层的风格损失函数："><a href="#对于每一层的风格损失函数：" class="headerlink" title="对于每一层的风格损失函数："></a>对于每一层的风格损失函数：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111337543.png" alt="在这里插入图片描述"></p><h4 id="其中M是特征矩阵的长-宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。"><a href="#其中M是特征矩阵的长-宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。" class="headerlink" title="其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。"></a>其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。</h4><h4 id="最终的风格损失为，每一层的风格损失加权和，再对层数取平均。"><a href="#最终的风格损失为，每一层的风格损失加权和，再对层数取平均。" class="headerlink" title="最终的风格损失为，每一层的风格损失加权和，再对层数取平均。"></a>最终的风格损失为，每一层的风格损失加权和，再对层数取平均。</h4><h4 id="函数为内容损失和风格损失的加权和："><a href="#函数为内容损失和风格损失的加权和：" class="headerlink" title="函数为内容损失和风格损失的加权和："></a>函数为内容损失和风格损失的加权和：</h4><p><img src="https://img-blog.csdnimg.cn/20191222111355281.png" alt="在这里插入图片描述"></p><h4 id="当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。"><a href="#当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。" class="headerlink" title="当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。"></a>当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。</h4><p><img src="https://img-blog.csdnimg.cn/20191222104239449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！"><a href="#原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！" class="headerlink" title="原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！"></a>原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！</h2><p><a href="https://blog.csdn.net/weixin_41108515/article/details/103651784" target="_blank" rel="noopener">卷积神经网络：（三）风格迁移——代码部分</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络：（二）风格迁移——原理部分&quot;&gt;&lt;a href=&quot;#卷积神经网络：（二）风格迁移——原理部分&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络：（二）风格迁移——原理部分&quot;&gt;&lt;/a&gt;卷积神经网络：（二）风格迁移——原理部分&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
      <category term="人工智能" scheme="http://ailous.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="卷积神经网络" scheme="http://ailous.top/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="风格迁移" scheme="http://ailous.top/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>风格迁移——环境配置</title>
    <link href="http://ailous.top/2018/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%80%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%20(1)/"/>
    <id>http://ailous.top/2018/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%EF%BC%88%E4%B8%80%EF%BC%89%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%20(1)/</id>
    <published>2018-10-27T05:47:40.000Z</published>
    <updated>2020-04-14T06:02:26.385Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络：（一）风格迁移——环境配置"><a href="#卷积神经网络：（一）风格迁移——环境配置" class="headerlink" title="卷积神经网络：（一）风格迁移——环境配置"></a>卷积神经网络：（一）风格迁移——环境配置</h1><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h4 id="emsp-emsp-本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。"><a href="#emsp-emsp-本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。" class="headerlink" title="&emsp;&emsp;本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。"></a>&emsp;&emsp;本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。</h4><p>&nbsp;<br>转载请注明出处：<a href="https://blog.csdn.net/weixin_41108515/article/details/103636284" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103636284</a><br>&nbsp;</p><h1 id="第一步：搭建python环境："><a href="#第一步：搭建python环境：" class="headerlink" title="第一步：搭建python环境："></a>第一步：搭建python环境：</h1><h2 id="方法一-直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）"><a href="#方法一-直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）" class="headerlink" title="方法一 : 直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）"></a>方法一 : 直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）</h2><h4 id="emsp-emsp-直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3-6版本，3-7及以上版本目前不支持）"><a href="#emsp-emsp-直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3-6版本，3-7及以上版本目前不支持）" class="headerlink" title="&emsp;&emsp;直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3.6版本，3.7及以上版本目前不支持）"></a>&emsp;&emsp;直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3.6版本，3.7及以上版本目前不支持）</h4><h4 id="下载地址：https-www-python-org-ftp-python-3-6-8-python-3-6-8-amd64-exe"><a href="#下载地址：https-www-python-org-ftp-python-3-6-8-python-3-6-8-amd64-exe" class="headerlink" title="下载地址：https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe"></a>下载地址：<a href="https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe" target="_blank" rel="noopener">https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe</a></h4><h4 id="emsp-emsp-如图示安装install-Now即可-注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。"><a href="#emsp-emsp-如图示安装install-Now即可-注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。" class="headerlink" title="&emsp;&emsp;如图示安装install Now即可,注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。"></a>&emsp;&emsp;如图示安装install Now即可,注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。<img src="https://img-blog.csdnimg.cn/20191220183312800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h6 id="注：默认路径C-users-用户名-Appdata-local-Programs-Python-Python36-也可以使用Customize-installation来更换路径。"><a href="#注：默认路径C-users-用户名-Appdata-local-Programs-Python-Python36-也可以使用Customize-installation来更换路径。" class="headerlink" title="注：默认路径C:\users\用户名\Appdata\local\Programs\Python\Python36 也可以使用Customize installation来更换路径。"></a>注：默认路径C:\users\用户名\Appdata\local\Programs\Python\Python36 也可以使用Customize installation来更换路径。</h6><h4 id="安装完成后，进入cmd界面，输入python，如图"><a href="#安装完成后，进入cmd界面，输入python，如图" class="headerlink" title="安装完成后，进入cmd界面，输入python，如图"></a>安装完成后，进入cmd界面，输入python，如图</h4><p><img src="https://img-blog.csdnimg.cn/20191220183344749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="出现此界面即可。"><a href="#出现此界面即可。" class="headerlink" title="出现此界面即可。"></a>出现此界面即可。</h4><p>&nbsp;</p><h2 id="方法二-：使用anaconda搭建环境"><a href="#方法二-：使用anaconda搭建环境" class="headerlink" title="方法二 ：使用anaconda搭建环境:"></a>方法二 ：使用anaconda搭建环境:</h2><h4 id="emsp-emsp-因为anaconda官方网站下下载较慢，这里推荐使用-清华镜像来下载。"><a href="#emsp-emsp-因为anaconda官方网站下下载较慢，这里推荐使用-清华镜像来下载。" class="headerlink" title="&emsp;&emsp;因为anaconda官方网站下下载较慢，这里推荐使用    清华镜像来下载。"></a>&emsp;&emsp;因为<a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">anaconda官方网站</a>下下载较慢，这里推荐使用    <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">清华镜像</a>来下载。</h4><h6 id="注：anaconda官方当前最新版为基于python3-7version，这个并不意味着搭建的python环境版本就固定为3-7，但是这里还是不推荐使用下面会说明原因。"><a href="#注：anaconda官方当前最新版为基于python3-7version，这个并不意味着搭建的python环境版本就固定为3-7，但是这里还是不推荐使用下面会说明原因。" class="headerlink" title="注：anaconda官方当前最新版为基于python3.7version，这个并不意味着搭建的python环境版本就固定为3.7，但是这里还是不推荐使用下面会说明原因。"></a>注：anaconda官方当前最新版为基于python3.7version，这个并不意味着搭建的python环境版本就固定为3.7，但是这里还是不推荐使用下面会说明原因。</h6><h4 id="清华镜像下载地址https-mirrors-tuna-tsinghua-edu-cn-anaconda-archive-Anaconda3-5-2-0-Windows-x86-64-exe"><a href="#清华镜像下载地址https-mirrors-tuna-tsinghua-edu-cn-anaconda-archive-Anaconda3-5-2-0-Windows-x86-64-exe" class="headerlink" title="清华镜像下载地址https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe"></a>清华镜像下载地址<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe</a></h4><h4 id="如图所示安装anaconda，这里先用的是官方3-7的包："><a href="#如图所示安装anaconda，这里先用的是官方3-7的包：" class="headerlink" title="如图所示安装anaconda，这里先用的是官方3.7的包："></a>如图所示安装anaconda，这里先用的是官方3.7的包：</h4><h6 id="注：存在问题：1、下载太慢。2、3-7—version需额外对环境更改，这里只对3-7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。"><a href="#注：存在问题：1、下载太慢。2、3-7—version需额外对环境更改，这里只对3-7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。" class="headerlink" title="注：存在问题：1、下载太慢。2、3.7—version需额外对环境更改，这里只对3.7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。"></a>注：存在问题：1、下载太慢。2、3.7—version需额外对环境更改，这里只对3.7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。</h6><h4 id="安装步骤："><a href="#安装步骤：" class="headerlink" title="安装步骤："></a>安装步骤：</h4><p>&nbsp;<img src="https://img-blog.csdnimg.cn/20191220222414658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20191220222510588.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。"><a href="#目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。" class="headerlink" title="目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。"></a>目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。</h4><p><img src="https://img-blog.csdnimg.cn/20191220222529582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="这里是指是否将anaconda里的python作为电脑默认python环境。"><a href="#这里是指是否将anaconda里的python作为电脑默认python环境。" class="headerlink" title="这里是指是否将anaconda里的python作为电脑默认python环境。"></a>这里是指是否将anaconda里的python作为电脑默认python环境。</h4><h4 id="emsp-emsp-安装完成后，选择Anaconda-Navigator-打开，接下来选择Environments-gt-create，-生成如下界面："><a href="#emsp-emsp-安装完成后，选择Anaconda-Navigator-打开，接下来选择Environments-gt-create，-生成如下界面：" class="headerlink" title="&emsp;&emsp;安装完成后，选择Anaconda Navigator 打开，接下来选择Environments -&gt;create，:生成如下界面："></a>&emsp;&emsp;安装完成后，选择Anaconda Navigator 打开，接下来选择Environments -&gt;create，:生成如下界面：</h4><p><img src="https://img-blog.csdnimg.cn/2019122022253567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="emsp-emsp-这里是3-7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装："><a href="#emsp-emsp-这里是3-7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装：" class="headerlink" title="&emsp;&emsp;这里是3.7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装："></a>&emsp;&emsp;这里是3.7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装：</h4><p><img src="https://img-blog.csdnimg.cn/2019122022312263.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="emsp-emsp-接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3-6。"><a href="#emsp-emsp-接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3-6。" class="headerlink" title="&emsp;&emsp;接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3.6。"></a>&emsp;&emsp;接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3.6。</h4><h2 id="第一步，总结"><a href="#第一步，总结" class="headerlink" title="第一步，总结"></a>第一步，总结</h2><h4 id="emsp-emsp-anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。"><a href="#emsp-emsp-anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。" class="headerlink" title="&emsp;&emsp;anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。"></a>&emsp;&emsp;anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。</h4><p>&nbsp;</p><h1 id="第二步，安装pycharm"><a href="#第二步，安装pycharm" class="headerlink" title="第二步，安装pycharm"></a>第二步，安装pycharm</h1><h4 id="这个在pycharm官网上下载即可-如图示安装即可。"><a href="#这个在pycharm官网上下载即可-如图示安装即可。" class="headerlink" title="这个在pycharm官网上下载即可,如图示安装即可。 "></a>这个在<a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">pycharm官网</a>上下载即可,如图示安装即可。 <img src="https://img-blog.csdnimg.cn/20191221095541168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="pycharm_1"></h4><p><img src="https://img-blog.csdnimg.cn/20191221095624451.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="pycharm_2">)<img src="https://img-blog.csdnimg.cn/2019122316264677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191221095701293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="pycharm_3"><br>安装完成。<br>&nbsp;</p><h1 id="第三步，安装必需包"><a href="#第三步，安装必需包" class="headerlink" title="第三步，安装必需包"></a>第三步，安装必需包</h1><h4 id="emsp-emsp-这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm-中的【ALT-SHIFT-ENTER】即可安装，如还有缺失请自行百度。"><a href="#emsp-emsp-这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm-中的【ALT-SHIFT-ENTER】即可安装，如还有缺失请自行百度。" class="headerlink" title="&emsp;&emsp;这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm 中的【ALT+SHIFT+ENTER】即可安装，如还有缺失请自行百度。"></a>&emsp;&emsp;这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm 中的【ALT+SHIFT+ENTER】即可安装，如还有缺失请自行百度。</h4><h2 id="方法一：直装python环境"><a href="#方法一：直装python环境" class="headerlink" title="方法一：直装python环境"></a>方法一：直装python环境</h2><h4 id="emsp-emsp-默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python："><a href="#emsp-emsp-默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python：" class="headerlink" title="&emsp;&emsp;默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python："></a>&emsp;&emsp;默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python：</h4><h4 id="若为下图：第二行中出现anaconda。"><a href="#若为下图：第二行中出现anaconda。" class="headerlink" title="若为下图：第二行中出现anaconda。"></a>若为下图：第二行中出现anaconda。<img src="https://img-blog.csdnimg.cn/20191221211939424.png" alt="在这里插入图片描述"></h4><h4 id="则需修改默认python环境。"><a href="#则需修改默认python环境。" class="headerlink" title="则需修改默认python环境。"></a>则需修改默认python环境。</h4><h4 id="若为下图，第二行中不是anaconda，则继续操作即可。"><a href="#若为下图，第二行中不是anaconda，则继续操作即可。" class="headerlink" title="若为下图，第二行中不是anaconda，则继续操作即可。"></a>若为下图，第二行中不是anaconda，则继续操作即可。<img src="https://img-blog.csdnimg.cn/20191221213513434.png" alt="在这里插入图片描述"></h4><h3 id="3-1-1-安装tensorflow包"><a href="#3-1-1-安装tensorflow包" class="headerlink" title="3.1.1 安装tensorflow包"></a>3.1.1 安装tensorflow包</h3><h4 id="emsp-emsp-tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https-github-com-fo40225-t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https-github-com-fo40225-tensorflow-windows-wheel-raw-master-1-4-0-py36-CPU-avx2-tensorflow-1-4-0-cp36-cp36m-win-amd64-whl"><a href="#emsp-emsp-tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https-github-com-fo40225-t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https-github-com-fo40225-tensorflow-windows-wheel-raw-master-1-4-0-py36-CPU-avx2-tensorflow-1-4-0-cp36-cp36m-win-amd64-whl" class="headerlink" title="&emsp;&emsp;tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https://github.com/fo40225/t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl"></a>&emsp;&emsp;tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：<a href="https://github.com/fo40225/tensorflow-windows-wheel" target="_blank" rel="noopener">https://github.com/fo40225/t与ensorflow-windows-wheel</a>上下载，对于不同版本的python环境有不同的whl文件可以下载。<a href="https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl" target="_blank" rel="noopener">https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl</a></h4><h4 id="emsp-emsp-下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。"><a href="#emsp-emsp-下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。" class="headerlink" title="&emsp;&emsp;下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。"></a>&emsp;&emsp;下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。<img src="https://img-blog.csdnimg.cn/20191221184310679.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="PowerShell"></h4><h4 id="输入-："><a href="#输入-：" class="headerlink" title="输入    ："></a>输入    ：</h4><pre><code>pip install .\tensorflow-1.4.0-cp36-cp36m-win_amd64.whl</code></pre><p><img src="https://img-blog.csdnimg.cn/20191221185322803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="运行完即可，若是提示pip版本有更新，更不更新都可。"><a href="#运行完即可，若是提示pip版本有更新，更不更新都可。" class="headerlink" title="运行完即可，若是提示pip版本有更新，更不更新都可。"></a>运行完即可，若是提示pip版本有更新，更不更新都可。</h5><pre><code>python -m pip install --upgrade pip</code></pre><h3 id="3-1-2-安装opencv包"><a href="#3-1-2-安装opencv包" class="headerlink" title="3.1.2 安装opencv包"></a>3.1.2 安装opencv包</h3><h4 id="opencv在代码中使用时：import-cv2"><a href="#opencv在代码中使用时：import-cv2" class="headerlink" title="opencv在代码中使用时：import cv2"></a>opencv在代码中使用时：import cv2</h4><h4 id="opencv较tensorflow简单，只需输入代码即可运行："><a href="#opencv较tensorflow简单，只需输入代码即可运行：" class="headerlink" title="opencv较tensorflow简单，只需输入代码即可运行："></a>opencv较tensorflow简单，只需输入代码即可运行：</h4><pre><code>pip install opencv-python</code></pre><p><img src="https://img-blog.csdnimg.cn/20191221191736729.png" alt="opencv"></p><h3 id="3-1-2-安装PIL（Python-Imaging-Library）包"><a href="#3-1-2-安装PIL（Python-Imaging-Library）包" class="headerlink" title="3.1.2 安装PIL（Python Imaging Library）包"></a>3.1.2 安装PIL（Python Imaging Library）包</h3><h4 id="PIL安装并不是通过直接键入pip-install-PIL-而是通过："><a href="#PIL安装并不是通过直接键入pip-install-PIL-而是通过：" class="headerlink" title="PIL安装并不是通过直接键入pip install PIL 而是通过："></a>PIL安装并不是通过直接键入pip install PIL 而是通过：</h4><pre><code>pip install pillow</code></pre><p><img src="https://img-blog.csdnimg.cn/20191221193010925.png" alt="在这里插入图片描述"></p><h3 id="方法一总结："><a href="#方法一总结：" class="headerlink" title="方法一总结："></a>方法一总结：</h3><h4 id="emsp-emsp-这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。"><a href="#emsp-emsp-这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。" class="headerlink" title="&emsp;&emsp;这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。"></a>&emsp;&emsp;这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。</h4><p>&nbsp;</p><h2 id="方法二：anaconda环境"><a href="#方法二：anaconda环境" class="headerlink" title="方法二：anaconda环境"></a>方法二：anaconda环境</h2><h4 id="emsp-emsp-这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。"><a href="#emsp-emsp-这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。" class="headerlink" title="&emsp;&emsp;这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。"></a>&emsp;&emsp;这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。<img src="https://img-blog.csdnimg.cn/20191221235217581.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="点击上方的选择框设置为all，然后进行搜索tensorflw"><a href="#点击上方的选择框设置为all，然后进行搜索tensorflw" class="headerlink" title="点击上方的选择框设置为all，然后进行搜索tensorflw"></a>点击上方的选择框设置为all，然后进行搜索tensorflw<img src="https://img-blog.csdnimg.cn/20191221235345841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h4><h4 id="选择tensorflow，点击apply"><a href="#选择tensorflow，点击apply" class="headerlink" title="选择tensorflow，点击apply"></a>选择tensorflow，点击apply</h4><p><img src="https://img-blog.csdnimg.cn/20191222000202665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="emsp-emsp-会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。"><a href="#emsp-emsp-会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。" class="headerlink" title="&emsp;&emsp;会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。"></a>&emsp;&emsp;会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。</h4><p>&nbsp;</p><h1 id="第四步，建立风格迁移工程"><a href="#第四步，建立风格迁移工程" class="headerlink" title="第四步，建立风格迁移工程"></a>第四步，建立风格迁移工程</h1><h4 id="在pycharm中选择：file-gt-New-project"><a href="#在pycharm中选择：file-gt-New-project" class="headerlink" title="在pycharm中选择：file-&gt;New project "></a>在pycharm中选择：file-&gt;New project <img src="https://img-blog.csdnimg.cn/20191221210833416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="NewProject"></h4><h4 id="emsp-emsp-这里要注意的是Base-interpreter-，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。"><a href="#emsp-emsp-这里要注意的是Base-interpreter-，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。" class="headerlink" title="&emsp;&emsp;这里要注意的是Base interpreter ，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。"></a>&emsp;&emsp;这里要注意的是Base interpreter ，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。</h4><h2 id="方法一：在创建项目是直接选择anaconda下的python："><a href="#方法一：在创建项目是直接选择anaconda下的python：" class="headerlink" title="方法一：在创建项目是直接选择anaconda下的python："></a>方法一：在创建项目是直接选择anaconda下的python：</h2><p><img src="https://img-blog.csdnimg.cn/20191222004253634.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="方法二：在项目创建完成后再选择路径为：File-gt-Settings-gt-project：项目名，如图点击齿轮——Add。"><a href="#方法二：在项目创建完成后再选择路径为：File-gt-Settings-gt-project：项目名，如图点击齿轮——Add。" class="headerlink" title="方法二：在项目创建完成后再选择路径为：File-&gt;Settings-&gt;project：项目名，如图点击齿轮——Add。"></a>方法二：在项目创建完成后再选择路径为：File-&gt;Settings-&gt;project：项目名，如图点击齿轮——Add。</h2><p><img src="https://img-blog.csdnimg.cn/20191222003319372.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="在Base-interpreter下选择anaconda中python路径即可。"><a href="#在Base-interpreter下选择anaconda中python路径即可。" class="headerlink" title="在Base interpreter下选择anaconda中python路径即可。"></a>在Base interpreter下选择anaconda中python路径即可。</h4><p><img src="https://img-blog.csdnimg.cn/20191222003949287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEwODUxNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="至此所有环境配置完毕。"><a href="#至此所有环境配置完毕。" class="headerlink" title="至此所有环境配置完毕。"></a>至此所有环境配置完毕。</h4><p>&nbsp;</p><h1 id="第五步，写风格迁移代码"><a href="#第五步，写风格迁移代码" class="headerlink" title="第五步，写风格迁移代码"></a>第五步，写风格迁移代码</h1><h4 id="emsp-emsp-这里主要参网址的为http-zh-gluon-ai-chapter-computer-vision-neural-style-html会在下一篇博客https-blog-csdn-net-weixin-41108515-article-details-103650964里详细解释过程。"><a href="#emsp-emsp-这里主要参网址的为http-zh-gluon-ai-chapter-computer-vision-neural-style-html会在下一篇博客https-blog-csdn-net-weixin-41108515-article-details-103650964里详细解释过程。" class="headerlink" title="&emsp;&emsp;这里主要参网址的为http://zh.gluon.ai/chapter_computer-vision/neural-style.html会在下一篇博客https://blog.csdn.net/weixin_41108515/article/details/103650964里详细解释过程。"></a>&emsp;&emsp;这里主要参网址的为<a href="http://zh.gluon.ai/chapter_computer-vision/neural-style.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_computer-vision/neural-style.html</a>会在下一篇博客<a href="https://blog.csdn.net/weixin_41108515/article/details/103650964" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41108515/article/details/103650964</a>里详细解释过程。</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络：（一）风格迁移——环境配置&quot;&gt;&lt;a href=&quot;#卷积神经网络：（一）风格迁移——环境配置&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络：（一）风格迁移——环境配置&quot;&gt;&lt;/a&gt;卷积神经网络：（一）风格迁移——环境配置&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
      <category term="人工智能" scheme="http://ailous.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="卷积神经网络" scheme="http://ailous.top/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="风格迁移" scheme="http://ailous.top/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
</feed>
