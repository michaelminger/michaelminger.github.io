{"meta":{"title":"Michaelming's Blog","subtitle":"不见你如我一般","description":"大地茫茫一片真干净","author":"Michaelming","url":"http://ailous.top"},"pages":[{"title":"about","date":"2018-09-30T09:25:30.000Z","updated":"2020-05-26T03:28:07.582Z","comments":true,"path":"about/index.html","permalink":"http://ailous.top/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-05-12T01:56:21.000Z","updated":"2020-05-12T01:56:57.012Z","comments":false,"path":"categories/index.html","permalink":"http://ailous.top/categories/index.html","excerpt":"","text":"","keywords":null},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-05-12T03:58:57.602Z","comments":false,"path":"bangumi/index.html","permalink":"http://ailous.top/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-03-16T10:41:30.000Z","comments":true,"path":"comment/index.html","permalink":"http://ailous.top/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"client/index.html","permalink":"http://ailous.top/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"donate/index.html","permalink":"http://ailous.top/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"lab/index.html","permalink":"http://ailous.top/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-03-16T10:41:30.000Z","comments":true,"path":"links/index.html","permalink":"http://ailous.top/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-05-26T04:44:35.196Z","comments":false,"path":"music/index.html","permalink":"http://ailous.top/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-05-12T01:50:55.687Z","comments":true,"path":"tags/index.html","permalink":"http://ailous.top/tags/index.html","excerpt":"","text":""},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-03-16T10:41:30.000Z","comments":true,"path":"rss/index.html","permalink":"http://ailous.top/rss/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"http://ailous.top/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"video/index.html","permalink":"http://ailous.top/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"网络爬虫——超级鹰源码下载","slug":"网络爬虫——超级鹰源码下载","date":"2020-04-27T05:47:40.000Z","updated":"2020-05-27T07:45:10.095Z","comments":true,"path":"2020/04/27/wang-luo-pa-chong-chao-ji-ying-yuan-ma-xia-zai/","link":"","permalink":"http://ailous.top/2020/04/27/wang-luo-pa-chong-chao-ji-ying-yuan-ma-xia-zai/","excerpt":"","text":"网络爬虫——超级鹰源码下载超级鹰官方网址：https://www.chaojiying.com/将文件下载再解压，这里使用源码： 源码：#!/usr/bin/env python # coding:utf-8 import requests from hashlib import md5 class Chaojiying_Client(object): def __init__(self, username, password, soft_id): self.username = username password = password.encode('utf8') self.password = md5(password).hexdigest() self.soft_id = soft_id self.base_params = { 'user': self.username, 'pass2': self.password, 'softid': self.soft_id, } self.headers = { 'Connection': 'Keep-Alive', 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)', } def PostPic(self, im, codetype): \"\"\" im: 图片字节 codetype: 题目类型 参考 http://www.chaojiying.com/price.html \"\"\" params = { 'codetype': codetype, } params.update(self.base_params) files = {'userfile': ('ccc.jpg', im)} r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, files=files, headers=self.headers) return r.json() def ReportError(self, im_id): \"\"\" im_id:报错题目的图片ID \"\"\" params = { 'id': im_id, } params.update(self.base_params) r = requests.post('http://upload.chaojiying.net/Upload/ReportError.php', data=params, headers=self.headers) return r.json() if __name__ == '__main__': chaojiying = Chaojiying_Client('超级鹰用户名', '超级鹰用户名的密码', '96001') #用户中心>>软件ID 生成一个替换 96001 im = open('a.jpg', 'rb').read() #本地图片文件路径 来替换 a.jpg 有时WIN系统须要// print chaojiying.PostPic(im, 1902) #1902 验证码类型 官方网站>>价格体系 3.4+版 print 后要加() a.jpg 为自己目标图片 超级鹰用户名及密码。需要自己去官网注册使用。 具体使用方法到我的后继博客中查找。","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"超级鹰","slug":"超级鹰","permalink":"http://ailous.top/tags/超级鹰/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"汇编题——CJNE","slug":"汇编题——CJNE","date":"2020-02-20T05:47:40.000Z","updated":"2020-05-26T03:50:23.413Z","comments":true,"path":"2020/02/20/hui-bian-ti-cjne/","link":"","permalink":"http://ailous.top/2020/02/20/hui-bian-ti-cjne/","excerpt":"","text":"汇编题——CJNE","categories":[{"name":"汇编","slug":"汇编","permalink":"http://ailous.top/categories/汇编/"}],"tags":[{"name":"CJNE","slug":"CJNE","permalink":"http://ailous.top/tags/CJNE/"},{"name":"DJNE","slug":"DJNE","permalink":"http://ailous.top/tags/DJNE/"}],"keywords":[{"name":"汇编","slug":"汇编","permalink":"http://ailous.top/categories/汇编/"}]},{"title":"网络爬虫——前程无忧网数据获取及存储（高级）","slug":"网络爬虫——前程无忧网数据获取及存储(高级)","date":"2019-09-03T05:47:40.000Z","updated":"2020-05-27T07:44:29.765Z","comments":true,"path":"2019/09/03/wang-luo-pa-chong-qian-cheng-wu-you-wang-shu-ju-huo-qu-ji-cun-chu-gao-ji/","link":"","permalink":"http://ailous.top/2019/09/03/wang-luo-pa-chong-qian-cheng-wu-you-wang-shu-ju-huo-qu-ji-cun-chu-gao-ji/","excerpt":"","text":"网络爬虫——前程无忧网数据获取及存储（高级）实验内容1目标网站：前程无忧招聘网 目标网址：https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html 目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到txt，csv文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def parsePage(html): ulist = [] clist = [] rlist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath('//*[@id=\"resultList\"]/div[@class=\"el\"]//text()') for i in range(len(result)): ulist.append(result[i].replace(\" \",\"\").replace('\\r',\"\").replace(\"\\n\",'')) while '' in ulist: ulist.remove('') length = len(ulist) weight = int(length / 5 ) for i in range(weight): for j in range(5): clist.append(ulist[i*5+j]) rlist.append(clist) clist = [] return rlist # def txtdata(data): # with open('top20.txt','w')as file: # for i in data: # for j in i: # print(j) # print('successful') def storedata(data): with open('top20.txt','w',encoding = 'utf-8')as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+'\\n') print('ok') def csvdata(data): with open('top20.csv','w',encoding = 'utf-8',newline='')as csvfile: fieldnames = ['职位名','公司名','工作地点','薪资','工作时间'] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({'职位名':i[0],'公司名':i[1],'工作地点':i[2],'薪资':i[3],'工作时间':i[4]}) print('ok') def main(): url=\"https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html\" html=getHtmlText(url) rlist=parsePage(html) # txtdata(data) storedata(rlist) csvdata(rlist) main() 结果输出：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"Xpath","slug":"Xpath","permalink":"http://ailous.top/tags/Xpath/"},{"name":"CSV","slug":"CSV","permalink":"http://ailous.top/tags/CSV/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫常见问题汇总","slug":"网络爬虫常见问题汇总","date":"2019-06-02T05:47:40.000Z","updated":"2020-05-26T03:51:28.641Z","comments":true,"path":"2019/06/02/wang-luo-pa-chong-chang-jian-wen-ti-hui-zong/","link":"","permalink":"http://ailous.top/2019/06/02/wang-luo-pa-chong-chang-jian-wen-ti-hui-zong/","excerpt":"","text":"网络爬虫常见问题汇总问题一：使用requests库或者urllib库获取源代码时无法正常显示中文解决方法： （1）requests库的文本中有两种类型，一种是文本类型，使用text属性，一种是针对音频、视频、图片等二进制数据类型，使用content属性；一般返回的是text属性时会出现中文乱码现象，因此在输出返回之前需要显示的修改属性encoding，将其赋值为“utf-8”或者是apparent_encoding即可。 （2）urllib库的文本只有一种就是使用read()方法进行读取。因此要解决中文问题，一定要在读取后加入.decode(“utf-8”)，进行显示的转码之后便不会出现乱码问题了。 问题二：文本节点首先看两个HTML代码: 这是你眼中的HTML代码这是计算机眼中的HTML代码:解决方法： 在BS4中, 我们在HTML中看到的换行符以及空格都是NavigableString 也就是文本节点. 问题三：滥用遍历文档树的方法常见的方法有: contentsdescendantsparentnext_siblingnext_element 这些方法都会遍历文档树中的所有节点, 包括文本节点. 也就是说: 只要你使用这些方法, 你就一定会选择出许多文本节点, 因为文本节点无处不在: 换行, 空格等. 解决方法： 使用过滤器find等方法: soup.find(name=’tagname’)当我们一旦在过滤器中指定了name关键字, 那么返回的结果就一定是tag对象, 因为文档节点没有name属性. 结论: 大多数情况下, 你需要的是find 这一类过滤器, 而不是遍历所有节点. 问题四：html.parserhtml.parser是个令人又恨又爱的解析器, 它是Python内置的解析器, 开箱即用. 但是在一些情况下, 它解析出来的文档会丢失信息. 解决方法： 如果你发现你的文档信息缺少了, 那么试着换其他解析器,例如: lxml","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"问题","slug":"问题","permalink":"http://ailous.top/tags/问题/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——前程无忧网数据获取及MYSQL存储","slug":"网络爬虫——前程无忧网数据获取及MYSQL存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-27T07:44:27.164Z","comments":true,"path":"2019/05/27/wang-luo-pa-chong-qian-cheng-wu-you-wang-shu-ju-huo-qu-ji-mysql-cun-chu/","link":"","permalink":"http://ailous.top/2019/05/27/wang-luo-pa-chong-qian-cheng-wu-you-wang-shu-ju-huo-qu-ji-mysql-cun-chu/","excerpt":"","text":"网络爬虫——前程无忧网数据获取及MYSQL存储实验内容1目标网站：前程无忧招聘网 目标网址：https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html 目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到MYSQL库文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree import pymysql from PIL import Image import pytesseract import traceback def connectMysql(): return pymysql.connect(host='localhost',user='root',password='123456',port=3306,db='spiders') def createMysqlTable(): db = connectMysql() cursor = db.cursor() # （1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 sql = 'create table if not exists proStr (\\ 职位名 varchar(255) not null ,\\ 公司名 varchar(255) not null,\\ 工作地点 varchar(255) not null,\\ 薪资 varchar(255) not null,\\ 发布时间 varchar(255) not null,\\ primary key(职位名))' cursor.execute(sql) print('ok') db.close() def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def parsePage(html): ulist = [] clist = [] rlist = [] ilist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath('//*[@id=\"content\"]/div[2]/table/tbody/tr/td//text()') imgs = newhtml.xpath('//*[@id=\"content\"]/div[2]/table/tbody/tr/td/a/img/@src', stream=True) j = 0 for img in imgs: j=j+1 with open(str(j)+'.png', 'wb') as fd: picture=requests.get(img).content fd.write(picture) for i in range(len(imgs)): str_ = str(i+1)+'.png' text = pytesseract.image_to_string(Image.open(str_)) ilist.append(text.replace(\" \",\".\").replace(\"M\",\"亿\").replace(\"a\",\"亿\")) # print(ilist) for i in range(len(result)): ulist.append(result[i].replace(\" \",\"\").replace('\\r',\"\").replace(\"\\n\",'')) while '' in ulist: ulist.remove('') length = len(ulist) weight = int(length / 8 ) for i in range(weight): for j in range(8): clist.append(ulist[i*8+j]) clist.append(ilist[i]) rlist.append(clist) clist = [] return rlist def mysqlData(datas): table = 'movies' keys = '名次,电影名称,日期,总场次,废场,人次,上座率,票价,票房' db = connectMysql() cursor = db.cursor() for data in datas: values = ','.join(['%s']*len(data)) sql = 'INSERT INTO {table}({keys}) VALUES({values})'.format(table=table,keys = keys ,values = values) print(sql) print(tuple(data)) try : if cursor.execute(sql, tuple(data)): print(\"Succcessful\") db.commit() except: traceback.print_exc() print(\"Failed\") db.rollback() db.close() def main(): createMysqlTable() url=\"http://58921.com/daily/wangpiao\" html=getHtmlText(url) rlist=parsePage(html) mysqlData(rlist) main() 结果输出效果：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"前程无忧","slug":"前程无忧","permalink":"http://ailous.top/tags/前程无忧/"},{"name":"MYSQL","slug":"MYSQL","permalink":"http://ailous.top/tags/MYSQL/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——二手房数据抓取及MYSQL存储","slug":"网络爬虫——二手房数据抓取及MYSQL存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-27T07:44:23.872Z","comments":true,"path":"2019/05/27/wang-luo-pa-chong-er-shou-fang-shu-ju-zhua-qu-ji-mysql-cun-chu/","link":"","permalink":"http://ailous.top/2019/05/27/wang-luo-pa-chong-er-shou-fang-shu-ju-zhua-qu-ji-mysql-cun-chu/","excerpt":"","text":"网络爬虫——二手房数据抓取及MYSQL存储目标网址：https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar 目标数据： 标题 + 链接地址 + 厅室+ 面积+ 层数+建造时间 + 地址 + 单价（或总价） 要求： （1）自选请求库和解析库获取目标数据； （2）第一个存储至txt或者csv中，第二个源码存储至Mysql中。 源码（1）：csv,txtimport requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def cleanData(clist): olist = [] for i in range(len(clist)): olist.append(clist[i].replace(\" \",\"\").replace('\\r',\"\").replace(\"\\n\",'').replace(\"\\xa0\\xa0\",',')) return olist def parsePage(html): ulist = [] clist = [] newhtml =etree.HTML(html,etree.HTMLParser()) titles =cleanData(newhtml.xpath('//*[@id=\"houselist-mod-new\"]/li/div[2]/div/a//text()')) hrefs = cleanData(newhtml.xpath('//*[@id=\"houselist-mod-new\"]/li/div[2]/div/a//@href', stream=True)) others =cleanData(newhtml.xpath('//*[@id=\"houselist-mod-new\"]/li/div[2]/div[2]/span//text()')) addresss =cleanData(newhtml.xpath('//*[@id=\"houselist-mod-new\"]/li/div[2]/div[3]/span//text()')) prices =cleanData(newhtml.xpath('//*[@id=\"houselist-mod-new\"]/li/div[3]/span[2]//text()')) length = len(titles) for i in range(length): ulist.append(titles[i]) ulist.append(hrefs[i]) ulist.append(others[i*4+0]) ulist.append(others[i*4+1]) ulist.append(others[i*4+2]) ulist.append(others[i*4+3]) ulist.append(addresss[i]) ulist.append(prices[i]) clist.append(ulist) ulist = [] return clist def txtdata(data): with open('data.txt','w')as file: for i in data: for j in i: print(j) print('successful') def storedata(data): with open('data.txt','w',encoding = 'utf-8')as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+'\\n') print('ok') def csvdata(data): with open('data.csv','w',encoding = 'utf-8',newline='')as csvfile: fieldnames = ['标题','链接地址','厅室','面积','层数','建造时间','地址','单价'] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({'标题':i[0],'链接地址':i[1],'厅室':i[2],'面积':i[3],'层数':i[4],'建造时间':i[5],'地址':i[6],'单价':i[7]}) print('ok') def main(): url=\"https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar\" html=getHtmlText(url) rlist=parsePage(html) txtdata(rlist) storedata(rlist) csvdata(rlist) main() 源码（1）：MYSQLimport requests import json import csv from requests.exceptions import RequestException from lxml import etree import pymysql import pytesseract import traceback def connectMysql(): return pymysql.connect(host=&#39;localhost&#39;,user=&#39;root&#39;,password=&#39;123456&#39;,port=3306,db=&#39;spiders&#39;) def createMysqlTable(): db = connectMysql() cursor = db.cursor() sql = &#39;create table if not exists data (\\ 标题 varchar(255) not null ,\\ 链接地址 varchar(255) not null ,\\ 厅室 varchar(255) not null,\\ 面积 varchar(255) not null,\\ 层数 varchar(255) not null,\\ 建造时间 varchar(255) not null,\\ 地址 varchar(255) not null,\\ 单价 varchar(255) not null,\\ primary key(标题))&#39; cursor.execute(sql) print(&#39;ok&#39;) db.close() def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def cleanData(clist): olist = [] for i in range(len(clist)): olist.append(clist[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;).replace(&quot;\\xa0\\xa0&quot;,&#39;,&#39;)) return olist def parsePage(html): ulist = [] clist = [] newhtml =etree.HTML(html,etree.HTMLParser()) titles =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div/a//text()&#39;)) hrefs = cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div/a//@href&#39;, stream=True)) others =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div[2]/span//text()&#39;)) addresss =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div[3]/span//text()&#39;)) prices =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[3]/span[2]//text()&#39;)) length = len(titles) for i in range(length): ulist.append(titles[i]) ulist.append(hrefs[i][0:100]) ulist.append(others[i*4+0]) ulist.append(others[i*4+1]) ulist.append(others[i*4+2]) ulist.append(others[i*4+3]) ulist.append(addresss[i]) ulist.append(prices[i]) clist.append(ulist) ulist = [] return clist def txtdata(data): with open(&#39;data.txt&#39;,&#39;w&#39;)as file: for i in data: for j in i: print(j) print(&#39;successful&#39;) def storedata(data): with open(&#39;data.txt&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;)as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+&#39;\\n&#39;) print(&#39;ok&#39;) def csvdata(data): with open(&#39;data.csv&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;,newline=&#39;&#39;)as csvfile: fieldnames = [&#39;标题&#39;,&#39;链接地址&#39;,&#39;厅室&#39;,&#39;面积&#39;,&#39;层数&#39;,&#39;建造时间&#39;,&#39;地址&#39;,&#39;单价&#39;] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({&#39;标题&#39;:i[0],&#39;链接地址&#39;:i[1],&#39;厅室&#39;:i[2],&#39;面积&#39;:i[3],&#39;层数&#39;:i[4],&#39;建造时间&#39;:i[5],&#39;地址&#39;:i[6],&#39;单价&#39;:i[7]}) print(&#39;ok&#39;) def mysqlData(datas): table = &#39;data&#39; keys = &#39;标题,链接地址,厅室,面积,层数,建造时间,地址,单价&#39; db = connectMysql() cursor = db.cursor() for data in datas: values = &#39;,&#39;.join([&#39;%s&#39;]*len(data)) sql = &#39;INSERT INTO {table}({keys}) VALUES({values})&#39;.format(table=table,keys = keys ,values = values) print(sql) print(tuple(data)) try : if cursor.execute(sql, tuple(data)): print(&quot;Succcessful&quot;) db.commit() except: traceback.print_exc() print(&quot;Failed&quot;) db.rollback() db.close() def main(): # createMysqlTable() url=&quot;https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar&quot; html=getHtmlText(url) rlist=parsePage(html) # txtdata(rlist) storedata(rlist) csvdata(rlist) mysqlData(rlist) main()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://ailous.top/tags/MYSQL/"},{"name":"二手房","slug":"二手房","permalink":"http://ailous.top/tags/二手房/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——票房网数据抓取及存储（初级）","slug":"网络爬虫——票房网数据抓取及存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-27T07:44:49.326Z","comments":true,"path":"2019/05/27/wang-luo-pa-chong-piao-fang-wang-shu-ju-zhua-qu-ji-cun-chu/","link":"","permalink":"http://ailous.top/2019/05/27/wang-luo-pa-chong-piao-fang-wang-shu-ju-zhua-qu-ji-cun-chu/","excerpt":"","text":"网络爬虫——票房网数据抓取及存储实验内容目标网站：电影票房网 目标网址：http://58921.com/daily/wangpiao 任务要求目标数据：（1）名次（2）电影名称 （3）日期（4）票房 （5）总场次（6）废场（7）人次（8）上座率（9）票价 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到csv文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def parsePage(html): ulist = [] clist = [] rlist = [] ilist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath('//*[@id=\"content\"]/div[2]/table/tbody/tr/td//text()') imgs = newhtml.xpath('//*[@id=\"content\"]/div[2]/table/tbody/tr/td/a/img/@src', stream=True) j = 0 for img in imgs: j=j+1 with open(str(j)+'.png', 'wb') as fd: picture=requests.get(img).content fd.write(picture) for i in range(len(imgs)): str_ = str(i+1)+'.png' text = pytesseract.image_to_string(Image.open(str_)) ilist.append(text.replace(\" \",\".\").replace(\"M\",\"亿\").replace(\"a\",\"亿\")) # print(ilist) for i in range(len(result)): ulist.append(result[i].replace(\" \",\"\").replace('\\r',\"\").replace(\"\\n\",'')) while '' in ulist: ulist.remove('') length = len(ulist) weight = int(length / 8 ) for i in range(weight): for j in range(8): clist.append(ulist[i*8+j]) clist.append(ilist[i]) rlist.append(clist) clist = [] return rlist # def txtdata(data): # with open('top20.txt','w')as file: # for i in data: # for j in i: # print(j) # print('successful') def storedata(data): with open('top20.txt','w',encoding = 'utf-8')as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+'\\n') print('ok') def csvdata(data): with open('top20.csv','w',encoding = 'utf-8',newline='')as csvfile: fieldnames = ['名次','电影名称','日期','票房','总场次','废场','人次','上座率','票价（元）'] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({'名次':i[0],'电影名称':i[1],'日期':i[2],'票房':i[8],'总场次':i[3],'废场':i[4],'人次':i[5],'上座率':i[6],'票价（元）':i[7]}) print('ok') def main(): url=\"http://58921.com/daily/wangpiao\" html=getHtmlText(url) rlist=parsePage(html) # txtdata(rlist) storedata(rlist) csvdata(rlist) main() 结果输出：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"票房","slug":"票房","permalink":"http://ailous.top/tags/票房/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——票房网数据抓取及MYSQL存储","slug":"网络爬虫——票房网数据抓取及MYSQL存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-27T07:44:46.488Z","comments":true,"path":"2019/05/27/wang-luo-pa-chong-piao-fang-wang-shu-ju-zhua-qu-ji-mysql-cun-chu/","link":"","permalink":"http://ailous.top/2019/05/27/wang-luo-pa-chong-piao-fang-wang-shu-ju-zhua-qu-ji-mysql-cun-chu/","excerpt":"","text":"网络爬虫——票房网数据抓取及MYSQL存储实验内容目标网站：电影票房网 目标网址：http://58921.com/daily/wangpiao 任务要求目标数据：（1）名次（2）电影名称 （3）日期（4）票房 （5）总场次（6）废场（7）人次（8）上座率（9）票价 要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到MYSQL库文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree import pymysql from PIL import Image import pytesseract import traceback def connectMysql(): return pymysql.connect(host='localhost',user='root',password='123456',port=3306,db='spiders') def createMysqlTable(): db = connectMysql() cursor = db.cursor() # fieldnames = ['名次','电影名称','日期','票房','总场次','废场','人次','上座率','票价'] sql = 'create table if not exists movies (\\ 名次 int not null ,\\ 电影名称 varchar(255) not null ,\\ 日期 varchar(255) not null,\\ 总场次 varchar(255) not null,\\ 废场 varchar(255) not null,\\ 人次 varchar(255) not null,\\ 上座率 varchar(255) not null,\\ 票价 varchar(255) not null,\\ 票房 varchar(255) not null,\\ primary key(名次))' cursor.execute(sql) print('ok') db.close() def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def parsePage(html): ulist = [] clist = [] rlist = [] ilist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath('//*[@id=\"content\"]/div[2]/table/tbody/tr/td//text()') imgs = newhtml.xpath('//*[@id=\"content\"]/div[2]/table/tbody/tr/td/a/img/@src', stream=True) j = 0 for img in imgs: j=j+1 with open(str(j)+'.png', 'wb') as fd: picture=requests.get(img).content fd.write(picture) for i in range(len(imgs)): str_ = str(i+1)+'.png' text = pytesseract.image_to_string(Image.open(str_)) ilist.append(text.replace(\" \",\".\").replace(\"M\",\"亿\").replace(\"a\",\"亿\")) # print(ilist) for i in range(len(result)): ulist.append(result[i].replace(\" \",\"\").replace('\\r',\"\").replace(\"\\n\",'')) while '' in ulist: ulist.remove('') length = len(ulist) weight = int(length / 8 ) for i in range(weight): for j in range(8): clist.append(ulist[i*8+j]) clist.append(ilist[i]) rlist.append(clist) clist = [] return rlist def mysqlData(datas): table = 'movies' keys = '名次,电影名称,日期,总场次,废场,人次,上座率,票价,票房' db = connectMysql() cursor = db.cursor() for data in datas: values = ','.join(['%s']*len(data)) sql = 'INSERT INTO {table}({keys}) VALUES({values})'.format(table=table,keys = keys ,values = values) print(sql) print(tuple(data)) try : if cursor.execute(sql, tuple(data)): print(\"Succcessful\") db.commit() except: traceback.print_exc() print(\"Failed\") db.rollback() db.close() def main(): createMysqlTable() url=\"http://58921.com/daily/wangpiao\" html=getHtmlText(url) rlist=parsePage(html) mysqlData(rlist) main() 结果输出效果：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://ailous.top/tags/MYSQL/"},{"name":"票房","slug":"票房","permalink":"http://ailous.top/tags/票房/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——搜狐最新时政新闻数据爬取——BS4","slug":"网络爬虫——搜狐最新时政新闻数据爬取","date":"2019-05-21T05:47:40.000Z","updated":"2020-05-27T07:44:40.206Z","comments":true,"path":"2019/05/21/wang-luo-pa-chong-sou-hu-zui-xin-shi-zheng-xin-wen-shu-ju-pa-qu/","link":"","permalink":"http://ailous.top/2019/05/21/wang-luo-pa-chong-sou-hu-zui-xin-shi-zheng-xin-wen-shu-ju-pa-qu/","excerpt":"","text":"网络爬虫——搜狐最新时政新闻数据爬取目标网址：https://www.sohu.com/c/8/1460?spm=smpc.null.side-nav.14.1584869506422WxyU9iK 目标数据描述：（1）标题 （2）链接地址要求： （1）使用urllib库或者requests抓取网页源代码； （2）使用BeautifulSoup的CSS选择器方法对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）利用框架结构，通过函数调用，参数传递，实现目标数据抓取，并尝试将结果写入文本文件中。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: print(\"A\") return \"\" def findUniverse(ulist , html): soup = BeautifulSoup(html,\"html.parser\") for div in soup.find(attrs=['class','news-list clearfix']).children: if isinstance(div ,bs4.element.Tag): list_0 = div.find('h4').find('a').get('href') list_1 = div.find('h4').string.replace(\" \",'').replace(\"\\n\",'') ulist.append([list_0,list_1]) def findSame(ulist,html): soup = BeautifulSoup(html,\"html.parser\") for div in soup.find(attrs=['class','second-nav']).children: if isinstance(div ,bs4.element.Tag): ulist.append(div.find('a').get('href')) return ulist def printUniverse(ulist): tplt = '{0:30}\\t{1:18}' print(tplt.format(\"网址\",\"名称\",chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],chr(12288))) def main(): ulist = [] ulist_Same = [] url = 'https://www.sohu.com/c/8/1460?spm=smpc.null.side-nav.14.1585491604691ZcX26aI' html = getHtmlText(url) ulist_Same = findSame(ulist_Same,html) for i in range(len(ulist_Same) - 2 ): url = 'https://www.sohu.com' + ulist_Same[i+1] html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"豆瓣电影排行榜数据抓取（高级）——BS4","slug":"网络爬虫——豆瓣电影排行榜数据抓取（高级）","date":"2019-05-04T05:47:40.000Z","updated":"2020-05-27T07:45:05.368Z","comments":true,"path":"2019/05/04/wang-luo-pa-chong-dou-ban-dian-ying-pai-xing-bang-shu-ju-zhua-qu-gao-ji/","link":"","permalink":"http://ailous.top/2019/05/04/wang-luo-pa-chong-dou-ban-dian-ying-pai-xing-bang-shu-ju-zhua-qu-gao-ji/","excerpt":"","text":"网络爬虫——豆瓣电影排行榜数据抓取（高级）目标网址：豆瓣电影排行：https://movie.douban.com/top250?start= 目标数据描述：排名、电影名称、导演、主演、评价人数等信息，将尽可能多的数据抓取保存任务明细： （1）使用requests库实现该网站网页源代码的获取； （2）使用BeautifulSoup对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 可以选择定义全局列表，将目标数据获取后添加到列表中，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding result=(result.text.replace('','')).replace('','') return result except: return \"\" def findUniverse(ulist , html): soup = BeautifulSoup(html,\"html.parser\") list_ = [0,0,0,0,0,0,0] for li in soup.find(attrs=['class','grid_view']).children: if isinstance(li ,bs4.element.Tag): list_[0] = li.find('em').string list_[1] = li.find(attrs=['class','title']).string list_[2] = li.find(attrs=['class','bd']).find(attrs=['class','']).string.strip().split(\" \")[1].replace(\" \",'') list_[3] = li.find(attrs=['class','bd']).find(attrs=['class','']).string.strip().split(\" \")[4].replace(\" \",'') list_[4] = li.find(attrs=['class','star']).span.find_next_sibling().string.strip() list_[5] = li.find(attrs=['class','star']).span.find_next_sibling().find_next_sibling().find_next_sibling().string.strip() if li.find(attrs=['class','quote']) is not None: list_[6] = li.find(attrs=['class','quote']).span.string else: list_[6] = None ulist.append([list_[0],list_[1],list_[2],list_[3],list_[4],list_[5],list_[6]]) def printUniverse(ulist): tplt = '{0:^4}\\t{1:^10}\\t{2:10}\\t{3:10}\\t{4:10}\\t{5:10}' print(tplt.format(\"排名\",\"电影名称\",\"导演\",\"主演\",\"评分\",\"评价人数\",chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],u[2],u[3],u[4],u[5],chr(12288))) def main(): ulist = [] for i in range(10): url = 'https://movie.douban.com/top250?start=' + str( 25 * i ) html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"基于Bilibili热门视频Top100最热分区爬取与分析","slug":"基于Bilibili热门视频Top100最热分区爬取与分析","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-27T07:43:57.645Z","comments":true,"path":"2019/05/03/ji-yu-bilibili-re-men-shi-pin-top100-zui-re-fen-qu-pa-qu-yu-fen-xi/","link":"","permalink":"http://ailous.top/2019/05/03/ji-yu-bilibili-re-men-shi-pin-top100-zui-re-fen-qu-pa-qu-yu-fen-xi/","excerpt":"","text":"网络爬虫——基于Bilibili热门视频Top100最热分区爬取与分析这个主要是配合之前报告使用的，这里就不多赘述了。先爬地址，再爬取地址里的分区，最后整合。csv存储。 实验内容目标网站：Bilibili热门视频Top100 目标网址：https://www.bilibili.com/ranking?（每过几天都会变的哦） from lxml import etree import time import jieba import numpy as np from PIL import Image import requests import re from requests.exceptions import RequestException from wordcloud import WordCloud as wc import csv class getUrl (): def __init__(self,url): self.url = url def getTxt(self): self.headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) result=newhtml.xpath('//*[@id=\"app\"]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[1]/a//@href') return result class getAvNum(): def __init__(self,url): self.url = url def getTxt(self): self.headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) rlist=newhtml.xpath('//*[@id=\"viewbox_report\"]/div[1]/span[1]/a[1]//text()') return rlist class Bilibili(): def __init__(self,oid): self.headers={ 'Host': 'api.bilibili.com', 'Connection': 'keep-alive', 'Cache-Control': 'max-age=0', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Cookie': 'finger=edc6ecda; LIVE_BUVID=AUTO1415378023816310; stardustvideo=1; CURRENT_FNVAL=8; buvid3=0D8F3D74-987D-442D-99CF-42BC9A967709149017infoc; rpdid=olwimklsiidoskmqwipww; fts=1537803390' } self.url='https://api.bilibili.com/x/v1/dm/list.so?oid='+str(oid) self.barrage_reault=self.get_page() def get_page(self): try: time.sleep(0.5) response=requests.get(self.url,headers=self.headers) except Exception as e: print('获取xml内容失败,%s' % e) return False else: if response.status_code == 200: with open('bilibili.xml','wb') as f: f.write(response.content) return True else: return False def param_page(self): time.sleep(1) if self.barrage_reault: html=etree.parse('bilibili.xml',etree.HTMLParser()) results=html.xpath('//d//text()') return results def remove_double_barrage(resultlist): double_barrage=[] results=[] barrage=set() for result in resultlist: if result not in results: results.append(result) else: double_barrage.append(result) barrage.add(result) return double_barrage,results,barrage def make_wordCould(resultlist): double_barrages,results,barrages=remove_double_barrage(resultlist) with open('barrages.txt','w', -1, 'utf-8', None, None) as f: for barrage in barrages: amount=double_barrages.count(barrage) stt = barrage+':'+str(amount+1)+'\\n' f.write(stt) stop_words=['【','】',',','.','?','!','。'] words=[] if results: for result in results: for stop in stop_words: result=''.join(result.split(stop)) words.append(result) # 列表拼接成字符串 words=''.join(words) words=jieba.cut(words) words=''.join(words) luo=np.array(Image.open('洛天依.jpg')) w=wc(font_path='‪C:/Windows/Fonts/SIMYOU.TTF',background_color='white',width=1600,height=1600,max_words=2000,mask=luo) w.generate(words) w.to_file('luo.jpg') def csvdata(data): with open('top20.csv','w',encoding = 'utf-8',newline='')as csvfile: fieldnames = ['分类'] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({'分类':i[0]}) print('ok') def main(): url=\"https://www.bilibili.com/ranking?spm_id_from=333.851.b_7072696d61727950616765546162.3\" urls = getUrl(url) strUrl = urls.parsePage() ress = [] for i in strUrl: AV = getAvNum(i) oid = AV.parsePage() ress.append(oid) csvdata(ress) if __name__ == '__main__': main()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"xpath","slug":"xpath","permalink":"http://ailous.top/tags/xpath/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"基于Bilibili热门视频Top100弹幕的数据爬取与分析（源代码）","slug":"基于Bilibili热门视频Top100弹幕的数据爬取与分析（源代码）","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-26T03:53:04.984Z","comments":true,"path":"2019/05/03/ji-yu-bilibili-re-men-shi-pin-top100-dan-mu-de-shu-ju-pa-qu-yu-fen-xi-yuan-dai-ma/","link":"","permalink":"http://ailous.top/2019/05/03/ji-yu-bilibili-re-men-shi-pin-top100-dan-mu-de-shu-ju-pa-qu-yu-fen-xi-yuan-dai-ma/","excerpt":"","text":"网络爬虫——基于Bilibili热门视频Top100弹幕的数据爬取与分析实验内容目标网站：Bilibili热门视频Top100 目标网址：https://www.bilibili.com/ranking?（每过几天都会变的哦） 任务要求实现对于Bilibili热门视频Top100弹幕的数据爬取与分析 源码from lxml import etree import time import jieba import numpy as np from PIL import Image import requests import re from requests.exceptions import RequestException from wordcloud import WordCloud as wc class getUrl (): def __init__(self,url): self.url = url def getTxt(self): self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[1]/a//@href&#39;) return result class getAvNum(): def __init__(self,url): self.url = url def getTxt(self): self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) rlist=newhtml.xpath(&#39;/html/head/meta[10]//@content&#39;) resp = requests.get(rlist[0],headers=self.headers) match_rule = r&#39;cid=(.*?)&amp;aid&#39; oid = re.search(match_rule,resp.text).group().replace(&#39;cid=&#39;,&#39;&#39;).replace(&#39;&amp;aid&#39;,&#39;&#39;) return oid class Bilibili(): def __init__(self,oid): self.headers={ &#39;Host&#39;: &#39;api.bilibili.com&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cache-Control&#39;: &#39;max-age=0&#39;, &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36&#39;, &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.9&#39;, &#39;Cookie&#39;: &#39;finger=edc6ecda; LIVE_BUVID=AUTO1415378023816310; stardustvideo=1; CURRENT_FNVAL=8; buvid3=0D8F3D74-987D-442D-99CF-42BC9A967709149017infoc; rpdid=olwimklsiidoskmqwipww; fts=1537803390&#39; } self.url=&#39;https://api.bilibili.com/x/v1/dm/list.so?oid=&#39;+str(oid) self.barrage_reault=self.get_page() def get_page(self): try: time.sleep(0.5) response=requests.get(self.url,headers=self.headers) except Exception as e: print(&#39;获取xml内容失败,%s&#39; % e) return False else: if response.status_code == 200: with open(&#39;bilibili.xml&#39;,&#39;wb&#39;) as f: f.write(response.content) return True else: return False def param_page(self): time.sleep(1) if self.barrage_reault: html=etree.parse(&#39;bilibili.xml&#39;,etree.HTMLParser()) results=html.xpath(&#39;//d//text()&#39;) return results def remove_double_barrage(resultlist): double_barrage=[] results=[] barrage=set() for result in resultlist: if result not in results: results.append(result) else: double_barrage.append(result) barrage.add(result) return double_barrage,results,barrage def make_wordCould(resultlist): double_barrages,results,barrages=remove_double_barrage(resultlist) # 重词计数 with open(&#39;barrages.txt&#39;,&#39;w&#39;, -1, &#39;utf-8&#39;, None, None) as f: for barrage in barrages: amount=double_barrages.count(barrage) stt = barrage+&#39;:&#39;+str(amount+1)+&#39;\\n&#39; f.write(stt) # 设置停用词 stop_words=[&#39;【&#39;,&#39;】&#39;,&#39;,&#39;,&#39;.&#39;,&#39;?&#39;,&#39;!&#39;,&#39;。&#39;] words=[] if results: for result in results: for stop in stop_words: result=&#39;&#39;.join(result.split(stop)) words.append(result) # 列表拼接成字符串 words=&#39;&#39;.join(words) words=jieba.cut(words) words=&#39;&#39;.join(words) luo=np.array(Image.open(&#39;洛天依.jpg&#39;)) w=wc(font_path=&#39;‪C:/Windows/Fonts/SIMYOU.TTF&#39;,background_color=&#39;white&#39;,width=1600,height=1600,max_words=2000,mask=luo) w.generate(words) w.to_file(&#39;luo.jpg&#39;) def main(): url=&quot;https://www.bilibili.com/ranking?spm_id_from=333.851.b_7072696d61727950616765546162.3&quot; urls = getUrl(url) strUrl = urls.parsePage() ress = [] for i in strUrl: AV = getAvNum(i) oid = AV.parsePage() b=Bilibili(oid) for j in b.param_page(): ress.append(j) make_wordCould(ress) if __name__ == &#39;__main__&#39;: main() 具体文件可以到我的github上面下载","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"xpath","slug":"xpath","permalink":"http://ailous.top/tags/xpath/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"基于Bilibili热门视频Top100最热分区爬取与分析（报告）","slug":"基于Bilibili热门视频Top100弹幕的数据爬取与分析（报告版）","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-26T03:50:14.229Z","comments":true,"path":"2019/05/03/ji-yu-bilibili-re-men-shi-pin-top100-dan-mu-de-shu-ju-pa-qu-yu-fen-xi-bao-gao-ban/","link":"","permalink":"http://ailous.top/2019/05/03/ji-yu-bilibili-re-men-shi-pin-top100-dan-mu-de-shu-ju-pa-qu-yu-fen-xi-bao-gao-ban/","excerpt":"","text":"一、研究背景能够在观看视频的过程中发表自己的评论，并且评论可以在你所希望的时间点、位置以滑行或停留的方式出现在视频中，所有观看视频的人都可以看见评论，这样一类的评论叫做弹幕，此类网站叫弹幕网站。弹幕视频系统源自日本弹幕视频分享网站（niconico动画），国内首先引进为AcFun以及后来的bilibili。大量吐槽评论从屏幕飘过时效果看上去像是飞行射击游戏里的弹幕，所以NICO网民将这种有大量的吐槽评论出现时的效果做弹幕。在中国，本来只有大量评论同时出现才能叫弹幕，但是随着误用单条评论也能叫弹幕了。在国内通常被认为本意是军事用语中密集的炮火射击，过于密集以至于像一张幕布一样。英文称“Bullet Hell”（子弹地狱）或“Bullet Curtain”（弹幕）弹幕可以给观众一种“实时互动”的错觉，虽然不同弹幕的发送时间有所区别，但是其只会在视频中特定的一个时间点出现，因此在相同时刻发送的弹幕基本上也具有相同的主题，在参与评论时就会有与其他观众同时评论的错觉。而传统的播放器评论系统是独立于播放器之外的，因此评论的内容大多围绕在整个视频上，话题性不强，也没有“实时互动”的感觉。 二、研究目的及意义在视频软件层出不穷的当下。除了优质的视频内容，对于社区文化如何探索成为当下各个公司极力探索的目标。新时期自媒体盛行，BiliBili相较于同类软件，其社区文化呈现欣欣向荣之势，拥有较高的日活跃量。而弹幕文化俨然成为其不可或缺的一部分。2019年年末，Bilibili曾发布年度十大热词，‘AWSL’夺得头筹。紧随其后的是‘名场面’，‘逮虾户’……，这些我们日常生活中耳熟能详的‘新词汇’。由于互联网屏蔽了用户的设备差异，因此网络视频服务商可以获得格式统一、更加规范的用户使用数据，根据不同用户的使用数据完成视频推荐。美国著名电子商务企业亚马逊很早就开始探索推荐算法在电子商务中的应用，并已经取得了显著的成果。发表弹幕、观看弹幕，本身就有一种实时互动的错觉，完成着实实在在的、直接的互动。网站会根据用户的点击量进行视频推荐，参与热烈讨论的视频会出现在主页，受众能最快时间地看到页面。将来也许还可以通过搜索引擎的优化，把热门话题，热门词语，热门搜索等作为视频推荐的依据，让受众观看到与自己兴趣最相符的视频。通过对于当前时间段热门排名TOP100中弹幕进行分析，将数据进行可视化处理，得到最热词汇，既可以知道在这一时间段网络舆论流行的大体趋势，把握用户心里态度，加强受众的互动反馈。还可以激发用户对于弹幕文化的探索的兴趣。使得用户保持新鲜度，延长软件寿命。三、实验环境及技术介绍1.爬虫技术简介网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，其实通俗的讲就是通过程序去获取web页面上自己想要的数据，也就是自动抓取数据。爬虫的目地在于将目标网页数据下载至本地，以便进行后续的数据分析.爬虫技术的兴起源于海量网络数据的可用性,通过爬虫技术,能够较为容易的获取网络数据，并通过对数据的分析,得出有具有价值的结论。Python语言简单易用,现成的爬虫框架和工具包降低了使用门槛,具体使用时配合正则表达式的运用,使得数据抓取工作变得生动有趣。2.所用到的python库2.1requestsRequests 是用Python语言编写，基于 urllib，采用Apache2 Licensed 开源协议的HTTP库。Requests 继承了urllib的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的URL和POST 数据自动编码。主要用于请求URL，获取返回信息，打印输出响应头，和重定向等。在爬取数据过程中，使用了request库的get方法来发送请求。Request请求方式有：GET:请求指定的页面信息，并返回实体主体。HEAD:只请求页面的首部。POST:请求服务器接受所指定的文档作为对所标识的URI的新的从属实体。PUT:从客户端向服务器传送的数据取代指定的文档的内容。DELETE:请求服务器删除指定的页面。get和post比较常见GET请求将提交的数据放置在HTTP请求协议头中POST提交的数据则放在实体数据中2.2XPathXPath 是一门在 XML文档中查找信息的语言。用于在 XML文档中通过元素和属性进行导航，选取XML文档中的节点或者节点集。这些路径表达式和在常规的电脑文件系统中看到的表达式非常相似。在爬取豆瓣TOP250的过程中，使用xpath查找歌曲名、歌手、评分等信息。图2-2 Xpath路径表达式图2.3lxmllxml 是一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML /XML数据。lxml和正则一样，也是用C实现的，是一款高性能的Python HTML /XML 解析器，可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。使用lxml.etree处理XML文档。简要讲述ElementTree API的主要要概念，和一些简单的增强，让处理XML更简单。2.4Jieba jieba是优秀的中文分词第三方库。通过分词的方式从中文文本获得单个的词语，但是属于拓展库，需要额外安装。其拥有三种分词模式，最简单只需掌握一个函数。jieba分词的原理： 其依靠中文词库，利用一个中文词库，确定汉字之间的关联概率， 将汉字间相邻概率大的组成词组，形成分词结果， 除了分词，用户还可以添加自定义的词组。2.5WordCloudwordcloud库，可以说是python非常优秀的词云展示第三方库。词云以词语为基本单位更加直观和艺术的展示文本，也叫文字云，对文本中出现频率较高的“关键词”予以视觉化的展现，过滤掉大量的低频低质的文本信息，使得浏览者只要一眼扫过文本就可领略文本的主旨。3.数据分析工具3.1Jupyter NotebookNotebooks是Donald Knuth 1984年提出的文本化编程的一种形式。Jupyter Notebook 的本质是一个 Web 应用程序，结合文本化编程，文本和代码交错在一起，而不是分成两个独立地本分。是个集成文本，数学公式，代码和可视化的可分享文本。便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。Notebooks 很快已经成为了数据操作不可或缺的工具。它在 大数据清理和探究,可视化,机器学习, 和 大数据分析中都有广泛运用。并且Notebooks 可以直接在github直接被读取. 这是一个非常有用的功能，可以方便地分享。3.2ExcelExcel不仅是一个数据存储工具，还是一个简单的数据分析工具，添加EXCEL数据分析插件后，可以做一些简单相关、回归等分析。而Excel可以说是万能但又不是万能的，学习Excel就是为了用来统计数据分析数据的，大而复杂的数据和分析有时候用Excel处理并不是最佳选择。Excel和数据库之间可以进行数据转换，但是当Excel的数据量过大的时候，它的查询和计算的速度会明显下降。Excel提供了有限的安全性，它只能限制用户访问和修改的权限，但是无法对用户进行角色的管理，也不能对数据进行行级的访问限制。当然Excel也有自身的的优势之处：1、数据透视功能。2、统计分析，非常独特，常用的检验方式一键搞定。3、图表功能， Excel拥有各种丰富的可开发的图表形式的独门武工。4、自动汇总功能，这个功能其他程序都有，但是Excel简便灵活。总地来说，Excel适合于开发单机版、访问量与开发维护量都不是很大、对数据有分析建模功能的应用程序。四、数据爬取4.1抓取Bilibili热门视频Top100的网址本实验在浏览器搜索Bilibili热门视频Top100的网址,就能得到要采集数据的页面，它的URL为https://www.bilibili.com/ranking，页面如下图4-1所示。图4-1Bilibili热门视频Top100排行图4.2对网址进行转化，获得Oid号码通过对网页源代码的处理，将网页源地址中的地址数据转换成AV地址样式，再通过正则表达式’cid=(.*?)&amp;aid’，获取其Cid码，最后再转化成Oid码。获取Oid码的源代码如下图4-2所示。图4-2 Oid码源代码获取图获取Oid码的代码运行结果如下图4-2所示。图4-3代码运行结果4.3抓取弹幕信息lxml文件首先，导入requests包，利用requests构建一个简单地的GET请求，把网页的headers（其中包含了User-Agent字段信息，是浏览器的标识信息）输入，这时网站会判断如果是客户端发起的GET请求，它会返回相应的请求信息。通过Oid码才能得到弹幕存储所在的lxml文件。Lxml代码如下图4-3所示。图4-4 Lxml获取代码图4-5 Lxml文件图4.4抓取所有弹幕信息存储并处理将所得到的lxml文件，通过xpath进行抓取并且存储到数组results中。图4-6 弹幕信息爬取代码通过append方法将所有弹幕信息存储，以待处理，如下所示：图4-7 所有弹幕整合代码通过自定义的remove_double_barrage（）方法将弹幕信息进行去重处理，如下所示：图4-8 弹幕信息去重代码4.5存储数据利用lxml库采集视频的弹幕内容信息。将弹幕内容barrage，及弹幕出现次数amount进行存储，具体代码如下图4-3所示。图4-9存储数据图4.6数据展示将采集的数据合并在一起,将采集的数据存入txt中，从文件中可以看出，收集的信息字段为‘弹幕内容’，‘出现次数’。进行数据分析就是对这两字段进行数据分析和可视化展示。部分数据展示如下图4-4所示：图4-10数据展示图 五、数据分析利用wordCloud对爬取的数据进行分析来进一步呈现。5.1最热弹幕分析对Bilibili热门视频Top100弹幕进行分析，选取榜上所有弹幕进行针对性分析：从图中可以明显看出，统计期间最热的弹幕要数“奔涌吧，后浪”。其后是：“狗头”“后浪，奔涌吧”“doge”……图5-1 最热弹幕词云5.2最热区分析Bilibili包括有动画、番剧、音乐、舞蹈、游戏、科技、生活、鬼畜、时尚、广告、娱乐、影视等多个分区，通过Top100上榜分区处理分析得到当前最热分区。图5-2 数据统计样本·图5-3 样本分析 六、总结从定义上来说，爬虫就是模拟用户自动浏览并且保存网络数据的程序，当然，大部分的爬虫都是爬取网页信息（文本，图片，媒体流）。无论是动态还是静态网页，所有的用户可以直观看到的，都有爬取下来的可能。最简单的流程：获取网页源代码，分析源码，存储数据。即从功能上来讲的，数据采集，处理，储存三个部分。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页URL，并重复上述过程，直到达到系统的某一条件时停止。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索；对于聚焦爬虫来说，这一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。发起请求：通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器响应。获取响应内容：如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。解析内容：得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。保存数据：保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。七、附录源代码请看下一篇","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"文章","slug":"文章","permalink":"http://ailous.top/tags/文章/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"豆瓣电影排行榜数据抓取（初级）——BS4","slug":"网络爬虫——豆瓣电影排行榜数据抓取（初级）","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-27T07:45:02.050Z","comments":true,"path":"2019/05/03/wang-luo-pa-chong-dou-ban-dian-ying-pai-xing-bang-shu-ju-zhua-qu-chu-ji/","link":"","permalink":"http://ailous.top/2019/05/03/wang-luo-pa-chong-dou-ban-dian-ying-pai-xing-bang-shu-ju-zhua-qu-chu-ji/","excerpt":"","text":"网络爬虫——豆瓣电影排行榜数据抓取（初级）目标网址：豆瓣电影排行：https://movie.douban.com/top250?start= 目标数据描述：（1）排名（2）电影名称任务明细： （1）使用requests库实现该网站网页源代码的获取； （2）使用BeautifulSoup对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 可以选择定义全局列表，将目标数据获取后添加到列表中，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。 下一阶段，目标数据增加导演、主演、评价人数等信息，将尽可能多的数据抓取保存。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding result=(result.text.replace('','')).replace('','') return result except: return \"\" def findUniverse(ulist , html): soup = BeautifulSoup(html,\"html.parser\") list = [0,0] for li in soup.find(attrs=['class','grid_view']).children: if isinstance(li ,bs4.element.Tag): list[0] = li.find('em').string list[1] = li.find(attrs=['class','title']).string ulist.append([list[0],list[1]]) def printUniverse(ulist): tplt = '{0:^10}\\t{1:^10}' print(tplt.format(\"排名\",\"电影名称\",chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],chr(12288))) def main(): ulist = [] url = 'https://movie.douban.com/top250?start=' html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——抓取TIOBE指数前20名排行开发语言","slug":"网络爬虫——抓取TIOBE指数前20名排行开发语言","date":"2019-04-27T05:47:40.000Z","updated":"2020-05-27T07:44:38.010Z","comments":true,"path":"2019/04/27/wang-luo-pa-chong-zhua-qu-tiobe-zhi-shu-qian-20-ming-pai-xing-kai-fa-yu-yan/","link":"","permalink":"http://ailous.top/2019/04/27/wang-luo-pa-chong-zhua-qu-tiobe-zhi-shu-qian-20-ming-pai-xing-kai-fa-yu-yan/","excerpt":"","text":"网络爬虫——抓取TIOBE指数前20名排行开发语言目标网址TIOBE指数前20名排行开发语言：https://www.tiobe.com/tiobe-index/ 说明 TIOBE排行榜是根据互联网上有经验的程序员、课程和第三方厂商的数量，并使用搜索引擎（如Google、Bing、Yahoo!）以及Wikipedia、Amazon、YouTube统计出排名数据，只是反映某个编程语言的热门程度，并不能说明一门编程语言好不好，或者一门语言所编写的代码数量多少。 该指数可以用来检阅开发者的编程技能能否跟上趋势，或是否有必要作出战略改变，以及什么编程语言是应该及时掌握的。观察认为，该指数反应的虽并非当前最流行或应用最广的语言，但对世界范围内开发语言的走势仍具有重要参考意义。 目标数据：（如上表所示） （1）2020年3月的排名（2）2019年3月排名（3）编程语言（4）评分（5）变化率 明细：（1）使用urllib或者requests库抓取目标网页中的网页源代码； （2）使用lxml库中的xpath方法解析源代码，提取上面所示的目标数据，并打印输出； （3）尝试着使用try..except方法及时捕获异常。 （4）可以尝试将获取的数据保存到文本文件中。 源码import requests from requests.exceptions import RequestException from lxml import etree def one_to_page(url): headers={ 'user-agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3756.400 QQBrowser/10.5.4039.400' } try: response=requests.get(url,headers=headers) body=response.text return body except RequestException as e: print('request is error!',e) def parsePage(html): htmlNew = etree.HTML(html,etree.HTMLParser()) result = htmlNew.xpath('//table[contains(@class,\"table-top20\")]/tbody/tr//text()') pos = 0 for i in range(20): if i == 0: yield result[i:5] else: yield result[pos:pos+5] pos += 5 def printRank(data): for i in data: rank = { \"2020年3月\":i[0], \"2019年3月\":i[1], \"编程语言\":i[2], \"评分\":i[3], \"变化率\":i[4], } print(rank) def printRankEasy(data): tplt = \"{0:^10}\\t{1:^10}\\t{2:^20}\\t{3:^10}\\t{4:^10}\" print(tplt.format(\"2020年3月\",\"2019年3月\",\"编程语言\",\"评分\",\"变化率\",chr(12288))) for i in data: print(tplt.format(i[0],i[1],i[2],i[3],i[4],chr(12288))) def main(): url = 'https://www.tiobe.com/tiobe-index/' html = one_to_page(url) data = parsePage(html) printRankEasy(data) main() 输出如下","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"Xpath","slug":"Xpath","permalink":"http://ailous.top/tags/Xpath/"},{"name":"TIOBE指数","slug":"TIOBE指数","permalink":"http://ailous.top/tags/TIOBE指数/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"中国大学排名数据抓取","slug":"网络爬虫——中国大学排名数据抓取","date":"2019-04-20T05:47:40.000Z","updated":"2020-05-27T07:44:20.249Z","comments":true,"path":"2019/04/20/wang-luo-pa-chong-zhong-guo-da-xue-pai-ming-shu-ju-zhua-qu/","link":"","permalink":"http://ailous.top/2019/04/20/wang-luo-pa-chong-zhong-guo-da-xue-pai-ming-shu-ju-zhua-qu/","excerpt":"","text":"网络爬虫——中国大学排名数据抓取目标网址中国大学排名网：http://www.zuihaodaxue.com/zuihaodaxuepaiming2019.html 全球有很多份大学排名，这里以上海交通大学研发的“软科中国最好大学排名2019”为例，，编写“大学排名爬虫”，从网络上获取数据 。拟从该网址爬取该名单上310 所国内大学的排名数据，并将它们打印出来。 大学排名爬虫的构建需要三个重要步骤： 第一，从网络上获取网页内容； 第二，分析网页内容并提取有用数据到恰当的数据结构中； 第三，利用数据结构展示或进一步处理数据。 由于大学排名是一个典型的二维数据，因此，采用二维列表存储该排名所涉及的表单数据。具体来说，采用requests 库爬取网页内容，使用beautifulsoup4 库分析网页中数据，提取310 个学校的排名及相关数据，存储到二维列表中，最后采用用户偏好的方式打印出来。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): try: result = requests.get(url,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def findUniverse(ulist , html): soup = BeautifulSoup(html,\"html.parser\") for tr in soup.find(attrs=['class','hidden_zhpm']).children: if isinstance(tr ,bs4.element.Tag): tds = tr('td') ulist.append([tds[0].string, tds[1].string,tds[3].string ]) def printUniverse(ulist): tplt = '{0:^10}\\t{1:{3}^10}\\t{2:^10}' print(tplt.format(\"排名\",\"学校名称\",\"总分\",chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],u[2],chr(12288))) def main(): ulist = [] url = 'http://www.zuihaodaxue.com/zuihaodaxuepaiming2019.html' html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 结果输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——前程无忧网数据获取及存储（低级）","slug":"网络爬虫——前程无忧网数据获取及存储（低级）","date":"2019-04-10T05:47:40.000Z","updated":"2020-05-27T07:44:32.329Z","comments":true,"path":"2019/04/10/wang-luo-pa-chong-qian-cheng-wu-you-wang-shu-ju-huo-qu-ji-cun-chu-di-ji/","link":"","permalink":"http://ailous.top/2019/04/10/wang-luo-pa-chong-qian-cheng-wu-you-wang-shu-ju-huo-qu-ji-cun-chu-di-ji/","excerpt":"","text":"网络爬虫——前程无忧网数据获取及存储（低级）目标网站：前程无忧招聘网目标网址：https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html 目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 任务要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）通过Xpath解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到txt文本文件中。 这里使用的是Xpath，相对于之前猫眼电影使用的，这个较为简单，但是数据处理上较为复杂。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69' } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return \"\" def parsePage(html): ulist = [] clist = [] rlist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath('//*[@id=\"resultList\"]/div[@class=\"el\"]//text()') for i in range(len(result)): ulist.append(result[i].replace(\" \",\"\").replace('\\r',\"\").replace(\"\\n\",'')) while '' in ulist: ulist.remove('') length = len(ulist) weight = int(length / 5 ) for i in range(weight): for j in range(5): clist.append(ulist[i*5+j]) rlist.append(clist) clist = [] return rlist # def txtdata(data): # with open('top20.txt','w')as file: # for i in data: # for j in i: # print(j) # print('successful') def storedata(data): with open('top20.txt','w',encoding = 'utf-8')as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+'\\n') print('ok') def csvdata(data): with open('top20.csv','w',encoding = 'utf-8',newline='')as csvfile: fieldnames = ['职位名','公司名','工作地点','薪资','工作时间'] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({'职位名':i[0],'公司名':i[1],'工作地点':i[2],'薪资':i[3],'工作时间':i[4]}) print('ok') def main(): url=\"https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html\" html=getHtmlText(url) rlist=parsePage(html) # txtdata(data) storedata(rlist) csvdata(rlist) main()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"Xpath","slug":"Xpath","permalink":"http://ailous.top/tags/Xpath/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"人工智能——mnist（手写识别）","slug":"人工智能——mnist（手写识别）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:35:59.218Z","comments":true,"path":"2018/12/09/ren-gong-zhi-neng-mnist-shou-xie-shi-bie/","link":"","permalink":"http://ailous.top/2018/12/09/ren-gong-zhi-neng-mnist-shou-xie-shi-bie/","excerpt":"","text":"人工智能——mnist（手写识别）因为时间较久了，就不详细介绍了。简单的说，一开始对已有的大量数字图片进行训练生成模型。然后通过对一张手写的数字图片进行读取，进入模型匹配，输出结果。 源码# Python3 # 使用LeNet5的七层卷积神经网络用于MNIST手写数字识别 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True) # 为输入图像和目标输出类别创建节点 x = tf.placeholder(tf.float32, shape=[None, 784]) # 训练所需数据 占位符 y_ = tf.placeholder(tf.float32, shape=[None, 10]) # 训练所需标签数据 占位符 # *************** 构建多层卷积网络 *************** # # 权重、偏置、卷积及池化操作初始化,以避免在建立模型的时候反复做初始化操作 def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) # 取随机值，符合均值为0，标准差stddev为0.1 return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) # x 的第一个参数为图片的数量，第二、三个参数分别为图片高度和宽度，第四个参数为图片通道数。 # W 的前两个参数为卷积核尺寸，第三个参数为图像通道数，第四个参数为卷积核数量 # strides为卷积步长，其第一、四个参数必须为1，因为卷积层的步长只对矩阵的长和宽有效 # padding表示卷积的形式，即是否考虑边界。\"SAME\"是考虑边界，不足的时候用0去填充周围，\"VALID\"则不考虑 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') # x 参数的格式同tf.nn.conv2d中的x，ksize为池化层过滤器的尺度，strides为过滤器步长 def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #把x更改为4维张量，第1维代表样本数量，第2维和第3维代表图像长宽， 第4维代表图像通道数 x_image = tf.reshape(x, [-1,28,28,1]) # -1表示任意数量的样本数,大小为28x28，深度为1的张量 # 第一层：卷积 W_conv1 = weight_variable([5, 5, 1, 32]) # 卷积在每个5x5的patch中算出32个特征。 b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # 第二层：池化 h_pool1 = max_pool_2x2(h_conv1) # 第三层：卷积 W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # 第四层：池化 h_pool2 = max_pool_2x2(h_conv2) # 第五层：全连接层 W_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) # 在输出层之前加入dropout以减少过拟合 keep_prob = tf.placeholder(\"float\") h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) # 第六层：全连接层 W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) # 第七层：输出层 y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) # *************** 训练和评估模型 *************** # # 为训练过程指定最小化误差用的损失函数，即目标类别和预测类别之间的交叉熵 cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv)) # 使用反向传播，利用优化器使损失函数最小化 train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # 检测我们的预测是否真实标签匹配(索引位置一样表示匹配) # tf.argmax(y_conv,dimension), 返回最大数值的下标 通常和tf.equal()一起使用，计算模型准确度 # dimension=0 按列找 dimension=1 按行找 correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) # 统计测试准确率， 将correct_prediction的布尔值转换为浮点数来代表对、错，并取平均值。 accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) saver = tf.train.Saver() # 定义saver # *************** 开始训练模型 *************** # with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(1000): batch = mnist.train.next_batch(50) if i%100 == 0: # 评估模型准确度，此阶段不使用Dropout train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0}) print(\"step %d, training accuracy %g\"%(i, train_accuracy)) # 训练模型，此阶段使用50%的Dropout train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) saver.save(sess, './save/model.ckpt') #模型储存位置 print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images [0:2000], y_: mnist.test.labels [0:2000], keep_prob: 1.0}))","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"手写识别","slug":"手写识别","permalink":"http://ailous.top/tags/手写识别/"},{"name":"mnist","slug":"mnist","permalink":"http://ailous.top/tags/mnist/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"嵌入式—LM3S1138介绍","slug":"嵌入式—LM3S1138介绍","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:31:17.598Z","comments":true,"path":"2018/12/09/qian-ru-shi-lm3s1138-jie-shao/","link":"","permalink":"http://ailous.top/2018/12/09/qian-ru-shi-lm3s1138-jie-shao/","excerpt":"","text":"嵌入式—LM3S1138介绍这是我用的较久的一款芯片，无论是电赛还是课程设计，兼容性都很好，现在想把所有数据整理一下，这是开发板的相关文档。后面会有相关代码介绍，和案例介绍。 EasyARM1138——内嵌 USB 仿真器的 Cortex-M3 开发板 EasyARM1138 是专门针对广大电子信息专业在校大学生而设计的一款基于 ARMCortex™-M3 先进内核的高性能、低价格开发板，用于教学、毕业设计、电子竞赛，等等。也是广大单片机爱好者、开发工程师首选的 Cortex™-M3 开发板。 EasyARM1138 的核心MCU是美国Luminary Micro公司的Stellaris®（群星）系列ARM之LM3S1138。该芯片采用的是国际上最优秀的MCU内核设计公司ARM最新推出的先进Cortex™-M3 处理器；由国内最大、技术最强的晶圆制造公司台积电（TSMC）代工；经世界上最专业的封装测试公司（OSE、i2a/IPAC）层层把关，确保产品的可靠性。Stellaris®（群星） 系列ARM芯片在电磁兼容性方面的优势明显。 【产品图片】 【功能特点】● 强大的 MCU 内核 ◆ 32 位 ARM Cortex™-M3 内核（ARM v7M 架构） ◆ 兼容 Thumb 的 Thumb-2 指令集，提高代码密度 25%以上 ◆ 50MHz 运行频率，1.25 DMIPS/MHz，加快 35%以上 ◆ 单周期乘法指令，2～12 周期硬件除法指令 ◆ 快速可嵌套中断，6～12 个时钟周期 ◆ 具有 MPU 保护设定访问规则 ◆ 64KB 单周期 Flash, 16KB 单周期 SRAM ◆ 内置可编程的 LDO 输出 2.25V～2.75V，步进 50mV ◆ 支持非对齐数据的访问，有效地压缩数据到内存 ◆ 支持位操作，最大限度使用内存，并提供创新的外设控制 ◆ 内置系统节拍定时器（SysTick），方便操作系统移植 ● 丰富的外设资源 ◆ 7 组 GPIO，可配置为输入、输出、开漏、弱上拉等模式 ◆ 4 个 32 位 Timer，每个都可拆分为 2 个独立的 16 位子定时器， 具有定时、捕获、PWM、RTC 等丰富功能 ◆ 3 路全双工 UART，位速率高达 3.125Mbps，16 单元接收 FIFO 和发送 FIFO，支持串行红外协议（IrDA SIR） ◆ 2 路I2C，支持 100kbps标准模式、400kbps快速模式 ◆ 2 路 SSI，兼容 Freescale SPI、MICROWIRE、Texas Instruments 串行通信协议，位速率高达 25Mbps ◆ 6 路 16 位 PWM，通过 CCP 管脚能产生高达 25MHz 的方波 ◆ 3 个模拟比较器 ◆ 8 通道 10 位 ADC，采样速率可达 1M/s，附带温度传感器 ◆ 内置看门狗定时器（WatchDog Timer），确保芯片可靠运行 ● 内嵌 USB 接口的下载仿真器 ◆ 仅需插入一根 USB 电缆就能实现“三合一”功能： 5V 供电、程序下载与在线仿真、UART 串行通信 ◆ 不再要求电脑具有串口或并口，无论台式机还是笔记本电脑， 只要拥有 USB 1.1 或 USB 2.0 接口就能运用自如 ◆ 除了能够下载仿真自身以外，保留的 JTAG 接口还可以 用来仿真其它 LM3S 系列开发板，短接 JP2 短接器的GND 和 U-RST，还可实现 JTAG 接口对内仿真功能 ◆ USB 接口提供虚拟 UART 的功能，不需要额外的接口电路 ● 简明的外围电路设计，调试时无需任何连线和跳线，操作极为方便 ◆ 5 只 LED 指示灯 ◆ 3 只 KEY ◆ 1 只交流蜂鸣器，可演奏动听乐曲，如《梁祝》 ◆ 两排插针引出全部 GPIO 资源，以及 ADC0～7、5V/3.3V/GND ◆ GPIO 插针间距正好为 2000mil(50.8mm)，很容易插接在万用板 或其它自制的电路板上，为教学实验提供了极大方便 ● 在软件上采用“C 语言＋驱动库”的新概念开发模式 由于 Luminary Micro 官方免费提供了基于 C 语言（符合 ANSI C 标准）的驱动库软件包，并且源代码是公开的，因此用户完全可以摒弃晦涩难懂的汇编语言，也不需要掌握底层寄存器的操作细节，只要懂 C 语言就能轻松玩转 LM3S 系列 ARM。这也使得 32 位 ARM 的入门门槛大大降低。以下是操作 GPIO 端口点亮 LED 的示例，非常简捷： SysCtlPeriEnable(SYSCTL_PERIPH_GPIOG); // 第 1 步：使能 GPIOG 端口GPIOPinTypeOut(GPIO_PORTG_BASE , GPIO_PIN_3); // 第 2 步：设置 PG3 为输出GPIOPinWrite(GPIO_PORTG_BASE , GPIO_PIN_3 , 0x00); // 第 3 步：令 PG3 = 0，点亮 LED ● 从基础实验到课题设计 ◆ GPIO 实验：LED 闪烁发光、GPIO 中断、按键控制 ◆ Timer 实验：定时/计数、脉冲捕获、RTC 时钟、PWM 方波 ◆ 串行通信：UART、I2C、SSI(兼容SPI)、RS-4852广州致远电子有限公司 电话：020-22644252 传真：020-38601859 http://www.embedtools.com ◆ 串行红外通信实验（IrDA SIR） ◆ 模拟功能：模拟比较器信号检测，10 位 ADC 数据采集 ◆ 片内温度传感器实验 ◆ 大容量 SD 卡读写实验 ◆ 支持各类电机实验：直流电机、步进电机、三相电机， 提供运动控制 PID 算法 ◆ PWM 语音播放：能够对声音信号进行采集、存储和播放 ● 开发软件 ◆ IAR Embedded Workbench for ARM 4.42A 或 5.11(内嵌 USB 下载仿真器，推荐) ◆ Keil µVision3(无需 U-LINK 仿真器) ◆ 支持 µC/OS II 操作系统(提供移植代码) ◆ 提供《Stellaris 外设驱动库》快速安装方法及 C 语言源代码","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}],"tags":[{"name":"1138","slug":"1138","permalink":"http://ailous.top/tags/1138/"}],"keywords":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}]},{"title":"嵌入式——1138纯净版代码（PWM）","slug":"嵌入式——1138纯净版代码（PWM）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:38:05.460Z","comments":true,"path":"2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-pwm/","link":"","permalink":"http://ailous.top/2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-pwm/","excerpt":"","text":"这些是我自己常用的代码，比官方的要简洁些，主要是自用，还有相关模块，使用代码。 具体文件可以去我github上下载。 pwm简介 脉冲宽度调制是利用微处理器的数字输出来对模拟电路进行控制的一种非常有效的技术，广泛应用在从测量、通信到功率控制与变换的许多领域中。 脉冲宽度调制是一种模拟控制方式，其根据相应载荷的变化来调制晶体管基极或MOS管栅极的偏置，来实现晶体管或MOS管导通时间的改变，从而实现开关稳压电源输出的改变。这种方式能使电源的输出电压在工作条件变化时保持恒定，是利用微处理器的数字信号对模拟电路进行控制的一种非常有效的技术。 PWM控制技术以其控制简单，灵活和动态响应好的优点而成为电力电子技术最广泛应用的控制方式，也是人们研究的热点。由于当今科学技术的发展已经没有了学科之间的界限，结合现代控制理论思想或实现无谐振波开关技术将会成为PWM控制技术发展的主要方向之一。其根据相应载荷的变化来调制晶体管基极或MOS管栅极的偏置，来实现晶体管或MOS管导通时间的改变，从而实现开关稳压电源输出的改变。这种方式能使电源的输出电压在工作条件变化时保持恒定，是利用微处理器的数字信号对模拟电路进行控制的一种非常有效的技术。 源码main函数#include \"systemInit.h\" #include &lt;ctype.h> #include &lt;timer.h> #define PART_LM3S1138 #include &lt;pin_map.h> #include &lt;math.h> int T1A , T1B , T2A , T2B ; void Moto_Init(void){ //³õÊ¼»¯µç»ú SysCtlPeriEnable(SYSCTL_PERIPH_TIMER1); // Ê¹ÄÜTIMER1Ä£¿é SysCtlPeriEnable(SYSCTL_PERIPH_TIMER2); // Ê¹ÄÜTIMER2Ä£¿é SysCtlPeriEnable(CCP2_PERIPH); // Ê¹ÄÜCCP2ËùÔÚµÄGPIO¶Ë¿Ú SysCtlPeriEnable(CCP3_PERIPH); // Ê¹ÄÜCCP3ËùÔÚµÄGPIO¶Ë¿Ú SysCtlPeriEnable(CCP4_PERIPH); // Ê¹ÄÜCCP4ËùÔÚµÄGPIO¶Ë¿Ú SysCtlPeriEnable(CCP5_PERIPH); // Ê¹ÄÜCCP5ËùÔÚµÄGPIO¶Ë¿Ú GPIOPinTypeTimer(CCP2_PORT, CCP2_PIN); // ÉèÖÃÏà¹Ø¹Ü½ÅÎªTimer¹¦ÄÜ GPIOPinTypeTimer(CCP3_PORT, CCP3_PIN); // ÉèÖÃÏà¹Ø¹Ü½ÅÎªTimer¹¦ÄÜ GPIOPinTypeTimer(CCP4_PORT, CCP4_PIN); // ÉèÖÃÏà¹Ø¹Ü½ÅÎªTimer¹¦ÄÜ GPIOPinTypeTimer(CCP5_PORT, CCP5_PIN); // ÉèÖÃÏà¹Ø¹Ü½ÅÎªTimer¹¦ÄÜ TimerConfigure(TIMER1_BASE, TIMER_CFG_16_BIT_PAIR | // ÅäÖÃTimerÎªË«16Î»PWM TIMER_CFG_A_PWM | TIMER_CFG_B_PWM); TimerConfigure(TIMER2_BASE, TIMER_CFG_16_BIT_PAIR | // ÅäÖÃTimerÎªË«16Î»PWM TIMER_CFG_A_PWM | TIMER_CFG_B_PWM); TimerControlLevel(TIMER1_BASE, TIMER_BOTH, true); // ¿ØÖÆPWMÊä³ö·´Ïà TimerControlLevel(TIMER2_BASE, TIMER_BOTH, true); TimerLoadSet(TIMER1_BASE, TIMER_BOTH, 1000); // ÉèÖÃTimerBoth³õÖµ TimerLoadSet(TIMER2_BASE, TIMER_BOTH, 1000); TimerMatchSet(TIMER1_BASE, TIMER_A, 0); // ÉèÖÃTimerBÆ¥ÅäÖµ TimerMatchSet(TIMER1_BASE, TIMER_B, 0); // ÉèÖÃTimerBÆ¥ÅäÖµ TimerMatchSet(TIMER2_BASE, TIMER_A, 0); // ÉèÖÃTimerBÆ¥ÅäÖµ TimerMatchSet(TIMER2_BASE, TIMER_B, 0); // ÉèÖÃTimerBÆ¥ÅäÖµ TimerEnable(TIMER1_BASE, TIMER_BOTH); // Ê¹ÄÜTimer¼ÆÊý£¬PWM¿ªÊ¼Êä³ö TimerEnable(TIMER2_BASE, TIMER_BOTH); } // Ö÷º¯Êý£¨³ÌÐòÈë¿Ú£© int main(void) { jtagWait(); // ·ÀÖ¹JTAGÊ§Ð§£¬ÖØÒª£¡ clockInit(); // Ê±ÖÓ³õÊ¼»¯£º¾§Õñ£¬6MHz Moto_Init(); while(1) { TimerMatchSet(TIMER1_BASE, TIMER_A, T1A); // ÉèÖÃTimerBÆ¥ÅäÖµ TimerMatchSet(TIMER1_BASE, TIMER_B, T1B); // ÉèÖÃTimerBÆ¥ÅäÖµ TimerMatchSet(TIMER2_BASE, TIMER_A, T2A); // ÉèÖÃTimerBÆ¥ÅäÖµ TimerMatchSet(TIMER2_BASE, TIMER_B, T2B); // ÉèÖÃTimerBÆ¥ÅäÖµ } }","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}],"tags":[{"name":"1138","slug":"1138","permalink":"http://ailous.top/tags/1138/"},{"name":"PWM","slug":"PWM","permalink":"http://ailous.top/tags/PWM/"}],"keywords":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}]},{"title":"嵌入式——1138纯净版代码（12864液晶）","slug":"嵌入式——1138纯净版代码（12864液晶）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:37:34.676Z","comments":true,"path":"2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-12864-ye-jing/","link":"","permalink":"http://ailous.top/2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-12864-ye-jing/","excerpt":"","text":"这些是我自己常用的代码，比官方的要简洁些，主要是自用，还有相关模块，使用代码。 具体文件可以去我github上下载。就是这款。 lcd12864简介 带中文字库的128X64是一种具有4位/8位并行、2线或3线串行多种接口方式，内部含有国标一级、二级简体中文字库的点阵图形液晶显示模块；其显示分辨率为128×64，内置8192个1616点汉字，和128个168点ASCII字符集。利用该模块灵活的接口方式和简单、方便的操作指令，可构成全中文人机交互图形界面。可以显示8×4行16×16点阵的汉字。也可完成图形显示。低电压低功耗是其又一显著特点。由该模块构成的液晶显示方案与同类型的图形点阵液晶显示模块相比，不论硬件电路结构或显示程序都要简洁得多，且该模块的价格也略低于相同点阵的图形液晶模块。 源码main函数#include \"lcd_driver.H\" #include \"SoftI2C.h\" #include &lt;hw_types.h> #include &lt;hw_memmap.h> #include &lt;sysctl.h> #include &lt;gpio.h> #include &lt;stdio.h> #include \"LM3S1138_PinMap.H\" // 将较长的标识符定义为较短的形式 #define SysCtlPeriEnable SysCtlPeripheralEnable #define SysCtlPeriDisable SysCtlPeripheralDisable #define GPIOPinTypeIn GPIOPinTypeGPIOInput #define GPIOPinTypeOut GPIOPinTypeGPIOOutput #define GPIOPinTypeOD GPIOPinTypeGPIOOutputOD // 定义KEY #define KEY_PERIPH SYSCTL_PERIPH_GPIOG #define KEY_PORT GPIO_PORTG_BASE #define KEY_PIN GPIO_PIN_5 // 防止JTAG失效 void JTAG_Wait(void){ SysCtlPeriEnable(KEY_PERIPH); // 使能KEY所在的GPIO端口 GPIOPinTypeIn(KEY_PORT , KEY_PIN); // 设置KEY所在管脚为输入 if ( GPIOPinRead(KEY_PORT , KEY_PIN) == 0x00 ){ // 如果复位时按下KEY，则进入 while(1); // 死循环，以等待JTAG连接 } SysCtlPeriDisable(KEY_PERIPH); // 禁止KEY所在的GPIO端口 } // 定义全局的系统时钟变量 unsigned long TheSysClock = 12000000UL; // 延时 //============================================================================================= //*** 函 数:Delay()\\''' //*** 功 能：延时 //*** 参 数: x 延时的时间 //============================================================================================ void Delay(unsigned long x){ unsigned long DelayValue = 0; for (DelayValue = 0; DelayValue &lt; x; DelayValue++); } // 系统初始化 void SystemInit(void){/* SysCtlLDOSet(SYSCTL_LDO_2_50V); // 设置LDO输出电压 SysCtlClockSet(SYSCTL_USE_OSC | // 系统时钟设置，采用主振荡器 SYSCTL_OSC_MAIN | SYSCTL_XTAL_6MHZ | SYSCTL_SYSDIV_1); SysCtlLDOSet(SYSCTL_LDO_2_75V); // 配置PLL前须将LDO电压设置为2.75V */ SysCtlClockSet(SYSCTL_USE_PLL | // 系统时钟设置，采用PLL SYSCTL_OSC_MAIN | // 主振荡器 SYSCTL_XTAL_6MHZ | // 外接6MHz晶振 SYSCTL_SYSDIV_4); // 分频结果为20MHz TheSysClock = SysCtlClockGet(); // 获取系统时钟，单位：Hz } int main(void){ JTAG_Wait(); // 防止JTAG失效，重要！ SystemInit(); init_lcd(); I2C_Init(); while(1){ zlg_disp(); } } lcd_driver.c// 包含必要的头文件 // 包含必要的头文件 #include \"lcd_driver.h\" #include &lt;hw_types.h> #include &lt;hw_memmap.h> #include &lt;hw_sysctl.h> #include &lt;hw_gpio.h> #include &lt;hw_adc.h> #include &lt;sysctl.h> #include &lt;gpio.h> #include &lt;adc.h> #include &lt;stdio.h> // 将较长的标识符定义为较短的形式 #define SysCtlPeriEnable SysCtlPeripheralEnable #define SysCtlPeriDisable SysCtlPeripheralDisable #define GPIOPinTypeIn GPIOPinTypeGPIOInput #define GPIOPinTypeOut GPIOPinTypeGPIOOutput #define GPIOPinTypeOD GPIOPinTypeGPIOOutputOD #define LCD_PORT GPIO_PORTF_BASE #define CS GPIO_PIN_1 #define SDAT GPIO_PIN_2 #define SCLK GPIO_PIN_3 unsigned char pos[4]={0x80,0x90,0x88,0x98}; unsigned char data[4][16]; unsigned int gui_disp_buf[8][64]={0};//建立缓冲区域 const unsigned char ascii_tab[]={\"0123456789ABCDEF\"}; unsigned char line_sign=0; extern float fTemp; extern void Delay(unsigned long x); //============================================================================================ //*** 函 数: sdelay() //*** 功 能：延时函数 //*** 参 数: 延时计数数据 //============================================================================================ void sdelay(unsigned long x) { while(x--); } //============================================================================================ //*** 函 数:send_data() //*** 功 能：液晶串行移位数据 //*** 参 数: data 串行移位的数据 //============================================================================================ void send_data(unsigned char data) //8位为待送数据 { unsigned char i=8,data1; while (i--) { data1=data &amp; 0x80; //看一下d7是高是低 if (data1) //是高的话送出1 { GPIOPinWrite(LCD_PORT , SDAT , 0xff); sdelay(400); } else //否则的话送出0 { GPIOPinWrite(LCD_PORT , SDAT , 0x00); sdelay(400); //不读忙线，但要等到不忙为止。 } GPIOPinWrite(LCD_PORT , SCLK , 0xff); //sclk=1 sdelay(400) ; GPIOPinWrite(LCD_PORT , SCLK , 0x00); //sclk=0 sdelay(400) ; data=data&lt;&lt;1; } } //============================================================================================ //*** 函 数:write_data() //*** 功 能: 写数据 //*** 参 数: data RW=1,RS=1时传送的数据指令 //============================================================================================ void write_data(unsigned char data) //八位是待写入数据 { unsigned char data1=0xfa,data2,data3; //data2中存放高4位数据，data3中存放低4位数据 GPIOPinWrite(LCD_PORT , CS , 0xff); //cs=1 sdelay(400); data2=data &amp; 0xf0; //取高4位数据 data3=(data &amp; 0x0f)&lt;&lt;4; //取低4位数据 send_data(data1); send_data(data2); send_data(data3); GPIOPinWrite(LCD_PORT , CS , 0x00); //cs=0 sdelay(400); } //============================================================================================ //*** 函 数:write_comm() //*** 功 能: 写命令 //*** 参 数: data RW=0,RS=0时传送的命今指令 //============================================================================================ void write_comm(unsigned char data) //低八位是待写入数据 { unsigned char data1=0xf8,data2,data3; //data2中存放高4位数据，data3中存放低4位数据 GPIOPinWrite(LCD_PORT , CS , 0xff); //cs=1 sdelay(400); data2=data &amp; 0xf0; //取高4位数据 data3=(data &amp; 0x0f)&lt;&lt;4; //取低4位数据 send_data(data1); send_data(data2); send_data(data3); GPIOPinWrite(LCD_PORT , CS , 0x00); //cs=0 sdelay(400); } //============================================================================================ //*** 函 数:init_lcd() //*** 功 能: LCM初始化 //*** 参 数: 无 //============================================================================================ void init_lcd(void) { SysCtlPeriEnable(SYSCTL_PERIPH_GPIOF); // 使能GPIOF端口 GPIOPinTypeOut(LCD_PORT , CS); // 设置PF1为输出类型 GPIOPinTypeOut(LCD_PORT , SDAT); // 设置PF2为输出类型 GPIOPinTypeOut(LCD_PORT , SCLK); // 设置PF3为输出类型 GPIOPinWrite(LCD_PORT , CS , 0x00); //CS=0 GPIOPinWrite(LCD_PORT , SDAT , 0x00); //SDAT=0 GPIOPinWrite(LCD_PORT , SCLK , 0x00); //SCLK=0 write_comm(0x30); //基本指令集 write_comm(0x01); //清除显示屏幕，把DDRAM位址计数器调整为\"00H\" sdelay(40000); write_comm(0x03); //把DDRAM位址计数器调整为\"00H\"，游标回原点，该功能不影响显示DDRAM sdelay(40000); write_comm(0x06); //光标右移 sdelay(4000); write_comm(0x0c); //显示屏打开 sdelay(4000); } //============================================================================================ //*** 函 数:screen() //*** 功 能: 全屏显示 //*** 参 数: data 基本指令集下要显示一维数组 //============================================================================================ void screen(unsigned char data[]) { unsigned char i,j; for (i=0;i&lt;4;i++) { switch(i) { case 0: write_comm(0x80); //设定DDRAM第一行，第一列 break; case 1: write_comm(0x90); //设定DDRAM第二行，第一列 break; case 2: write_comm(0x88); //设定DDRAM第三行，第一列 break; case 3: write_comm(0x98); //设定DDRAM第四行，第一列 break; } for(j=0;j&lt;16;j++) write_data(data[i*16+j]); } } //============================================================================================ //*** 函 数:disp_line(unsigned char line,unsigned char data[]) //*** 功 能: 写一行数据 //*** 参 数: line 行（0~3） //*** 参 数: data 基本指令集下要显示一维数组(16字节） //============================================================================================ void disp_line(unsigned char line,unsigned char data[]) { unsigned char j; switch(line) { case 0: write_comm(0x80); //设定DDRAM第一行，第一列 break; case 1: write_comm(0x90); //设定DDRAM第二行，第一列 break; case 2: write_comm(0x88); //设定DDRAM第三行，第一列 break; case 3: write_comm(0x98); //设定DDRAM第四行，第一列 break; } for(j=0;j&lt;16;j++) write_data(data[j]); write_comm(0xa0); } //============================================================================================ //*** 函 数:GUI_ClearSCR() //*** 功 能: 清屏（扩充指令集下） //*** 参 数: mode: 模式 // 0:基本指令集 // 1:扩充指令集 //============================================================================================ void GUI_ClearSCR(char mode) { unsigned char x,y; unsigned char i,j; switch(mode) { case 0: write_comm(0x32); //基本指令集 write_comm(0x01); //清除屏幕 break; case 1: write_comm(0x36); //扩充指令集 for(i=0;i&lt;8;i++) //填写缓冲区域 { for(j=0;j&lt;64;j++) gui_disp_buf[i][j]=0x00; } x=0x80;y=0x80; write_comm(y); //设置坐标 write_comm(x); for (j=0;j&lt;32;j++) { for (i=0;i&lt;16;i++) { write_data(0x00); } write_comm(++y); write_comm(x); } x=0x88;y=0x80; write_data(0x00); write_comm(y); write_comm(x); for (j=32;j&lt;64;j++) { for (i=0;i&lt;16;i++) { write_data(0x00); } write_comm(++y); write_comm(x); } write_comm(0x32);//基本指令集，绘图显示 break; } } void lcd_demo(void) { GUI_ClearSCR(0); paint(paint_buffer1); Delay(200000); GUI_ClearSCR(0); } //============================================================================================ //*** 函 数:location(int y,int x) //*** 功 能:汉字定位 //*** 参 数: //============================================================================================ void location(int y,int x) { switch(y) {case 0: write_comm(0x80 | x); break; case 1: write_comm(0x90 | x); break; case 2: write_comm(0x88 | x); break; case 3: write_comm(0x98 | x); break; } } void display( char *str) { while(*str != '\\0') { write_data(*str); str++; } } //========================================================== // 在LCD上显示 //========================================================== void zlg_disp() { // unsigned x; unsigned char lcd_buffer[16]; unsigned char hz_1[]={\"年月日星期日一二三四五六键盘温度\"}; //x=fTemp*10; lcd_buffer[0]=' '; lcd_buffer[1]=' '; lcd_buffer[2]=' '; lcd_buffer[3]=' '; lcd_buffer[4]=hz_1[24]; lcd_buffer[5]=hz_1[25]; lcd_buffer[6]=hz_1[26]; lcd_buffer[7]=hz_1[27]; lcd_buffer[8]=':'; lcd_buffer[9]=ascii_tab[1]; lcd_buffer[10]=ascii_tab[2]; lcd_buffer[11]='H'; lcd_buffer[12]=' '; lcd_buffer[13]=' '; lcd_buffer[14]=' '; lcd_buffer[15]=' '; disp_line(3,lcd_buffer); } 这个不用修改优先级，所以startup_ewarm.c不用动。 具体的就不讲解了，地址后期会附上","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}],"tags":[{"name":"1138","slug":"1138","permalink":"http://ailous.top/tags/1138/"},{"name":"12864","slug":"12864","permalink":"http://ailous.top/tags/12864/"},{"name":"lcd","slug":"lcd","permalink":"http://ailous.top/tags/lcd/"}],"keywords":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}]},{"title":"嵌入式——1138纯净版代码（自带按键）","slug":"嵌入式——1138纯净版代码（自带按键）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:39:36.342Z","comments":true,"path":"2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-zi-dai-an-jian/","link":"","permalink":"http://ailous.top/2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-zi-dai-an-jian/","excerpt":"","text":"这些是我自己常用的代码，比官方的要简洁些，主要是自用，还有相关模块，使用代码。 具体文件可以去我github上下载。 这个没什么好讲的，就是自带的几个按键使用罢了。不想外接用这个。 源码main函数// 基于《Stellaris外设驱动库》的例程：交流蜂鸣器发出、按键控制LED // 包含必要的头文件 #include \"KEY.H\" #include &lt;hw_types.h> #include &lt;hw_memmap.h> #include &lt;hw_sysctl.h> #include &lt;hw_gpio.h> #include &lt;sysctl.h> #include &lt;gpio.h> // 将较长的标识符定义成较短的形式 #define SysCtlPeriEnable SysCtlPeripheralEnable #define SysCtlPeriDisable SysCtlPeripheralDisable #define GPIOPinTypeIn GPIOPinTypeGPIOInput #define GPIOPinTypeOut GPIOPinTypeGPIOOutput // 定义KEY #define KEY_PERIPH SYSCTL_PERIPH_GPIOG #define KEY_PORT GPIO_PORTG_BASE #define KEY_PIN GPIO_PIN_5 // 防止JTAG失效 void JTAG_Wait(void) { SysCtlPeriEnable(KEY_PERIPH); // 使能KEY所在的GPIO端口 GPIOPinTypeIn(KEY_PORT , KEY_PIN); // 设置KEY所在管脚为输入 if ( GPIOPinRead(KEY_PORT , KEY_PIN) == 0x00 ) // 如果复位时按下KEY，则进入 { while(1); // 死循环，以等待JTAG连接 } SysCtlPeriDisable(KEY_PERIPH); // 禁止KEY所在的GPIO端口 } // 定义全局的系统时钟变量 unsigned long TheSysClock = 12000000UL; // 延时 void Delay(unsigned long ulVal){ while ( --ulVal != 0 ); } // 系统初始化 void SystemInit(void) { /* SysCtlLDOSet(SYSCTL_LDO_2_50V); // 设置LDO输出电压 SysCtlClockSet(SYSCTL_USE_OSC | // 系统时钟设置，采用主振荡器 SYSCTL_OSC_MAIN | SYSCTL_XTAL_6MHZ | SYSCTL_SYSDIV_1); */ SysCtlLDOSet(SYSCTL_LDO_2_75V); // 配置PLL前将LDO电压设置为2.75V SysCtlClockSet(SYSCTL_USE_PLL | // 系统时钟设置，采用PLL SYSCTL_OSC_MAIN | // 主振荡器 SYSCTL_XTAL_6MHZ | // 外接6MHz晶振 SYSCTL_SYSDIV_4); // 分频结果为50MHz TheSysClock = SysCtlClockGet(); // 获取系统时钟，单位：Hz KEY_Init(KEY1 | KEY2); // KEY初始化 } // 主函数（程序入口） int main(void){ JTAG_Wait(); // 防止JTAG失效，重要！ SystemInit(); // 系统初始化 while(1){ if ( KEY_Get(KEY1 | KEY2) ){ } else{ } } } 其他的不用改。","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}],"tags":[{"name":"1138","slug":"1138","permalink":"http://ailous.top/tags/1138/"},{"name":"自带按键","slug":"自带按键","permalink":"http://ailous.top/tags/自带按键/"}],"keywords":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}]},{"title":"嵌入式——1138纯净版代码（UART2A）","slug":"嵌入式——1138纯净版代码（UART2A）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:38:35.253Z","comments":true,"path":"2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-uart2a/","link":"","permalink":"http://ailous.top/2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-uart2a/","excerpt":"","text":"这些是我自己常用的代码，比官方的要简洁些，主要是自用，还有相关模块，使用代码。 具体文件可以去我github上下载。 UART 简介计算机与外部设备的连接，基本上使用了两类接口：串行接口与并行接口。并行接口是指数据的各个位同时进行传送，其特点是传输速度块，但当传输距离远、位数又多时，通信线路变复杂且成本提高。串行通信是指数据一位位地顺序传送，其特点是适合于远距离通信，通信线路简单，只要一对传输线就可以实现双向通信，从而大大降低了成本。串行通信又分为异步与同步两类。UART（Universal Asynchronous Receiver/Transmitter，通用异步收发器）正是设备间进行异步通信的关键模块。它的重要作用如下所示： 处理数据总路线和串行口之间的串/并、并/串转换； 通信双方只要采用相同的帧格式和波特率，就能在未共享时钟信号的情况下，仅用两根信号线（Rx 和 Tx）就可以完成通信过程； 采用异步方式，数据收发完毕后，可通过中断或置位标志位的方式通知微控制器进行处理，大大提高微控制器的工作效率。 若加入一个合适的电平转换器，如 SP3232E、SP3485，UART 还能用于 RS-232、RS-485通信，或与计算机的端口连接。UART 应用非常广泛，手机、工业控制、PC 等应用中都要用到 UART。 源码main函数#include \"systemInit.h\" #include &lt;uart.h> #include &lt;ctype.h> #include &lt;string.h> #include \"uartGetPut.h\" #include &lt;stdio.h> #include &lt;timer.h> #define PART_LM3S1138 #include &lt;pin_map.h> #include &lt;math.h> char Num ; // 主函数（程序入口） int main(void) { jtagWait(); // 防止JTAG失效，重要！ clockInit(); // 时钟初始化：晶振，6MHz uartInit(); // UART初始化 while(1) { } } void UART2_ISR(void) { unsigned long ulStatus; ulStatus = UARTIntStatus(UART2_BASE, true); // 读取当前中断状态 UARTIntClear(UART2_BASE, ulStatus); // 清除中断状态 if ((ulStatus &amp; UART_INT_RX) || (ulStatus &amp; UART_INT_RT)) // 若是接收中断或者 { Num = UARTCharGet(UART2_BASE); // 等待接收字符 if (Num == 'a'){ // 如果遇到a&lt;CR> } } } startup_ewarm.c（修改优先级） 这些是要改的部分。 具体的就不讲解了，地址后期会附上","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}],"tags":[{"name":"1138","slug":"1138","permalink":"http://ailous.top/tags/1138/"},{"name":"UART2A","slug":"UART2A","permalink":"http://ailous.top/tags/UART2A/"}],"keywords":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}]},{"title":"嵌入式——1138纯净版代码（定时器）","slug":"嵌入式——1138纯净版代码（定时器）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:39:02.191Z","comments":true,"path":"2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-ding-shi-qi/","link":"","permalink":"http://ailous.top/2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-ding-shi-qi/","excerpt":"","text":"这些是我自己常用的代码，比官方的要简洁些，主要是自用，还有相关模块，使用代码。 具体文件可以去我github上下载。 Timer 总体特性在 Stellaris 系列 ARM 内部通常集成有 2～4 个通用定时器模块（General-Purpose TimerModule，GPTM），分别称为 Timer0、Timer1、Timer2 和 Timer3。它们的用法是相同的：每 个 Timer 模块都可以配置为一个 32 位定时器或一个 32 位 RTC 定时器；也可以拆分为两个16 位的定时/计数器 TimerA 和 TimerB，它们可以被配置为独立运行的定时器、事件计数器或 PWM。 Timer 模块具有非常丰富的功能：32 位定时器模式：可编程单次触发（one-shot）定时器 可编程周期（periodic）定时器 实时时钟 RTC（Real Time Clock）  软件可控的事件暂停（用于单步调试时暂停计数，RTC 模式除外） 16 位定时器模式： 带 8 位预分频器的通用定时器功能 可编程单次触发（one-shot）定时器 可编程周期（periodic）定时器 软件可控的事件暂停 16 位输入捕获模式： 输入边沿计数捕获 输入边沿定时捕获 16 位 PWM 模式： 用法简单的 PWM（Pulse-Width Modulation，脉宽调制）模式 可通过软件实现 PWM 信号周期、占空比、输出反相等的控制 Timer 功能概述Timer 模块的功能在总体上可以分成 32 位模式和 16 位模式两大类。在 32 位模式下，TimerA 和 TimerB 被连在一起形成一个完整的 32 位计数器，对 Timer 的各项操作，如装载初值、运行控制、中断控制等，都用对 TimerA 的操作作为总体上的 32 位控制，而对 TimerB的操作无任何效果。在 16 位模式下，对 TimerA 的操作仅对 TimerA 有效，对 TimerB 的操作仅对 TimerB 有效，即对两者的操控是完全独立进行的。 每一个 Timer 模块对应两个 CCP 管脚。CCP 是“Capture Compare PWM”的缩写，意为“捕获/比较/脉宽调制”。在 32 位单次触发和周期定时模式下，CCP 功能无效（与之复用的 GPIO 管脚功能仍然正常）。在 32 位 RTC 模式下，偶数 CCP 管脚（CCP0、CCP2、CCP4等）作为 RTC 时钟源的输入，而奇数 CCP 管脚（CCP1、CCP3、CCP5 等）无效。在 16 位模式下，计数捕获、定时捕获、PWM 功能都会用到 CCP 管脚，对应关系是：Timer0A 对应CCP0、Timer0B 对应 CCP1，Timer1A 对应 CCP2、Timer1B 对应 CCP3，依此类推。 32 位单次触发/周期定时器在这两种模式中，Timer 都被配置成一个 32 位的递减计数器，用法类似，只是单次触发模式只能定时一次，如果需要再次定时则必须重新配置，而周期模式则可以周而复始地定时，除非被关闭。在计数到 0x00000000 时，可以在软件的控制下触发中断或输出一个内部的单时钟周期脉冲信号，该信号可以用来触发 ADC 采样。 32 位 RTC 定时器在该模式中，Timer 被配置成一个 32 位的递增计数器。RTC 功能的时钟源来自偶数 CCP 管脚的输入。在 LM3S101/102 里，RTC 时钟信号从专门的“32KHz”管脚输入。输入的时钟频率应当为精准的 32.768KHz，在芯片内部有一个RTC 专用的预分频器，固定为 32768 分频。因此最终输入到 RTC 计数器的时钟频率正好是1Hz，即每过 1 秒钟 RTC 计数器增 1。RTC计数器从 0x00000000 开始计满需要 232秒，这是个极长的时间，有 136 年！因此RTC真正的用法是：初始化后不需要更改配置（调整时间或日期时例外），只需要修改匹配寄存器的值，而且要保证匹配值总是超前于当前计数值。每次匹配时可产生中断（如果中断已被使能），据此可以计算出当前的年月日、时分秒以及星期。在中断服务函数里应当重新设置匹配值，并且匹配值仍要超前于当前的计数值。注意：在实际应用当中一般不会真正采用 Timer 模块的 RTC 功能来实现一个低功耗万年历系统，因为芯片一旦出现复位或断电的情况就会清除 RTC 计数值。取而代之的是冬眠模块（Hibernation Module）的 RTC 功能，由于采用了后备电池，因此不怕复位和 VDD 断电，并且功耗很低。 16 位单次触发/周期定时器一个 32 位的 Timer 可以被拆分为两个单独运行的 16 位定时/计数器，每一个都可以被配置成带 8 位预分频（可选功能）的 16 位递减计数器。如果使用 8 位预分频功能，则相当于24位定时器。具体用法跟32位单次触发/周期定时模式类似，不同的是对TimerA和TimerB的操作是分别独立进行的。 16 位输入边沿计数捕获在该模式中，TimerA 或 TimerB 被配置为能够捕获外部输入脉冲边沿事件的递减计数器。共有 3 种边沿事件类型：正边沿、负边沿、双边沿。该模式的工作过程是：设置装载值，并预设一个匹配值（应当小于装载值）；计数使能后，在特定的 CCP 管脚每输入 1 个脉冲（正边沿、负边沿或双边沿有效），计数值就减 1；当计数值与匹配值相等时停止运行并触发中断（如果中断已被使能）。如果需要再次捕获外部脉冲，则要重新进行配置。 16 位输入边沿定时捕获在该模式中，TimerA 或 TimerB 被配置为自由运行的 16 位递减计数器，允许在输入信号的上升沿或下降沿捕获事件。该模式的工作过程是：设置装载值（默认为 0xFFFF）、捕获边沿类型；计数器被使能后开始自由运行，从装载值开始递减计数，计数到 0 时重装初值，继续计数；如果从 CCP 管脚上出现有效的输入脉冲边沿事件，则当前计数值被自动复制到一个特定的寄存器里，该值会一直保存不变，直至遇到下一个有效输入边沿时被刷新。为了能够及时读取捕获到的计数值，应当使能边沿事件捕获中断，并在中断服务函数里读取。 16 位 PWMTimer 模块还可以用来产生简单的 PWM 信号。在 Stellaris 系列 ARM 众多型号当中，对于片内未集成专用 PWM 模块的，可以利用 Timer 模块的 16 位 PWM 功能来产生 PWM 信号，只不过功能较为简单。对于片内已集成专用 PWM 模块的，但仍然不够用时，则可以从Timer 模块借用。在 PWM 模式中，TimerA 或 TimerB 被配置为 16 位的递减计数器，通过设置适当的装载值（决定 PWM 周期）和匹配值（决定 PWM 占空比）来自动地产生 PWM 方波信号从相应的 CCP 管脚输出。在软件上，还可以控制输出反相，参见函数 TimerControlLevel( )。 源码main函数#include \"systemInit.h\" #include &lt;timer.h> void Time_Init( void ) { SysCtlPeriEnable(SYSCTL_PERIPH_TIMER0); // 使能Timer模块 TimerConfigure(TIMER0_BASE, TIMER_CFG_16_BIT_PAIR | // 配置Timer为16位周期定时器 TIMER_CFG_A_PERIODIC); TimerPrescaleSet(TIMER0_BASE, TIMER_A, 99); // 预先进行100分频 TimerLoadSet(TIMER0_BASE, TIMER_A, 30000); // 设置Timer初值，定时500ms TimerIntEnable(TIMER0_BASE, TIMER_TIMA_TIMEOUT); // 使能Timer超时中断 IntEnable(INT_TIMER0A); // 使能Timer中断 IntMasterEnable(); // 使能处理器中断 TimerEnable(TIMER0_BASE, TIMER_A); // 使能Timer计数 } // 主函数（程序入口） int main(void){ jtagWait(); // 防止JTAG失效，重要！ clockInit(); // 时钟初始化：晶振，6MHz Time_Init(); while(1){ } } // 定时器的中断服务函数 void Timer0A_ISR(void) { unsigned long ulStatus; ulStatus = TimerIntStatus(TIMER0_BASE, true); // 读取中断状态 TimerIntClear(TIMER0_BASE, ulStatus); // 清除中断状态，重要！ if (ulStatus &amp; TIMER_TIMA_TIMEOUT) // 如果是Timer超时中断 { } } startup_ewarm.c 这些是要改的部分。 具体的就不讲解了，地址后期会附上。","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}],"tags":[{"name":"1138","slug":"1138","permalink":"http://ailous.top/tags/1138/"},{"name":"定时器","slug":"定时器","permalink":"http://ailous.top/tags/定时器/"}],"keywords":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}]},{"title":"嵌入式——1138纯净版代码（陀螺仪6050+UART）","slug":"嵌入式——1138纯净版代码（陀螺仪6050+UART）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:40:13.675Z","comments":true,"path":"2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-tuo-luo-yi-6050-uart/","link":"","permalink":"http://ailous.top/2018/12/09/qian-ru-shi-1138-chun-jing-ban-dai-ma-tuo-luo-yi-6050-uart/","excerpt":"","text":"这些是我自己常用的代码，比官方的要简洁些，主要是自用，还有相关模块，使用代码。 具体文件可以去我github上下载。 MPU-6050 简介供电电源：3-5v（内部低压差稳压） 通信方式：标准IIC通信协议 芯片内置16bit AD转换器,16位数据输出 陀螺仪范围：±250 500 1000 2000 °/s 加速度范围：±2±4±8±16g MPU-6000（MPU-6000数据手册）为全球首例整合性6轴运动处理组件，相较于多组件方案，免除了组合陀螺仪与加速器时之轴间差的问题，减少了大量的包装空间。MPU-6000整合了3轴陀螺仪、3轴加速器，并含可藉由第二个I2C端口连接其他厂牌之加速器、磁力传感器、或其他传感器的数位运动处理(DMP: Digital Motion Processor)硬件加速引擎，由主要I2C端口以单一数据流的形式，向应用端输出完整的9轴融合演算技术 InvenSense的运动处理资料库，可处理运动感测的复杂数据，降低了运动处理运算对操作系统的负荷，并为应用开发提供架构化的API。 MPU-6000的角速度全格感测范围为±250、±500、±1000与±2000°/sec (dps)，可准确追緃快速与慢速动作，并且，用户可程式控制的加速器全格感测范围为±2g、±4g±8g与±16g。产品传输可透过最高至400kHz的I2C或最高达20MHz的SPI。 MPU-6000可在不同电压下工作，VDD供电电压介为2.5V±5%、3.0V±5%或3.3V±5%，逻辑接口VVDIO供电为1.8V± 5%。MPU-6000的包装尺寸4x4x0.9mm(QFN)，在业界是革命性的尺寸。其他的特征包含内建的温度感测器、包含在运作环境中仅有±1%变动的振荡器。 特征 以数字输出6轴或9轴的旋转矩阵、四元数(quaternion)、欧拉角格式(Euler Angle forma)的融合演算数据。 具有131 LSBs/°/sec 敏感度与全格感测范围为±250、±500、±1000与±2000°/sec 的3轴角速度感测器(陀螺仪)。 可程式控制，且程式控制范围为±2g、±4g、±8g和±16g的3轴加速器。 移除加速器与陀螺仪轴间敏感度，降低设定给予的影响与感测器的飘移。 数字运动处理(DMP: Digital Motion Processing)引擎可减少复杂的融合演算数据、感测器同步化、姿势感应等的负荷。 运动处理数据库支持Android、Linux与Windows 内建之运作时间偏差与磁力感测器校正演算技术，免除了客户须另外进行校正的需求。 以数位输出的温度传感器 以数位输入的同步引脚(Sync pin)支援视频电子影相稳定技术与GPS 可程式控制的中断(interrupt)支援姿势识别、摇摄、画面放大缩小、滚动、快速下降中断、high-G中断、零动作感应、触击感应、摇动感应功能。 VDD供电电压为2.5V±5%、3.0V±5%、3.3V±5%；VDDIO为1.8V± 5% 陀螺仪运作电流：5mA，陀螺仪待命电流：5A；加速器运作电流：350A，加速器省电模式电流： 20A@10Hz 高达400kHz快速模式的I2C，或最高至20MHz的SPI串行主机接口(serial host interface) 内建频率产生器在所有温度范围(full temperature range)仅有±1%频率变化。 使用者亲自测试 10,000 g 碰撞容忍度 为可携式产品量身订作的最小最薄包装 (4x4x0.9mm QFN) 源码main函数#include \"systemInit.h\" #include &lt;uart.h> #include &lt;ctype.h> #include &lt;string.h> #include \"uartGetPut.h\" #include &lt;stdio.h> #include &lt;timer.h> #include &lt;adc.h> #include &lt;stdio.h> #define PART_LM3S1138 #include &lt;pin_map.h> unsigned char Re_buf[11],counter=0; unsigned char ucStra[6],ucStrw[6],ucStrAngle[6]; float Value[9]; void delay(int a){ int x,y; for(x=0;x&lt;a;x++) for(y=0;y&lt;255;y++); } void DataTreating() { //float Value[8]; // char a[40]; // char b[40]; char c[40]; Value[0] = ((short)(ucStra[1]&lt;&lt;8| ucStra[0]))/32768.0*16; Value[1] = ((short)(ucStra[3]&lt;&lt;8| ucStra[2]))/32768.0*16; Value[2] = ((short)(ucStra[5]&lt;&lt;8| ucStra[4]))/32768.0*16; Value[3] = ((short)(ucStrw[1]&lt;&lt;8| ucStrw[0]))/32768.0*2000; Value[4] = ((short)(ucStrw[3]&lt;&lt;8| ucStrw[2]))/32768.0*2000; Value[5] = ((short)(ucStrw[5]&lt;&lt;8| ucStrw[4]))/32768.0*2000; Value[6] = ((short)(ucStrAngle[1]&lt;&lt;8| ucStrAngle[0]))/32768.0*180; Value[7] = ((short)(ucStrAngle[3]&lt;&lt;8| ucStrAngle[2]))/32768.0*180; Value[8] = ((short)(ucStrAngle[5]&lt;&lt;8| ucStrAngle[4]))/32768.0*180; //sprintf(a,\"加速度是x:%.2f y:%.2f z:%.2f \\r\\n\",Value[0],Value[1],Value[2]); // sprintf(b,\"角速度是x:%.2f y:%.2f z:%.2f \\r\\n\",Value[3],Value[4],Value[5]); sprintf(c,\"角度是x:%.2f y:%.2f z:%.2f \\r\\n\",Value[6],Value[7],Value[8]); //uartPuts(a); // uartPuts(b); uartPuts(c); delay(10); } // 主函数（程序入口） int main(void) { //float Value[8]; jtagWait(); // 防止JTAG失效，重要！ clockInit(); // 时钟初始化：晶振，6MHz uartInit(); // UART初始化 for (;;) { DataTreating(); } } // UART2中断服务函数 void UART0_ISR(void) { unsigned long ulStatus; ulStatus = UARTIntStatus(UART0_BASE, true); // 读取当前中断状态 UARTIntClear(UART0_BASE, ulStatus); // 清除中断状态 if ((ulStatus &amp; UART_INT_RX) || (ulStatus &amp; UART_INT_RT)) // 若是接收中断或者 { for(;;) {Re_buf[counter]= uartGetc0(); if(counter==0&amp;&amp;Re_buf[0]!=0x55) return; //第0号数据不是帧头 counter++; if(counter==11) //接收到11个数据 { counter=0; //重新赋值，准备下一帧数据的接收 switch(Re_buf [1]) { case 0x51: ucStra[0]=Re_buf[2]; ucStra[1]=Re_buf[3]; ucStra[2]=Re_buf[4]; ucStra[3]=Re_buf[5]; ucStra[4]=Re_buf[6]; ucStra[5]=Re_buf[7]; break; case 0x52: ucStrw[0]=Re_buf[2]; ucStrw[1]=Re_buf[3]; ucStrw[2]=Re_buf[4]; ucStrw[3]=Re_buf[5]; ucStrw[4]=Re_buf[6]; ucStrw[5]=Re_buf[7]; break; case 0x53: ucStrAngle[0]=Re_buf[2]; ucStrAngle[1]=Re_buf[3]; ucStrAngle[2]=Re_buf[4]; ucStrAngle[3]=Re_buf[5]; ucStrAngle[4]=Re_buf[6]; ucStrAngle[5]=Re_buf[7]; break; } //DataTreating(); } break; } } } startup_ewarm.c（修改优先级）这里用的是UART0 传输文件。 具体的就不讲解了，地址后期会附上","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}],"tags":[{"name":"1138","slug":"1138","permalink":"http://ailous.top/tags/1138/"},{"name":"陀螺仪6050","slug":"陀螺仪6050","permalink":"http://ailous.top/tags/陀螺仪6050/"},{"name":"UART","slug":"UART","permalink":"http://ailous.top/tags/UART/"}],"keywords":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://ailous.top/categories/嵌入式/"}]},{"title":"算法——最长公共子序列","slug":"算法——最长公共子序列","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:44:15.645Z","comments":true,"path":"2018/12/09/suan-fa-zui-chang-gong-gong-zi-xu-lie/","link":"","permalink":"http://ailous.top/2018/12/09/suan-fa-zui-chang-gong-gong-zi-xu-lie/","excerpt":"","text":"算法——最长公共子序列链接：https://ac.nowcoder.com/acm/problem/19978 来源：牛客网 题目描述字符序列的子序列是指从给定字符序列中随意地（不一定连续）去掉若干个字符（可能一个也不去掉）后所形成的字符序列。令给定的字符序列X=“x0，x1，…，xm-1”，序列Y=“y0，y1，…，yk-1”是X的子序列，存在X的一个严格递增下标序列 &lt; i0，i1，…，ik-1 &gt; ，使得对所有的j=0，1，…，k-1，有xij = yj。例如，X=“ABCBDAB”，Y=“BCDB”是X的一个子序列。对给定的两个字符序列，求出他们最长的公共子序列长度，以及最长公共子序列个数。 输入描述:第1行为第1个字符序列，都是大写字母组成，以”.”结束。长度小于5000。第2行为第2个字符序列，都是大写字母组成，以”.”结束，长度小于5000。 输出描述:第1行输出上述两个最长公共子序列的长度。第2行输出所有可能出现的最长公共子序列个数，答案可能很大，只要将答案对100,000,000求余即可。 源码：#include&lt;cstdio> #include&lt;cstring> #define N 5010 #define mod 100000000 int f[2][N],g[2][N]; char A[N],B[N]; int main() { int n,m,i,j,d; scanf(\"%s%s\",A+1,B+1); n=strlen(A+1)-1,m=strlen(B+1)-1; for(i=0;i&lt;=m;i++) f[0][i]=0,g[0][i]=1; for(d=i=1;i&lt;=n;i++,d^=1) { f[d][0]=0,g[d][0]=1; for(j=1;j&lt;=m;j++) { if(f[d^1][j]>f[d][j-1]) f[d][j]=f[d^1][j], g[d][j]=g[d^1][j]; else if(f[d^1][j]&lt;f[d][j-1]) f[d][j]=f[d][j-1],g[d][j]=g[d][j-1]; else { f[d][j]=f[d^1][j],g[d][j]=(g[d^1][j]+g[d][j-1])%mod; if(f[d][j]==f[d^1][j-1]) g[d][j]=(g[d][j]-g[d^1][j-1]+mod)%mod; } if(A[i]==B[j]) { if(f[d^1][j-1]+1>f[d][j]) f[d][j]=f[d^1][j-1]+1,g[d][j]=g[d^1][j-1]; else if(f[d^1][j-1]+1==f[d][j]) g[d][j]=(g[d][j]+g[d^1][j-1])%mod; } } } printf(\"%d\\n%d\\n\",f[n&amp;1][m],g[n&amp;1][m]); return 0; }","categories":[{"name":"算法","slug":"算法","permalink":"http://ailous.top/categories/算法/"}],"tags":[{"name":"公共子序列","slug":"公共子序列","permalink":"http://ailous.top/tags/公共子序列/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://ailous.top/categories/算法/"}]},{"title":"网络爬虫——古诗文网中验证码（超级鹰）","slug":"网络爬虫——古诗文网中验证码（超级鹰）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-27T07:44:35.151Z","comments":true,"path":"2018/12/09/wang-luo-pa-chong-gu-shi-wen-wang-zhong-yan-zheng-ma-chao-ji-ying/","link":"","permalink":"http://ailous.top/2018/12/09/wang-luo-pa-chong-gu-shi-wen-wang-zhong-yan-zheng-ma-chao-ji-ying/","excerpt":"","text":"网络爬虫——古诗文网中验证码（超级鹰）目标网址: 古诗文网目标网址：https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx 任务要求：（1）通过selenium的方式模拟该网站的登录，并成功输入用户名和密码； （2）保存验证码图片，并使用输入式验证码识别的方式识别验证码的文字，获取后输入到输入框中， （3）验证登录是否成功。 源码：超级鹰源码：import requests from hashlib import md5 class Chaojiying_Client(object): def __init__(self, username, password, soft_id): self.username = username # todo:更改点一 self.password = md5(password.encode(\"utf-8\")).hexdigest() self.soft_id = soft_id self.base_params = { 'user': self.username, 'pass2': self.password, 'softid': self.soft_id, } self.headers = { 'Connection': 'Keep-Alive', 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)', } def PostPic(self, im, codetype): params = { 'codetype': codetype, } params.update(self.base_params) files = {'userfile': ('ccc.jpg', im)} r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, files=files, headers=self.headers) return r.json() def ReportError(self, im_id): \"\"\" im_id:报错题目的图片ID \"\"\" params = { 'id': im_id, } params.update(self.base_params) r = requests.post('http://upload.chaojiying.net/Upload/ReportError.php', data=params, headers=self.headers) return r.json() 识别源码：from selenium import webdriver from selenium.common.exceptions import TimeoutException , NoSuchElementException import time from PIL import Image import pytesseract import chaojiying browser = webdriver.Edge(&#39;E:\\\\anaconda\\\\Scripts\\\\msedgedriver.exe&#39;) # browser = webdriver.Chrome() try: browser.get(&#39;https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx&#39;) except TimeoutException: print(&#39;Time Out&#39;) try: username = browser.find_element_by_xpath(&#39;//*[@id=&quot;email&quot;]&#39;) username.send_keys(&#39;自己账号&#39;) time.sleep(1) password = browser.find_element_by_xpath(&#39;//*[@id=&quot;pwd&quot;]&#39;) password.send_keys(&#39;自己密码&#39;) time.sleep(1) pictureN = browser.find_element_by_xpath(&#39;//*[@id=&quot;imgCode&quot;]&#39;) browser.save_screenshot(&#39;login.png&#39;) loc = pictureN.location size = pictureN.size left = loc[&#39;x&#39;] top = loc[&#39;y&#39;] bottom = top+size[&#39;height&#39;] right = left+size[&#39;width&#39;] page = Image.open(&#39;login.png&#39;) Code = page.crop((left,top,right,bottom)) Code.save(&#39;code.png&#39;) chaojiying = Chaojiying_Client(&#39;超级鹰账号&#39;, &#39;密码&#39;, &#39;ID&#39;)#ID 具体看软件ID。 im = open(&#39;code.png&#39;, &#39;rb&#39;).read() text = chaojiying.PostPic(im,2004)[&#39;pic_str&#39;] print(text) # text = pytesseract.image_to_string(Image.open(&#39;code.png&#39;)) # print(text) CodeWhere = browser.find_element_by_xpath(&#39;//*[@id=&quot;code&quot;]&#39;) CodeWhere.send_keys(text) time.sleep(5) Submit = browser.find_element_by_xpath(&#39;//*[@id=&quot;denglu&quot;]&#39;) Submit.click() time.sleep(5) except NoSuchElementException: print(&#39;No Element&#39;) finally: browser.close()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"古诗文网","slug":"古诗文网","permalink":"http://ailous.top/tags/古诗文网/"},{"name":"超级鹰","slug":"超级鹰","permalink":"http://ailous.top/tags/超级鹰/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"风格迁移——代码部分","slug":"卷积神经网络：（三）风格迁移——代码部分","date":"2018-10-29T05:47:40.000Z","updated":"2020-05-27T07:43:33.638Z","comments":true,"path":"2018/10/29/juan-ji-shen-jing-wang-luo-san-feng-ge-qian-yi-dai-ma-bu-fen/","link":"","permalink":"http://ailous.top/2018/10/29/juan-ji-shen-jing-wang-luo-san-feng-ge-qian-yi-dai-ma-bu-fen/","excerpt":"","text":"卷积神经网络：（三）风格迁移——代码部分引言本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。友情提示：风格迁移跑的时间会很长。有点耐心哦。 若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284， 想知道原理，请点击https://blog.csdn.net/weixin_41108515/article/details/103650964 转载请注明出处：https://blog.csdn.net/weixin_41108515/article/details/103651784 这里引用的是：https://blog.csdn.net/aaronjny/article/details/79681080http://zh.gluon.ai/chapter_computer-vision/neural-style.html这两篇都非常详细，并且经调试可以使用，但是第二个并未使用tensorflow。以及理论帮助的一篇https://juejin.im/post/5d29e818e51d454f73356de0 第一部分 ：简介主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。 所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting.jpg），数据集将存放在百度网盘。 content文件选取的是qd.jpg，在content文件夹下。最终实现效果如下。 第二部分 ：操作1.获取模型VGG是Visual Geometry Group 这个实验室发明的，VGG是在2014年的 ILSVRC localization and classification 两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。 选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。 CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。注： 预训练好的VGG19模型可以从http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat下载，下载较慢的话，网盘https://pan.baidu.com/s/1uFinsEArbrgYRc2FWY9zVw：。 2.模型修改这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。 现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。 风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。理论上简单理解了，开始操作。 这里建立py文件 models.py，下面内容我会写在注释里。 # 导入必须的包 import tensorflow as tf import numpy as np import settings import scipy.io import scipy.misc class Model(object): def __init__(self, content_path, style_path): self.content = self.loadimg(content_path) # 加载内容图片 self.style = self.loadimg(style_path) # 加载风格图片 self.random_img = self.get_random_img() # 生成噪音内容图片 self.net = self.vggnet() # 建立vgg网络 def vggnet(self): # 读取预训练的vgg模型 # 这里装的是misc，安装opencv的也亦可以使用opencv等其他方法 vgg = scipy.io.loadmat(settings.VGG_MODEL_PATH) vgg_layers = vgg['layers'][0] net = {} # 使用预训练的模型参数构建vgg网络的卷积层和池化层 # 全连接层不需要 # 注意，除了input之外，这里参数都为常量，不训练vgg的参数（权重比），这个以及训练完不需调整。 # 需要进行训练的是input，它即是我们最终生成的图像 net['input'] = tf.Variable(np.zeros([1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3]), dtype=tf.float32) # 参数对应的层数可以参考vgg模型图 net['conv1_1'] = self.conv_relu(net['input'], self.get_wb(vgg_layers, 0)) net['conv1_2'] = self.conv_relu(net['conv1_1'], self.get_wb(vgg_layers, 2)) net['pool1'] = self.pool(net['conv1_2']) net['conv2_1'] = self.conv_relu(net['pool1'], self.get_wb(vgg_layers, 5)) net['conv2_2'] = self.conv_relu(net['conv2_1'], self.get_wb(vgg_layers, 7)) net['pool2'] = self.pool(net['conv2_2']) net['conv3_1'] = self.conv_relu(net['pool2'], self.get_wb(vgg_layers, 10)) net['conv3_2'] = self.conv_relu(net['conv3_1'], self.get_wb(vgg_layers, 12)) net['conv3_3'] = self.conv_relu(net['conv3_2'], self.get_wb(vgg_layers, 14)) net['conv3_4'] = self.conv_relu(net['conv3_3'], self.get_wb(vgg_layers, 16)) net['pool3'] = self.pool(net['conv3_4']) net['conv4_1'] = self.conv_relu(net['pool3'], self.get_wb(vgg_layers, 19)) net['conv4_2'] = self.conv_relu(net['conv4_1'], self.get_wb(vgg_layers, 21)) net['conv4_3'] = self.conv_relu(net['conv4_2'], self.get_wb(vgg_layers, 23)) net['conv4_4'] = self.conv_relu(net['conv4_3'], self.get_wb(vgg_layers, 25)) net['pool4'] = self.pool(net['conv4_4']) net['conv5_1'] = self.conv_relu(net['pool4'], self.get_wb(vgg_layers, 28)) net['conv5_2'] = self.conv_relu(net['conv5_1'], self.get_wb(vgg_layers, 30)) net['conv5_3'] = self.conv_relu(net['conv5_2'], self.get_wb(vgg_layers, 32)) net['conv5_4'] = self.conv_relu(net['conv5_3'], self.get_wb(vgg_layers, 34)) net['pool5'] = self.pool(net['conv5_4']) return net def conv_relu(self, input, wb): \"\"\" 进行先卷积、后relu的运算 :param input: 输入层 :param wb: wb[0],wb[1] == w,b :return: relu后的结果 \"\"\" conv = tf.nn.conv2d(input, wb[0], strides=[1, 1, 1, 1], padding='SAME') relu = tf.nn.relu(conv + wb[1]) return relu def pool(self, input): \"\"\" 进行max_pool操作 :param input: 输入层 :return: 池化后的结果 \"\"\" return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') def get_wb(self, layers, i): \"\"\" 从预训练好的vgg模型中读取参数 :param layers: 训练好的vgg模型 :param i: vgg指定层数 :return: 该层的w,b \"\"\" w = tf.constant(layers[i][0][0][0][0][0]) bias = layers[i][0][0][0][0][1] b = tf.constant(np.reshape(bias, (bias.size))) return w, b def get_random_img(self): \"\"\" 根据噪音和内容图片，生成一张随机图片 :return: \"\"\" noise_image = np.random.uniform(-20, 20, [1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3]) random_img = noise_image * settings.NOISE + self.content * (1 - settings.NOISE) return random_img def loadimg(self, path): \"\"\" 加载一张图片，将其转化为符合要求的格式 :param path: :return: \"\"\" # 读取图片 image = scipy.misc.imread(path) # 重新设定图片大小 image = scipy.misc.imresize(image, [settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH]) # 改变数组形状，其实就是把它变成一个batch_size=1的batch image = np.reshape(image, (1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3)) # 减去均值，使其数据分布接近0 image = image - settings.IMAGE_MEAN_VALUE return image if __name__ == '__main__': Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE) 3.模型训练但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。 就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。 这里建立py文件 train.py，下面内容我会写在注释里。 # -*- coding: utf-8 -*- import tensorflow as tf import settings import models import numpy as np import scipy.misc def loss(sess, model): \"\"\" 定义模型的损失函数 :param sess: tf session :param model: 神经网络模型 :return: 内容损失和风格损失的加权和损失 \"\"\" # 先计算内容损失函数 # 获取定义内容损失的vgg层名称列表及权重 content_layers = settings.CONTENT_LOSS_LAYERS # 将内容图片作为输入，方便后面提取内容图片在各层中的特征矩阵 sess.run(tf.assign(model.net['input'], model.content)) # 内容损失累加量 content_loss = 0.0 # 逐个取出衡量内容损失的vgg层名称及对应权重 for layer_name, weight in content_layers: # 提取内容图片在layer_name层中的特征矩阵 p = sess.run(model.net[layer_name]) # 提取噪音图片在layer_name层中的特征矩阵 x = model.net[layer_name] # 长x宽 M = p.shape[1] * p.shape[2] # 信道数 N = p.shape[3] # 根据公式计算损失，并进行累加 content_loss += (1.0 / (2 * M * N)) * tf.reduce_sum(tf.pow(p - x, 2)) * weight # 将损失对层数取平均 content_loss /= len(content_layers) # 再计算风格损失函数 style_layers = settings.STYLE_LOSS_LAYERS # 将风格图片作为输入，方便后面提取风格图片在各层中的特征矩阵 sess.run(tf.assign(model.net['input'], model.style)) # 风格损失累加量 style_loss = 0.0 # 逐个取出衡量风格损失的vgg层名称及对应权重 for layer_name, weight in style_layers: # 提取风格图片在layer_name层中的特征矩阵 a = sess.run(model.net[layer_name]) # 提取噪音图片在layer_name层中的特征矩阵 x = model.net[layer_name] # 长x宽 M = a.shape[1] * a.shape[2] # 信道数 N = a.shape[3] # 求风格图片特征的gram矩阵 A = gram(a, M, N) # 求噪音图片特征的gram矩阵 G = gram(x, M, N) # 根据公式计算损失，并进行累加 style_loss += (1.0 / (4 * M * M * N * N)) * tf.reduce_sum(tf.pow(G - A, 2)) * weight # 将损失对层数取平均 style_loss /= len(style_layers) # 将内容损失和风格损失加权求和，构成总损失函数 loss = settings.ALPHA * content_loss + settings.BETA * style_loss return loss def gram(x, size, deep): \"\"\" 创建给定矩阵的格莱姆矩阵，用来衡量风格 :param x:给定矩阵 :param size:矩阵的行数与列数的乘积 :param deep:矩阵信道数 :return:格莱姆矩阵 \"\"\" # 改变shape为（size,deep） x = tf.reshape(x, (size, deep)) # 求xTx g = tf.matmul(tf.transpose(x), x) return g def train(): # 创建一个模型 model = models.Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE) # 创建session with tf.Session() as sess: # 全局初始化 sess.run(tf.global_variables_initializer()) # 定义损失函数 cost = loss(sess, model) # 创建优化器 optimizer = tf.train.AdamOptimizer(1.0).minimize(cost) # 再初始化一次（主要针对于第一次初始化后又定义的运算，不然可能会报错） sess.run(tf.global_variables_initializer()) # 使用噪声图片进行训练 sess.run(tf.assign(model.net['input'], model.random_img)) # 迭代指定次数 for step in range(settings.TRAIN_STEPS): # 进行一次反向传播 sess.run(optimizer) # 每隔一定次数，输出一下进度，并保存当前训练结果 if step % 50 == 0: print('step {} is down.'.format(step)) # 取出input的内容，这是生成的图片 img = sess.run(model.net['input']) # 训练过程是减去均值的，这里要加上 img += settings.IMAGE_MEAN_VALUE # 这里是一个batch_size=1的batch，所以img[0]才是图片内容 img = img[0] # 将像素值限定在0-255，并转为整型 img = np.clip(img, 0, 255).astype(np.uint8) # 保存图片 scipy.misc.imsave('{}-{}.jpg'.format(settings.OUTPUT_IMAGE,step), img) # 保存最终训练结果 img = sess.run(model.net['input']) img += settings.IMAGE_MEAN_VALUE img = img[0] img = np.clip(img, 0, 255).astype(np.uint8) scipy.misc.imsave('{}.jpg'.format(settings.OUTPUT_IMAGE), img) if __name__ == '__main__': train() 4.系统文件配置这里建立py文件 setting.py。 # -*- coding: utf-8 -*- # 内容图片路径 CONTENT_IMAGE = 'content/qd.jpg' # 路径/图片 自己在工程文件夹下建立。 # 风格图片路径 STYLE_IMAGE = 'style/painting.jpg' # 路径/图片 自己在工程文件夹下建立。 # 输出图片路径 OUTPUT_IMAGE = 'output/output' # 路径/图片开始名 自己在工程文件夹下建立。 # 预训练的vgg模型路径 VGG_MODEL_PATH = 'imagenet-vgg-verydeep-19.mat' # 直接置于工程文件夹即可。 # 图片宽度 IMAGE_WIDTH = 450 # 图片高度 IMAGE_HEIGHT = 300 # 定义计算内容损失的vgg层名称及对应权重的列表 CONTENT_LOSS_LAYERS = [('conv4_2', 0.5),('conv5_2',0.5)] # 定义计算风格损失的vgg层名称及对应权重的列表 STYLE_LOSS_LAYERS = [('conv1_1', 0.2), ('conv2_1', 0.2), ('conv3_1', 0.2), ('conv4_1', 0.2), ('conv5_1', 0.2)] # 噪音比率 NOISE = 0.5 # 图片RGB均值 IMAGE_MEAN_VALUE = [128.0, 128.0, 128.0] # 内容损失权重 ALPHA = 1 # 风格损失权重 BETA = 500 # 训练次数 TRAIN_STEPS = 3000 # 这里推荐几百次就行，确实时间太长。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://ailous.top/tags/卷积神经网络/"},{"name":"风格迁移","slug":"风格迁移","permalink":"http://ailous.top/tags/风格迁移/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"风格迁移——原理部分","slug":"卷积神经网络：（二）风格迁移——原理部分","date":"2018-10-28T05:47:40.000Z","updated":"2020-05-26T23:44:54.207Z","comments":true,"path":"2018/10/28/juan-ji-shen-jing-wang-luo-er-feng-ge-qian-yi-yuan-li-bu-fen/","link":"","permalink":"http://ailous.top/2018/10/28/juan-ji-shen-jing-wang-luo-er-feng-ge-qian-yi-yuan-li-bu-fen/","excerpt":"","text":"卷积神经网络：（二）风格迁移——原理部分引言本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。&nbsp;转载请注明出处：https://blog.csdn.net/weixin_41108515/article/details/103650964&nbsp;这里引用的是：http://zh.gluon.ai/chapter_computer-vision/neural-style.htmlhttps://blog.csdn.net/aaronjny/article/details/79681080这两篇都非常详细，并且经调试可以使用。 涉及到的相关原理：1、神经网络部分原理：1.1 神经网络基础介绍神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。 纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。 1.2 卷积神经网络基本结构卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。 1.2.1 输入层卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。 1.2.2 隐含层1.卷积层（1）卷积层 利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。 （2）卷积层参数卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。 （3）激励函数一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质： 1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。 2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。 3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。 2.池化层池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：（1）一般池化(General Pooling)1）mean-pooling，即只要求邻域中特征点的平均值； 2）max-pooling，即在邻域中提取最大特征点；3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。特征提取的误差主要来自两个方面： 1）邻域大小受限造成的估计值方差增大； 2）卷积层参数误差导致估计均值的偏移。 一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示， （2）空间金字塔池化(Spatial pyramid pooling)一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。 3.全连接层卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。 1.2.3 输出层卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。 1.3 卷积神经网络的卷积过程卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。端部是个全连接层，得到输出向量。卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。 2、迁移学习相关原理2.1 迁移学习在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。 2.2TensorFlowTensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。 （1）支持多种硬件的平台例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。 （2）支持多种开发环境 支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。 2.3VGG卷积神经网络模型VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图：在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。 3、通过VGG实现风格迁移3.1 图像风格迁移的原理VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示：具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。 3.2 代价函数要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。Jcontent(C,G) 第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。Jstyle(S,G)然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。J(G)=αJcontent(C,G)+βJstyle(S,G) 最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。 3.3 内容代价函数风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。 首先定义内容代价部分。用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。 之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。 为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。 3.4 风格代价函数图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。风格矩阵是一组向量的内积对称矩阵，比如向量组的Gram矩阵是取内积即欧几里得空间上的标准内积，即 假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足 l≤i≤N_l，l≤j≤M_lGram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。 3.5 模型训练过程首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。将内容图片输入网络，计算内容图片在网络指定层上的输出值。 计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。对应每一层的内容损失函数：其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。将风格图片输入网络，计算风格图片在网络指定层上的输出值。 计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。 对于每一层的风格损失函数：其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。最终的风格损失为，每一层的风格损失加权和，再对层数取平均。 函数为内容损失和风格损失的加权和：当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。 原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！卷积神经网络：（三）风格迁移——代码部分","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://ailous.top/tags/卷积神经网络/"},{"name":"风格迁移","slug":"风格迁移","permalink":"http://ailous.top/tags/风格迁移/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"风格迁移——环境配置","slug":"卷积神经网络：（一）风格迁移——环境配置","date":"2018-10-27T05:47:40.000Z","updated":"2020-05-26T23:35:38.374Z","comments":true,"path":"2018/10/27/juan-ji-shen-jing-wang-luo-yi-feng-ge-qian-yi-huan-jing-pei-zhi/","link":"","permalink":"http://ailous.top/2018/10/27/juan-ji-shen-jing-wang-luo-yi-feng-ge-qian-yi-huan-jing-pei-zhi/","excerpt":"","text":"卷积神经网络：（一）风格迁移——环境配置引言本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。&nbsp;转载请注明出处：https://blog.csdn.net/weixin_41108515/article/details/103636284&nbsp; 第一步：搭建python环境：方法一 : 直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3.6版本，3.7及以上版本目前不支持）下载地址：https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe如图示安装install Now即可,注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。注：默认路径C:\\users\\用户名\\Appdata\\local\\Programs\\Python\\Python36 也可以使用Customize installation来更换路径。安装完成后，进入cmd界面，输入python，如图出现此界面即可。 方法二 ：使用anaconda搭建环境:因为anaconda官方网站下下载较慢，这里推荐使用 清华镜像来下载。清华镜像下载地址https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe如图所示安装anaconda，这里先用的是官方3.7的包：注：anaconda官方当前最新版为基于python3.7version，这个并不意味着搭建的python环境版本就固定为3.7，但是这里还是不推荐使用下面会说明原因。注：存在问题：1、下载太慢。2、3.7—version需额外对环境更改，这里只对3.7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。 安装步骤： 目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。这里是指是否将anaconda里的python作为电脑默认python环境。&emsp;&emsp;安装完成后，选择Anaconda Navigator 打开，接下来选择Environments -&gt;create，:生成如下界面：这里是3.7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装：接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3.6。 第一步，总结anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。&nbsp; 第二步，安装pycharm这个在pycharm官网上下载即可,如图示安装即可。 安装完成。 第三步，安装必需包这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm 中的【ALT+SHIFT+ENTER】即可安装，如还有缺失请自行百度。 方法一：直装python环境默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python：若为下图：第二行中出现anaconda。则需修改默认python环境。若为下图，第二行中不是anaconda，则继续操作即可。 3.1.1 安装tensorflow包tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https://github.com/fo40225/t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。 输入 ： pip install .\\tensorflow-1.4.0-cp36-cp36m-win_amd64.whl 运行完即可，若是提示pip版本有更新，更不更新都可。 python -m pip install –upgrade pip 3.1.2 安装opencv包opencv在代码中使用时：import cv2opencv较tensorflow简单，只需输入代码即可运行： pip install opencv-python 3.1.2 安装PIL（Python Imaging Library）包PIL安装并不是通过直接键入pip install PIL 而是通过： pip install pillow 方法一总结：这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。&nbsp; 方法二：anaconda环境这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。点击上方的选择框设置为all，然后进行搜索tensorflw选择tensorflow，点击apply会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。 第四步，建立风格迁移工程在pycharm中选择：file-&gt;New project 这里要注意的是Base interpreter ，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。 方法一：在创建项目是直接选择anaconda下的python： 方法二：在项目创建完成后再选择路径为：File-&gt;Settings-&gt;project：项目名，如图点击齿轮——Add。在Base interpreter下选择anaconda中python路径即可。至此所有环境配置完毕。 第五步，写风格迁移代码这里主要参网址的为http://zh.gluon.ai/chapter_computer-vision/neural-style.html会在下一篇博客https://blog.csdn.net/weixin_41108515/article/details/103650964里详细解释过程。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://ailous.top/tags/卷积神经网络/"},{"name":"风格迁移","slug":"风格迁移","permalink":"http://ailous.top/tags/风格迁移/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"网络爬虫——豆瓣读书数据抓取——RE（正则表达式）","slug":"网络爬虫——豆瓣读书数据抓取——RE（正则表达式）","date":"2018-06-09T05:47:40.000Z","updated":"2020-05-27T07:45:07.822Z","comments":true,"path":"2018/06/09/wang-luo-pa-chong-dou-ban-du-shu-shu-ju-zhua-qu-re-zheng-ze-biao-da-shi/","link":"","permalink":"http://ailous.top/2018/06/09/wang-luo-pa-chong-dou-ban-du-shu-shu-ju-zhua-qu-re-zheng-ze-biao-da-shi/","excerpt":"","text":"网络爬虫——豆瓣读书数据抓取——RE（正则表达式）目标网址：https://book.douban.com/ 目标数据：（1）书名（2）书的链接地址（3）作者（4）发行时间（5）出版社分析网页结构，通过获取网页源代码，使用re库解析网页结构，完成豆瓣读书项目中目标数据的爬取。 源码import requests import re def getcode(url): url = url headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3756.400 QQBrowser/10.5.4039.400' } response = requests.get(url,headers=headers) response.encoding = 'utf_8' code = response.text return code def parse_page(code): ulist = [] pattern = re.compile('.*?.*?(.*?).*?(.*?).*?(.*?)',re.S) items = re.findall(pattern,code) # print(items) for item in items: ulist.append([item[1],item[0].strip(),item[2].strip(),item[3].strip(),item[4].strip()]) return (ulist) def main(): ulist = [] url = \"https://book.douban.com/\" code = getcode(url) ulist = parse_page(code) print(len(ulist)) print() for i in range(len(ulist)): print(ulist[i]) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"RE（正则表达式","slug":"RE（正则表达式","permalink":"http://ailous.top/tags/RE（正则表达式/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——猫眼电影数据抓取——RE（正则表达式）","slug":"网络爬虫——猫眼电影数据抓取——RE（正则表达式）","date":"2018-05-29T05:47:40.000Z","updated":"2020-05-27T07:44:42.861Z","comments":true,"path":"2018/05/29/wang-luo-pa-chong-mao-yan-dian-ying-shu-ju-zhua-qu-re-zheng-ze-biao-da-shi/","link":"","permalink":"http://ailous.top/2018/05/29/wang-luo-pa-chong-mao-yan-dian-ying-shu-ju-zhua-qu-re-zheng-ze-biao-da-shi/","excerpt":"","text":"网络爬虫——猫眼电影数据抓取——RE（正则表达式）猫眼电影榜单网址：https://maoyan.com/board/4 目标数据描述：（1）排名 （2）电影名称 （3）主演 （4）上映时间 （5）评分任务要求 （1）使用requests库实现该网站网页源代码的获取； （2）使用正则表达式对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 可以使用生成器，将目标数据获取后放到字典中返回，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。 源码import json import requests from requests.exceptions import RequestException import re import time def get_one_page(url): try: headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'} response=requests.get(\"https://maoyan.com/board/4\",headers = headers) if response.status_code == 200: return response.text return None except RequestException: return None def parse_one_page(html): pattern= re.compile('.*?board-index.*?>(\\d+).*?data-src=\"(.*?)\".*?name\">(.*?).*?star\">(.*?).*?releasetime\">(.*?)' + '.*?integer\">(.*?).*?fraction\">(.*?).*?',re.S) items = re.findall(pattern,html) for item in items: yield{ 'index': item[0], 'image': item[1], 'title': item[2], 'actor': item[3].strip()[3:], 'time': item[4].strip()[5:], 'score': item[5] + item[6] } def write_to_file(content): with open('result.txt', 'a' , encoding='utf-8') as f: f.write(json.dumps(content, ensure_ascii=False) + '\\n') def main(offset): url = 'https://maoyan.com/board/4' html=get_one_page(url) for item in parse_one_page(html): print(item) write_to_file(item) if __name__ == '__main__': for i in range(10): main(offset=i * 10) time.sleep(1) 输出如下","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"RE（正则表达式）","slug":"RE（正则表达式）","permalink":"http://ailous.top/tags/RE（正则表达式）/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]}]}