{"meta":{"title":"Michaelming's Blog","subtitle":"不见你如我一般","description":"大地茫茫一片真干净","author":"Michaelming","url":"http://ailous.top"},"pages":[{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-05-12T03:58:57.602Z","comments":false,"path":"bangumi/index.html","permalink":"http://ailous.top/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"about/index.html","permalink":"http://ailous.top/about/index.html","excerpt":"","text":"[さくら荘のhojun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"categories","date":"2020-05-12T01:56:21.000Z","updated":"2020-05-12T01:56:57.012Z","comments":false,"path":"categories/index.html","permalink":"http://ailous.top/categories/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"client/index.html","permalink":"http://ailous.top/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-03-16T10:41:30.000Z","comments":true,"path":"comment/index.html","permalink":"http://ailous.top/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"donate/index.html","permalink":"http://ailous.top/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"lab/index.html","permalink":"http://ailous.top/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-05-22T08:09:56.003Z","comments":false,"path":"music/index.html","permalink":"http://ailous.top/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-03-16T10:41:30.000Z","comments":true,"path":"rss/index.html","permalink":"http://ailous.top/rss/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"http://ailous.top/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-03-16T10:41:30.000Z","comments":true,"path":"links/index.html","permalink":"http://ailous.top/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-05-12T01:50:55.687Z","comments":true,"path":"tags/index.html","permalink":"http://ailous.top/tags/index.html","excerpt":"","text":""},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-03-16T10:41:30.000Z","comments":false,"path":"video/index.html","permalink":"http://ailous.top/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"网络爬虫——超级鹰源码下载","slug":"网络爬虫——超级鹰源码下载","date":"2020-04-27T05:47:40.000Z","updated":"2020-05-11T14:55:18.495Z","comments":true,"path":"2020/04/27/网络爬虫——超级鹰源码下载/","link":"","permalink":"http://ailous.top/2020/04/27/网络爬虫——超级鹰源码下载/","excerpt":"","text":"网络爬虫——超级鹰源码下载超级鹰官方网址：https://www.chaojiying.com/将文件下载再解压，这里使用源码： 源码：#!/usr/bin/env python # coding:utf-8 import requests from hashlib import md5 class Chaojiying_Client(object): def __init__(self, username, password, soft_id): self.username = username password = password.encode(&#39;utf8&#39;) self.password = md5(password).hexdigest() self.soft_id = soft_id self.base_params = { &#39;user&#39;: self.username, &#39;pass2&#39;: self.password, &#39;softid&#39;: self.soft_id, } self.headers = { &#39;Connection&#39;: &#39;Keep-Alive&#39;, &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#39;, } def PostPic(self, im, codetype): &quot;&quot;&quot; im: 图片字节 codetype: 题目类型 参考 http://www.chaojiying.com/price.html &quot;&quot;&quot; params = { &#39;codetype&#39;: codetype, } params.update(self.base_params) files = {&#39;userfile&#39;: (&#39;ccc.jpg&#39;, im)} r = requests.post(&#39;http://upload.chaojiying.net/Upload/Processing.php&#39;, data=params, files=files, headers=self.headers) return r.json() def ReportError(self, im_id): &quot;&quot;&quot; im_id:报错题目的图片ID &quot;&quot;&quot; params = { &#39;id&#39;: im_id, } params.update(self.base_params) r = requests.post(&#39;http://upload.chaojiying.net/Upload/ReportError.php&#39;, data=params, headers=self.headers) return r.json() if __name__ == &#39;__main__&#39;: chaojiying = Chaojiying_Client(&#39;超级鹰用户名&#39;, &#39;超级鹰用户名的密码&#39;, &#39;96001&#39;) #用户中心&gt;&gt;软件ID 生成一个替换 96001 im = open(&#39;a.jpg&#39;, &#39;rb&#39;).read() #本地图片文件路径 来替换 a.jpg 有时WIN系统须要// print chaojiying.PostPic(im, 1902) #1902 验证码类型 官方网站&gt;&gt;价格体系 3.4+版 print 后要加() a.jpg 为自己目标图片 超级鹰用户名及密码。需要自己去官网注册使用。 具体使用方法到我的后继博客中查找。","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"超级鹰","slug":"超级鹰","permalink":"http://ailous.top/tags/超级鹰/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"汇编题——CJNE","slug":"汇编题——CJNE","date":"2020-02-20T05:47:40.000Z","updated":"2020-05-11T14:54:33.455Z","comments":true,"path":"2020/02/20/汇编题——CJNE/","link":"","permalink":"http://ailous.top/2020/02/20/汇编题——CJNE/","excerpt":"","text":"汇编题——CJNE","categories":[{"name":"汇编","slug":"汇编","permalink":"http://ailous.top/categories/汇编/"}],"tags":[{"name":"CJNE","slug":"CJNE","permalink":"http://ailous.top/tags/CJNE/"},{"name":"DJNE","slug":"DJNE","permalink":"http://ailous.top/tags/DJNE/"}],"keywords":[{"name":"汇编","slug":"汇编","permalink":"http://ailous.top/categories/汇编/"}]},{"title":"网络爬虫——前程无忧网数据获取及存储（高级）","slug":"网络爬虫——前程无忧网数据获取及存储(高级)","date":"2019-09-03T05:47:40.000Z","updated":"2020-05-11T14:56:02.176Z","comments":true,"path":"2019/09/03/网络爬虫——前程无忧网数据获取及存储(高级)/","link":"","permalink":"http://ailous.top/2019/09/03/网络爬虫——前程无忧网数据获取及存储(高级)/","excerpt":"","text":"网络爬虫——前程无忧网数据获取及存储（高级）实验内容1目标网站：前程无忧招聘网 目标网址：https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html 目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到txt，csv文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(html): ulist = [] clist = [] rlist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;resultList&quot;]/div[@class=&quot;el&quot;]//text()&#39;) for i in range(len(result)): ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;)) while &#39;&#39; in ulist: ulist.remove(&#39;&#39;) length = len(ulist) weight = int(length / 5 ) for i in range(weight): for j in range(5): clist.append(ulist[i*5+j]) rlist.append(clist) clist = [] return rlist # def txtdata(data): # with open(&#39;top20.txt&#39;,&#39;w&#39;)as file: # for i in data: # for j in i: # print(j) # print(&#39;successful&#39;) def storedata(data): with open(&#39;top20.txt&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;)as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+&#39;\\n&#39;) print(&#39;ok&#39;) def csvdata(data): with open(&#39;top20.csv&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;,newline=&#39;&#39;)as csvfile: fieldnames = [&#39;职位名&#39;,&#39;公司名&#39;,&#39;工作地点&#39;,&#39;薪资&#39;,&#39;工作时间&#39;] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({&#39;职位名&#39;:i[0],&#39;公司名&#39;:i[1],&#39;工作地点&#39;:i[2],&#39;薪资&#39;:i[3],&#39;工作时间&#39;:i[4]}) print(&#39;ok&#39;) def main(): url=&quot;https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html&quot; html=getHtmlText(url) rlist=parsePage(html) # txtdata(data) storedata(rlist) csvdata(rlist) main() 结果输出：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"CSV","slug":"CSV","permalink":"http://ailous.top/tags/CSV/"},{"name":"Xpath","slug":"Xpath","permalink":"http://ailous.top/tags/Xpath/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫常见问题汇总","slug":"网络爬虫常见问题汇总","date":"2019-06-02T05:47:40.000Z","updated":"2020-05-11T14:55:14.644Z","comments":true,"path":"2019/06/02/网络爬虫常见问题汇总/","link":"","permalink":"http://ailous.top/2019/06/02/网络爬虫常见问题汇总/","excerpt":"","text":"网络爬虫常见问题汇总问题一：使用requests库或者urllib库获取源代码时无法正常显示中文解决方法： （1）requests库的文本中有两种类型，一种是文本类型，使用text属性，一种是针对音频、视频、图片等二进制数据类型，使用content属性；一般返回的是text属性时会出现中文乱码现象，因此在输出返回之前需要显示的修改属性encoding，将其赋值为“utf-8”或者是apparent_encoding即可。 （2）urllib库的文本只有一种就是使用read()方法进行读取。因此要解决中文问题，一定要在读取后加入.decode(“utf-8”)，进行显示的转码之后便不会出现乱码问题了。 问题二：文本节点首先看两个HTML代码: 这是你眼中的HTML代码这是计算机眼中的HTML代码:解决方法： 在BS4中, 我们在HTML中看到的换行符以及空格都是NavigableString 也就是文本节点. 问题三：滥用遍历文档树的方法常见的方法有: contentsdescendantsparentnext_siblingnext_element 这些方法都会遍历文档树中的所有节点, 包括文本节点. 也就是说: 只要你使用这些方法, 你就一定会选择出许多文本节点, 因为文本节点无处不在: 换行, 空格等. 解决方法： 使用过滤器find等方法: soup.find(name=’tagname’)当我们一旦在过滤器中指定了name关键字, 那么返回的结果就一定是tag对象, 因为文档节点没有name属性. 结论: 大多数情况下, 你需要的是find 这一类过滤器, 而不是遍历所有节点. 问题四：html.parserhtml.parser是个令人又恨又爱的解析器, 它是Python内置的解析器, 开箱即用. 但是在一些情况下, 它解析出来的文档会丢失信息. 解决方法： 如果你发现你的文档信息缺少了, 那么试着换其他解析器,例如: lxml","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"问题","slug":"问题","permalink":"http://ailous.top/tags/问题/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——二手房数据抓取及MYSQL存储","slug":"网络爬虫——二手房数据抓取及MYSQL存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-11T14:55:35.893Z","comments":true,"path":"2019/05/27/网络爬虫——二手房数据抓取及MYSQL存储/","link":"","permalink":"http://ailous.top/2019/05/27/网络爬虫——二手房数据抓取及MYSQL存储/","excerpt":"","text":"网络爬虫——二手房数据抓取及MYSQL存储目标网址：https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar 目标数据： 标题 + 链接地址 + 厅室+ 面积+ 层数+建造时间 + 地址 + 单价（或总价） 要求： （1）自选请求库和解析库获取目标数据； （2）第一个存储至txt或者csv中，第二个源码存储至Mysql中。 源码（1）：csv,txtimport requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def cleanData(clist): olist = [] for i in range(len(clist)): olist.append(clist[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;).replace(&quot;\\xa0\\xa0&quot;,&#39;,&#39;)) return olist def parsePage(html): ulist = [] clist = [] newhtml =etree.HTML(html,etree.HTMLParser()) titles =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div/a//text()&#39;)) hrefs = cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div/a//@href&#39;, stream=True)) others =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div[2]/span//text()&#39;)) addresss =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div[3]/span//text()&#39;)) prices =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[3]/span[2]//text()&#39;)) length = len(titles) for i in range(length): ulist.append(titles[i]) ulist.append(hrefs[i]) ulist.append(others[i*4+0]) ulist.append(others[i*4+1]) ulist.append(others[i*4+2]) ulist.append(others[i*4+3]) ulist.append(addresss[i]) ulist.append(prices[i]) clist.append(ulist) ulist = [] return clist def txtdata(data): with open(&#39;data.txt&#39;,&#39;w&#39;)as file: for i in data: for j in i: print(j) print(&#39;successful&#39;) def storedata(data): with open(&#39;data.txt&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;)as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+&#39;\\n&#39;) print(&#39;ok&#39;) def csvdata(data): with open(&#39;data.csv&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;,newline=&#39;&#39;)as csvfile: fieldnames = [&#39;标题&#39;,&#39;链接地址&#39;,&#39;厅室&#39;,&#39;面积&#39;,&#39;层数&#39;,&#39;建造时间&#39;,&#39;地址&#39;,&#39;单价&#39;] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({&#39;标题&#39;:i[0],&#39;链接地址&#39;:i[1],&#39;厅室&#39;:i[2],&#39;面积&#39;:i[3],&#39;层数&#39;:i[4],&#39;建造时间&#39;:i[5],&#39;地址&#39;:i[6],&#39;单价&#39;:i[7]}) print(&#39;ok&#39;) def main(): url=&quot;https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar&quot; html=getHtmlText(url) rlist=parsePage(html) txtdata(rlist) storedata(rlist) csvdata(rlist) main() 源码（1）：MYSQLimport requests import json import csv from requests.exceptions import RequestException from lxml import etree import pymysql import pytesseract import traceback def connectMysql(): return pymysql.connect(host=&#39;localhost&#39;,user=&#39;root&#39;,password=&#39;123456&#39;,port=3306,db=&#39;spiders&#39;) def createMysqlTable(): db = connectMysql() cursor = db.cursor() sql = &#39;create table if not exists data (\\ 标题 varchar(255) not null ,\\ 链接地址 varchar(255) not null ,\\ 厅室 varchar(255) not null,\\ 面积 varchar(255) not null,\\ 层数 varchar(255) not null,\\ 建造时间 varchar(255) not null,\\ 地址 varchar(255) not null,\\ 单价 varchar(255) not null,\\ primary key(标题))&#39; cursor.execute(sql) print(&#39;ok&#39;) db.close() def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def cleanData(clist): olist = [] for i in range(len(clist)): olist.append(clist[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;).replace(&quot;\\xa0\\xa0&quot;,&#39;,&#39;)) return olist def parsePage(html): ulist = [] clist = [] newhtml =etree.HTML(html,etree.HTMLParser()) titles =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div/a//text()&#39;)) hrefs = cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div/a//@href&#39;, stream=True)) others =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div[2]/span//text()&#39;)) addresss =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[2]/div[3]/span//text()&#39;)) prices =cleanData(newhtml.xpath(&#39;//*[@id=&quot;houselist-mod-new&quot;]/li/div[3]/span[2]//text()&#39;)) length = len(titles) for i in range(length): ulist.append(titles[i]) ulist.append(hrefs[i][0:100]) ulist.append(others[i*4+0]) ulist.append(others[i*4+1]) ulist.append(others[i*4+2]) ulist.append(others[i*4+3]) ulist.append(addresss[i]) ulist.append(prices[i]) clist.append(ulist) ulist = [] return clist def txtdata(data): with open(&#39;data.txt&#39;,&#39;w&#39;)as file: for i in data: for j in i: print(j) print(&#39;successful&#39;) def storedata(data): with open(&#39;data.txt&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;)as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+&#39;\\n&#39;) print(&#39;ok&#39;) def csvdata(data): with open(&#39;data.csv&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;,newline=&#39;&#39;)as csvfile: fieldnames = [&#39;标题&#39;,&#39;链接地址&#39;,&#39;厅室&#39;,&#39;面积&#39;,&#39;层数&#39;,&#39;建造时间&#39;,&#39;地址&#39;,&#39;单价&#39;] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({&#39;标题&#39;:i[0],&#39;链接地址&#39;:i[1],&#39;厅室&#39;:i[2],&#39;面积&#39;:i[3],&#39;层数&#39;:i[4],&#39;建造时间&#39;:i[5],&#39;地址&#39;:i[6],&#39;单价&#39;:i[7]}) print(&#39;ok&#39;) def mysqlData(datas): table = &#39;data&#39; keys = &#39;标题,链接地址,厅室,面积,层数,建造时间,地址,单价&#39; db = connectMysql() cursor = db.cursor() for data in datas: values = &#39;,&#39;.join([&#39;%s&#39;]*len(data)) sql = &#39;INSERT INTO {table}({keys}) VALUES({values})&#39;.format(table=table,keys = keys ,values = values) print(sql) print(tuple(data)) try : if cursor.execute(sql, tuple(data)): print(&quot;Succcessful&quot;) db.commit() except: traceback.print_exc() print(&quot;Failed&quot;) db.rollback() db.close() def main(): # createMysqlTable() url=&quot;https://qd.anjuke.com/sale/jiaozhoushi/?from=SearchBar&quot; html=getHtmlText(url) rlist=parsePage(html) # txtdata(rlist) storedata(rlist) csvdata(rlist) mysqlData(rlist) main()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"二手房","slug":"二手房","permalink":"http://ailous.top/tags/二手房/"},{"name":"MYSQL","slug":"MYSQL","permalink":"http://ailous.top/tags/MYSQL/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——前程无忧网数据获取及MYSQL存储","slug":"网络爬虫——前程无忧网数据获取及MYSQL存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-11T14:55:54.311Z","comments":true,"path":"2019/05/27/网络爬虫——前程无忧网数据获取及MYSQL存储/","link":"","permalink":"http://ailous.top/2019/05/27/网络爬虫——前程无忧网数据获取及MYSQL存储/","excerpt":"","text":"网络爬虫——前程无忧网数据获取及MYSQL存储实验内容1目标网站：前程无忧招聘网 目标网址：https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html 目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到MYSQL库文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree import pymysql from PIL import Image import pytesseract import traceback def connectMysql(): return pymysql.connect(host=&#39;localhost&#39;,user=&#39;root&#39;,password=&#39;123456&#39;,port=3306,db=&#39;spiders&#39;) def createMysqlTable(): db = connectMysql() cursor = db.cursor() # （1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 sql = &#39;create table if not exists proStr (\\ 职位名 varchar(255) not null ,\\ 公司名 varchar(255) not null,\\ 工作地点 varchar(255) not null,\\ 薪资 varchar(255) not null,\\ 发布时间 varchar(255) not null,\\ primary key(职位名))&#39; cursor.execute(sql) print(&#39;ok&#39;) db.close() def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(html): ulist = [] clist = [] rlist = [] ilist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;content&quot;]/div[2]/table/tbody/tr/td//text()&#39;) imgs = newhtml.xpath(&#39;//*[@id=&quot;content&quot;]/div[2]/table/tbody/tr/td/a/img/@src&#39;, stream=True) j = 0 for img in imgs: j=j+1 with open(str(j)+&#39;.png&#39;, &#39;wb&#39;) as fd: picture=requests.get(img).content fd.write(picture) for i in range(len(imgs)): str_ = str(i+1)+&#39;.png&#39; text = pytesseract.image_to_string(Image.open(str_)) ilist.append(text.replace(&quot; &quot;,&quot;.&quot;).replace(&quot;M&quot;,&quot;亿&quot;).replace(&quot;a&quot;,&quot;亿&quot;)) # print(ilist) for i in range(len(result)): ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;)) while &#39;&#39; in ulist: ulist.remove(&#39;&#39;) length = len(ulist) weight = int(length / 8 ) for i in range(weight): for j in range(8): clist.append(ulist[i*8+j]) clist.append(ilist[i]) rlist.append(clist) clist = [] return rlist def mysqlData(datas): table = &#39;movies&#39; keys = &#39;名次,电影名称,日期,总场次,废场,人次,上座率,票价,票房&#39; db = connectMysql() cursor = db.cursor() for data in datas: values = &#39;,&#39;.join([&#39;%s&#39;]*len(data)) sql = &#39;INSERT INTO {table}({keys}) VALUES({values})&#39;.format(table=table,keys = keys ,values = values) print(sql) print(tuple(data)) try : if cursor.execute(sql, tuple(data)): print(&quot;Succcessful&quot;) db.commit() except: traceback.print_exc() print(&quot;Failed&quot;) db.rollback() db.close() def main(): createMysqlTable() url=&quot;http://58921.com/daily/wangpiao&quot; html=getHtmlText(url) rlist=parsePage(html) mysqlData(rlist) main() 结果输出效果：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://ailous.top/tags/MYSQL/"},{"name":"前程无忧","slug":"前程无忧","permalink":"http://ailous.top/tags/前程无忧/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——票房网数据抓取及MYSQL存储","slug":"网络爬虫——票房网数据抓取及MYSQL存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-11T14:55:47.273Z","comments":true,"path":"2019/05/27/网络爬虫——票房网数据抓取及MYSQL存储/","link":"","permalink":"http://ailous.top/2019/05/27/网络爬虫——票房网数据抓取及MYSQL存储/","excerpt":"","text":"网络爬虫——票房网数据抓取及MYSQL存储实验内容目标网站：电影票房网 目标网址：http://58921.com/daily/wangpiao 任务要求目标数据：（1）名次（2）电影名称 （3）日期（4）票房 （5）总场次（6）废场（7）人次（8）上座率（9）票价 要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到MYSQL库文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree import pymysql from PIL import Image import pytesseract import traceback def connectMysql(): return pymysql.connect(host=&#39;localhost&#39;,user=&#39;root&#39;,password=&#39;123456&#39;,port=3306,db=&#39;spiders&#39;) def createMysqlTable(): db = connectMysql() cursor = db.cursor() # fieldnames = [&#39;名次&#39;,&#39;电影名称&#39;,&#39;日期&#39;,&#39;票房&#39;,&#39;总场次&#39;,&#39;废场&#39;,&#39;人次&#39;,&#39;上座率&#39;,&#39;票价&#39;] sql = &#39;create table if not exists movies (\\ 名次 int not null ,\\ 电影名称 varchar(255) not null ,\\ 日期 varchar(255) not null,\\ 总场次 varchar(255) not null,\\ 废场 varchar(255) not null,\\ 人次 varchar(255) not null,\\ 上座率 varchar(255) not null,\\ 票价 varchar(255) not null,\\ 票房 varchar(255) not null,\\ primary key(名次))&#39; cursor.execute(sql) print(&#39;ok&#39;) db.close() def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(html): ulist = [] clist = [] rlist = [] ilist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;content&quot;]/div[2]/table/tbody/tr/td//text()&#39;) imgs = newhtml.xpath(&#39;//*[@id=&quot;content&quot;]/div[2]/table/tbody/tr/td/a/img/@src&#39;, stream=True) j = 0 for img in imgs: j=j+1 with open(str(j)+&#39;.png&#39;, &#39;wb&#39;) as fd: picture=requests.get(img).content fd.write(picture) for i in range(len(imgs)): str_ = str(i+1)+&#39;.png&#39; text = pytesseract.image_to_string(Image.open(str_)) ilist.append(text.replace(&quot; &quot;,&quot;.&quot;).replace(&quot;M&quot;,&quot;亿&quot;).replace(&quot;a&quot;,&quot;亿&quot;)) # print(ilist) for i in range(len(result)): ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;)) while &#39;&#39; in ulist: ulist.remove(&#39;&#39;) length = len(ulist) weight = int(length / 8 ) for i in range(weight): for j in range(8): clist.append(ulist[i*8+j]) clist.append(ilist[i]) rlist.append(clist) clist = [] return rlist def mysqlData(datas): table = &#39;movies&#39; keys = &#39;名次,电影名称,日期,总场次,废场,人次,上座率,票价,票房&#39; db = connectMysql() cursor = db.cursor() for data in datas: values = &#39;,&#39;.join([&#39;%s&#39;]*len(data)) sql = &#39;INSERT INTO {table}({keys}) VALUES({values})&#39;.format(table=table,keys = keys ,values = values) print(sql) print(tuple(data)) try : if cursor.execute(sql, tuple(data)): print(&quot;Succcessful&quot;) db.commit() except: traceback.print_exc() print(&quot;Failed&quot;) db.rollback() db.close() def main(): createMysqlTable() url=&quot;http://58921.com/daily/wangpiao&quot; html=getHtmlText(url) rlist=parsePage(html) mysqlData(rlist) main() 结果输出效果：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://ailous.top/tags/MYSQL/"},{"name":"票房","slug":"票房","permalink":"http://ailous.top/tags/票房/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——票房网数据抓取及存储（初级）","slug":"网络爬虫——票房网数据抓取及存储","date":"2019-05-27T05:47:40.000Z","updated":"2020-05-11T14:55:51.067Z","comments":true,"path":"2019/05/27/网络爬虫——票房网数据抓取及存储/","link":"","permalink":"http://ailous.top/2019/05/27/网络爬虫——票房网数据抓取及存储/","excerpt":"","text":"网络爬虫——票房网数据抓取及存储实验内容目标网站：电影票房网 目标网址：http://58921.com/daily/wangpiao 任务要求目标数据：（1）名次（2）电影名称 （3）日期（4）票房 （5）总场次（6）废场（7）人次（8）上座率（9）票价 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）自主选择re、bs4、lxml中的一种解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到csv文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(html): ulist = [] clist = [] rlist = [] ilist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;content&quot;]/div[2]/table/tbody/tr/td//text()&#39;) imgs = newhtml.xpath(&#39;//*[@id=&quot;content&quot;]/div[2]/table/tbody/tr/td/a/img/@src&#39;, stream=True) j = 0 for img in imgs: j=j+1 with open(str(j)+&#39;.png&#39;, &#39;wb&#39;) as fd: picture=requests.get(img).content fd.write(picture) for i in range(len(imgs)): str_ = str(i+1)+&#39;.png&#39; text = pytesseract.image_to_string(Image.open(str_)) ilist.append(text.replace(&quot; &quot;,&quot;.&quot;).replace(&quot;M&quot;,&quot;亿&quot;).replace(&quot;a&quot;,&quot;亿&quot;)) # print(ilist) for i in range(len(result)): ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;)) while &#39;&#39; in ulist: ulist.remove(&#39;&#39;) length = len(ulist) weight = int(length / 8 ) for i in range(weight): for j in range(8): clist.append(ulist[i*8+j]) clist.append(ilist[i]) rlist.append(clist) clist = [] return rlist # def txtdata(data): # with open(&#39;top20.txt&#39;,&#39;w&#39;)as file: # for i in data: # for j in i: # print(j) # print(&#39;successful&#39;) def storedata(data): with open(&#39;top20.txt&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;)as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+&#39;\\n&#39;) print(&#39;ok&#39;) def csvdata(data): with open(&#39;top20.csv&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;,newline=&#39;&#39;)as csvfile: fieldnames = [&#39;名次&#39;,&#39;电影名称&#39;,&#39;日期&#39;,&#39;票房&#39;,&#39;总场次&#39;,&#39;废场&#39;,&#39;人次&#39;,&#39;上座率&#39;,&#39;票价（元）&#39;] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({&#39;名次&#39;:i[0],&#39;电影名称&#39;:i[1],&#39;日期&#39;:i[2],&#39;票房&#39;:i[8],&#39;总场次&#39;:i[3],&#39;废场&#39;:i[4],&#39;人次&#39;:i[5],&#39;上座率&#39;:i[6],&#39;票价（元）&#39;:i[7]}) print(&#39;ok&#39;) def main(): url=&quot;http://58921.com/daily/wangpiao&quot; html=getHtmlText(url) rlist=parsePage(html) # txtdata(rlist) storedata(rlist) csvdata(rlist) main() 结果输出：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"票房","slug":"票房","permalink":"http://ailous.top/tags/票房/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——搜狐最新时政新闻数据爬取——BS4","slug":"网络爬虫——搜狐最新时政新闻数据爬取","date":"2019-05-21T05:47:40.000Z","updated":"2020-05-11T14:56:05.935Z","comments":true,"path":"2019/05/21/网络爬虫——搜狐最新时政新闻数据爬取/","link":"","permalink":"http://ailous.top/2019/05/21/网络爬虫——搜狐最新时政新闻数据爬取/","excerpt":"","text":"网络爬虫——搜狐最新时政新闻数据爬取目标网址：https://www.sohu.com/c/8/1460?spm=smpc.null.side-nav.14.1584869506422WxyU9iK 目标数据描述：（1）标题 （2）链接地址要求： （1）使用urllib库或者requests抓取网页源代码； （2）使用BeautifulSoup的CSS选择器方法对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）利用框架结构，通过函数调用，参数传递，实现目标数据抓取，并尝试将结果写入文本文件中。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: print(&quot;A&quot;) return &quot;&quot; def findUniverse(ulist , html): soup = BeautifulSoup(html,&quot;html.parser&quot;) for div in soup.find(attrs=[&#39;class&#39;,&#39;news-list clearfix&#39;]).children: if isinstance(div ,bs4.element.Tag): list_0 = div.find(&#39;h4&#39;).find(&#39;a&#39;).get(&#39;href&#39;) list_1 = div.find(&#39;h4&#39;).string.replace(&quot; &quot;,&#39;&#39;).replace(&quot;\\n&quot;,&#39;&#39;) ulist.append([list_0,list_1]) def findSame(ulist,html): soup = BeautifulSoup(html,&quot;html.parser&quot;) for div in soup.find(attrs=[&#39;class&#39;,&#39;second-nav&#39;]).children: if isinstance(div ,bs4.element.Tag): ulist.append(div.find(&#39;a&#39;).get(&#39;href&#39;)) return ulist def printUniverse(ulist): tplt = &#39;{0:30}\\t{1:18}&#39; print(tplt.format(&quot;网址&quot;,&quot;名称&quot;,chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],chr(12288))) def main(): ulist = [] ulist_Same = [] url = &#39;https://www.sohu.com/c/8/1460?spm=smpc.null.side-nav.14.1585491604691ZcX26aI&#39; html = getHtmlText(url) ulist_Same = findSame(ulist_Same,html) for i in range(len(ulist_Same) - 2 ): url = &#39;https://www.sohu.com&#39; + ulist_Same[i+1] html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"豆瓣电影排行榜数据抓取（高级）——BS4","slug":"网络爬虫——豆瓣电影排行榜数据抓取（高级）","date":"2019-05-04T05:47:40.000Z","updated":"2020-05-11T14:55:28.591Z","comments":true,"path":"2019/05/04/网络爬虫——豆瓣电影排行榜数据抓取（高级）/","link":"","permalink":"http://ailous.top/2019/05/04/网络爬虫——豆瓣电影排行榜数据抓取（高级）/","excerpt":"","text":"网络爬虫——豆瓣电影排行榜数据抓取（高级）目标网址：豆瓣电影排行：https://movie.douban.com/top250?start= 目标数据描述：排名、电影名称、导演、主演、评价人数等信息，将尽可能多的数据抓取保存任务明细： （1）使用requests库实现该网站网页源代码的获取； （2）使用BeautifulSoup对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 可以选择定义全局列表，将目标数据获取后添加到列表中，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding result=(result.text.replace(&#39;&lt;br&gt;&#39;,&#39;&#39;)).replace(&#39;&lt;br/&gt;&#39;,&#39;&#39;) return result except: return &quot;&quot; def findUniverse(ulist , html): soup = BeautifulSoup(html,&quot;html.parser&quot;) list_ = [0,0,0,0,0,0,0] for li in soup.find(attrs=[&#39;class&#39;,&#39;grid_view&#39;]).children: if isinstance(li ,bs4.element.Tag): list_[0] = li.find(&#39;em&#39;).string list_[1] = li.find(attrs=[&#39;class&#39;,&#39;title&#39;]).string list_[2] = li.find(attrs=[&#39;class&#39;,&#39;bd&#39;]).find(attrs=[&#39;class&#39;,&#39;&#39;]).string.strip().split(&quot; &quot;)[1].replace(&quot; &quot;,&#39;&#39;) list_[3] = li.find(attrs=[&#39;class&#39;,&#39;bd&#39;]).find(attrs=[&#39;class&#39;,&#39;&#39;]).string.strip().split(&quot; &quot;)[4].replace(&quot; &quot;,&#39;&#39;) list_[4] = li.find(attrs=[&#39;class&#39;,&#39;star&#39;]).span.find_next_sibling().string.strip() list_[5] = li.find(attrs=[&#39;class&#39;,&#39;star&#39;]).span.find_next_sibling().find_next_sibling().find_next_sibling().string.strip() if li.find(attrs=[&#39;class&#39;,&#39;quote&#39;]) is not None: list_[6] = li.find(attrs=[&#39;class&#39;,&#39;quote&#39;]).span.string else: list_[6] = None ulist.append([list_[0],list_[1],list_[2],list_[3],list_[4],list_[5],list_[6]]) def printUniverse(ulist): tplt = &#39;{0:^4}\\t{1:^10}\\t{2:10}\\t{3:10}\\t{4:10}\\t{5:10}&#39; print(tplt.format(&quot;排名&quot;,&quot;电影名称&quot;,&quot;导演&quot;,&quot;主演&quot;,&quot;评分&quot;,&quot;评价人数&quot;,chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],u[2],u[3],u[4],u[5],chr(12288))) def main(): ulist = [] for i in range(10): url = &#39;https://movie.douban.com/top250?start=&#39; + str( 25 * i ) html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"基于Bilibili热门视频Top100最热分区爬取与分析（报告）","slug":"基于Bilibili热门视频Top100弹幕的数据爬取与分析（报告版）","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-17T22:13:58.558Z","comments":true,"path":"2019/05/03/基于Bilibili热门视频Top100弹幕的数据爬取与分析（报告版）/","link":"","permalink":"http://ailous.top/2019/05/03/基于Bilibili热门视频Top100弹幕的数据爬取与分析（报告版）/","excerpt":"","text":"一、研究背景能够在观看视频的过程中发表自己的评论，并且评论可以在你所希望的时间点、位置以滑行或停留的方式出现在视频中，所有观看视频的人都可以看见评论，这样一类的评论叫做弹幕，此类网站叫弹幕网站。弹幕视频系统源自日本弹幕视频分享网站（niconico动画），国内首先引进为AcFun以及后来的bilibili。大量吐槽评论从屏幕飘过时效果看上去像是飞行射击游戏里的弹幕，所以NICO网民将这种有大量的吐槽评论出现时的效果做弹幕。在中国，本来只有大量评论同时出现才能叫弹幕，但是随着误用单条评论也能叫弹幕了。在国内通常被认为本意是军事用语中密集的炮火射击，过于密集以至于像一张幕布一样。英文称“Bullet Hell”（子弹地狱）或“Bullet Curtain”（弹幕）弹幕可以给观众一种“实时互动”的错觉，虽然不同弹幕的发送时间有所区别，但是其只会在视频中特定的一个时间点出现，因此在相同时刻发送的弹幕基本上也具有相同的主题，在参与评论时就会有与其他观众同时评论的错觉。而传统的播放器评论系统是独立于播放器之外的，因此评论的内容大多围绕在整个视频上，话题性不强，也没有“实时互动”的感觉。 二、研究目的及意义在视频软件层出不穷的当下。除了优质的视频内容，对于社区文化如何探索成为当下各个公司极力探索的目标。新时期自媒体盛行，BiliBili相较于同类软件，其社区文化呈现欣欣向荣之势，拥有较高的日活跃量。而弹幕文化俨然成为其不可或缺的一部分。2019年年末，Bilibili曾发布年度十大热词，‘AWSL’夺得头筹。紧随其后的是‘名场面’，‘逮虾户’……，这些我们日常生活中耳熟能详的‘新词汇’。由于互联网屏蔽了用户的设备差异，因此网络视频服务商可以获得格式统一、更加规范的用户使用数据，根据不同用户的使用数据完成视频推荐。美国著名电子商务企业亚马逊很早就开始探索推荐算法在电子商务中的应用，并已经取得了显著的成果。发表弹幕、观看弹幕，本身就有一种实时互动的错觉，完成着实实在在的、直接的互动。网站会根据用户的点击量进行视频推荐，参与热烈讨论的视频会出现在主页，受众能最快时间地看到页面。将来也许还可以通过搜索引擎的优化，把热门话题，热门词语，热门搜索等作为视频推荐的依据，让受众观看到与自己兴趣最相符的视频。通过对于当前时间段热门排名TOP100中弹幕进行分析，将数据进行可视化处理，得到最热词汇，既可以知道在这一时间段网络舆论流行的大体趋势，把握用户心里态度，加强受众的互动反馈。还可以激发用户对于弹幕文化的探索的兴趣。使得用户保持新鲜度，延长软件寿命。三、实验环境及技术介绍1.爬虫技术简介网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，其实通俗的讲就是通过程序去获取web页面上自己想要的数据，也就是自动抓取数据。爬虫的目地在于将目标网页数据下载至本地，以便进行后续的数据分析.爬虫技术的兴起源于海量网络数据的可用性,通过爬虫技术,能够较为容易的获取网络数据，并通过对数据的分析,得出有具有价值的结论。Python语言简单易用,现成的爬虫框架和工具包降低了使用门槛,具体使用时配合正则表达式的运用,使得数据抓取工作变得生动有趣。2.所用到的python库2.1requestsRequests 是用Python语言编写，基于 urllib，采用Apache2 Licensed 开源协议的HTTP库。Requests 继承了urllib的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的URL和POST 数据自动编码。主要用于请求URL，获取返回信息，打印输出响应头，和重定向等。在爬取数据过程中，使用了request库的get方法来发送请求。Request请求方式有：GET:请求指定的页面信息，并返回实体主体。HEAD:只请求页面的首部。POST:请求服务器接受所指定的文档作为对所标识的URI的新的从属实体。PUT:从客户端向服务器传送的数据取代指定的文档的内容。DELETE:请求服务器删除指定的页面。get和post比较常见GET请求将提交的数据放置在HTTP请求协议头中POST提交的数据则放在实体数据中2.2XPathXPath 是一门在 XML文档中查找信息的语言。用于在 XML文档中通过元素和属性进行导航，选取XML文档中的节点或者节点集。这些路径表达式和在常规的电脑文件系统中看到的表达式非常相似。在爬取豆瓣TOP250的过程中，使用xpath查找歌曲名、歌手、评分等信息。图2-2 Xpath路径表达式图2.3lxmllxml 是一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML /XML数据。lxml和正则一样，也是用C实现的，是一款高性能的Python HTML /XML 解析器，可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。使用lxml.etree处理XML文档。简要讲述ElementTree API的主要要概念，和一些简单的增强，让处理XML更简单。2.4Jieba jieba是优秀的中文分词第三方库。通过分词的方式从中文文本获得单个的词语，但是属于拓展库，需要额外安装。其拥有三种分词模式，最简单只需掌握一个函数。jieba分词的原理： 其依靠中文词库，利用一个中文词库，确定汉字之间的关联概率， 将汉字间相邻概率大的组成词组，形成分词结果， 除了分词，用户还可以添加自定义的词组。2.5WordCloudwordcloud库，可以说是python非常优秀的词云展示第三方库。词云以词语为基本单位更加直观和艺术的展示文本，也叫文字云，对文本中出现频率较高的“关键词”予以视觉化的展现，过滤掉大量的低频低质的文本信息，使得浏览者只要一眼扫过文本就可领略文本的主旨。3.数据分析工具3.1Jupyter NotebookNotebooks是Donald Knuth 1984年提出的文本化编程的一种形式。Jupyter Notebook 的本质是一个 Web 应用程序，结合文本化编程，文本和代码交错在一起，而不是分成两个独立地本分。是个集成文本，数学公式，代码和可视化的可分享文本。便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。Notebooks 很快已经成为了数据操作不可或缺的工具。它在 大数据清理和探究,可视化,机器学习, 和 大数据分析中都有广泛运用。并且Notebooks 可以直接在github直接被读取. 这是一个非常有用的功能，可以方便地分享。3.2ExcelExcel不仅是一个数据存储工具，还是一个简单的数据分析工具，添加EXCEL数据分析插件后，可以做一些简单相关、回归等分析。而Excel可以说是万能但又不是万能的，学习Excel就是为了用来统计数据分析数据的，大而复杂的数据和分析有时候用Excel处理并不是最佳选择。Excel和数据库之间可以进行数据转换，但是当Excel的数据量过大的时候，它的查询和计算的速度会明显下降。Excel提供了有限的安全性，它只能限制用户访问和修改的权限，但是无法对用户进行角色的管理，也不能对数据进行行级的访问限制。当然Excel也有自身的的优势之处：1、数据透视功能。2、统计分析，非常独特，常用的检验方式一键搞定。3、图表功能， Excel拥有各种丰富的可开发的图表形式的独门武工。4、自动汇总功能，这个功能其他程序都有，但是Excel简便灵活。总地来说，Excel适合于开发单机版、访问量与开发维护量都不是很大、对数据有分析建模功能的应用程序。四、数据爬取4.1抓取Bilibili热门视频Top100的网址本实验在浏览器搜索Bilibili热门视频Top100的网址,就能得到要采集数据的页面，它的URL为https://www.bilibili.com/ranking，页面如下图4-1所示。图4-1Bilibili热门视频Top100排行图4.2对网址进行转化，获得Oid号码通过对网页源代码的处理，将网页源地址中的地址数据转换成AV地址样式，再通过正则表达式’cid=(.*?)&amp;aid’，获取其Cid码，最后再转化成Oid码。获取Oid码的源代码如下图4-2所示。图4-2 Oid码源代码获取图获取Oid码的代码运行结果如下图4-2所示。图4-3代码运行结果4.3抓取弹幕信息lxml文件首先，导入requests包，利用requests构建一个简单地的GET请求，把网页的headers（其中包含了User-Agent字段信息，是浏览器的标识信息）输入，这时网站会判断如果是客户端发起的GET请求，它会返回相应的请求信息。通过Oid码才能得到弹幕存储所在的lxml文件。Lxml代码如下图4-3所示。图4-4 Lxml获取代码图4-5 Lxml文件图4.4抓取所有弹幕信息存储并处理将所得到的lxml文件，通过xpath进行抓取并且存储到数组results中。图4-6 弹幕信息爬取代码通过append方法将所有弹幕信息存储，以待处理，如下所示：图4-7 所有弹幕整合代码通过自定义的remove_double_barrage（）方法将弹幕信息进行去重处理，如下所示：图4-8 弹幕信息去重代码4.5存储数据利用lxml库采集视频的弹幕内容信息。将弹幕内容barrage，及弹幕出现次数amount进行存储，具体代码如下图4-3所示。图4-9存储数据图4.6数据展示将采集的数据合并在一起,将采集的数据存入txt中，从文件中可以看出，收集的信息字段为‘弹幕内容’，‘出现次数’。进行数据分析就是对这两字段进行数据分析和可视化展示。部分数据展示如下图4-4所示：图4-10数据展示图 五、数据分析利用wordCloud对爬取的数据进行分析来进一步呈现。5.1最热弹幕分析对Bilibili热门视频Top100弹幕进行分析，选取榜上所有弹幕进行针对性分析：从图中可以明显看出，统计期间最热的弹幕要数“奔涌吧，后浪”。其后是：“狗头”“后浪，奔涌吧”“doge”……图5-1 最热弹幕词云5.2最热区分析Bilibili包括有动画、番剧、音乐、舞蹈、游戏、科技、生活、鬼畜、时尚、广告、娱乐、影视等多个分区，通过Top100上榜分区处理分析得到当前最热分区。图5-2 数据统计样本·图5-3 样本分析 六、总结从定义上来说，爬虫就是模拟用户自动浏览并且保存网络数据的程序，当然，大部分的爬虫都是爬取网页信息（文本，图片，媒体流）。无论是动态还是静态网页，所有的用户可以直观看到的，都有爬取下来的可能。最简单的流程：获取网页源代码，分析源码，存储数据。即从功能上来讲的，数据采集，处理，储存三个部分。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页URL，并重复上述过程，直到达到系统的某一条件时停止。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索；对于聚焦爬虫来说，这一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。发起请求：通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器响应。获取响应内容：如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。解析内容：得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。保存数据：保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。七、附录源代码请看下一篇","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"文章","slug":"文章","permalink":"http://ailous.top/tags/文章/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"基于Bilibili热门视频Top100弹幕的数据爬取与分析（源代码）","slug":"基于Bilibili热门视频Top100弹幕的数据爬取与分析（源代码）","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-17T22:13:55.007Z","comments":true,"path":"2019/05/03/基于Bilibili热门视频Top100弹幕的数据爬取与分析（源代码）/","link":"","permalink":"http://ailous.top/2019/05/03/基于Bilibili热门视频Top100弹幕的数据爬取与分析（源代码）/","excerpt":"","text":"网络爬虫——基于Bilibili热门视频Top100弹幕的数据爬取与分析实验内容目标网站：Bilibili热门视频Top100 目标网址：https://www.bilibili.com/ranking?（每过几天都会变的哦） 任务要求实现对于Bilibili热门视频Top100弹幕的数据爬取与分析 源码from lxml import etree import time import jieba import numpy as np from PIL import Image import requests import re from requests.exceptions import RequestException from wordcloud import WordCloud as wc class getUrl (): def __init__(self,url): self.url = url def getTxt(self): self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[1]/a//@href&#39;) return result class getAvNum(): def __init__(self,url): self.url = url def getTxt(self): self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) rlist=newhtml.xpath(&#39;/html/head/meta[10]//@content&#39;) resp = requests.get(rlist[0],headers=self.headers) match_rule = r&#39;cid=(.*?)&amp;aid&#39; oid = re.search(match_rule,resp.text).group().replace(&#39;cid=&#39;,&#39;&#39;).replace(&#39;&amp;aid&#39;,&#39;&#39;) return oid class Bilibili(): def __init__(self,oid): self.headers={ &#39;Host&#39;: &#39;api.bilibili.com&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cache-Control&#39;: &#39;max-age=0&#39;, &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36&#39;, &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.9&#39;, &#39;Cookie&#39;: &#39;finger=edc6ecda; LIVE_BUVID=AUTO1415378023816310; stardustvideo=1; CURRENT_FNVAL=8; buvid3=0D8F3D74-987D-442D-99CF-42BC9A967709149017infoc; rpdid=olwimklsiidoskmqwipww; fts=1537803390&#39; } self.url=&#39;https://api.bilibili.com/x/v1/dm/list.so?oid=&#39;+str(oid) self.barrage_reault=self.get_page() def get_page(self): try: time.sleep(0.5) response=requests.get(self.url,headers=self.headers) except Exception as e: print(&#39;获取xml内容失败,%s&#39; % e) return False else: if response.status_code == 200: with open(&#39;bilibili.xml&#39;,&#39;wb&#39;) as f: f.write(response.content) return True else: return False def param_page(self): time.sleep(1) if self.barrage_reault: html=etree.parse(&#39;bilibili.xml&#39;,etree.HTMLParser()) results=html.xpath(&#39;//d//text()&#39;) return results def remove_double_barrage(resultlist): double_barrage=[] results=[] barrage=set() for result in resultlist: if result not in results: results.append(result) else: double_barrage.append(result) barrage.add(result) return double_barrage,results,barrage def make_wordCould(resultlist): double_barrages,results,barrages=remove_double_barrage(resultlist) # 重词计数 with open(&#39;barrages.txt&#39;,&#39;w&#39;, -1, &#39;utf-8&#39;, None, None) as f: for barrage in barrages: amount=double_barrages.count(barrage) stt = barrage+&#39;:&#39;+str(amount+1)+&#39;\\n&#39; f.write(stt) # 设置停用词 stop_words=[&#39;【&#39;,&#39;】&#39;,&#39;,&#39;,&#39;.&#39;,&#39;?&#39;,&#39;!&#39;,&#39;。&#39;] words=[] if results: for result in results: for stop in stop_words: result=&#39;&#39;.join(result.split(stop)) words.append(result) # 列表拼接成字符串 words=&#39;&#39;.join(words) words=jieba.cut(words) words=&#39;&#39;.join(words) luo=np.array(Image.open(&#39;洛天依.jpg&#39;)) w=wc(font_path=&#39;‪C:/Windows/Fonts/SIMYOU.TTF&#39;,background_color=&#39;white&#39;,width=1600,height=1600,max_words=2000,mask=luo) w.generate(words) w.to_file(&#39;luo.jpg&#39;) def main(): url=&quot;https://www.bilibili.com/ranking?spm_id_from=333.851.b_7072696d61727950616765546162.3&quot; urls = getUrl(url) strUrl = urls.parsePage() ress = [] for i in strUrl: AV = getAvNum(i) oid = AV.parsePage() b=Bilibili(oid) for j in b.param_page(): ress.append(j) make_wordCould(ress) if __name__ == &#39;__main__&#39;: main() 具体文件可以到我的github上面下载","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"xpath","slug":"xpath","permalink":"http://ailous.top/tags/xpath/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"基于Bilibili热门视频Top100最热分区爬取与分析","slug":"基于Bilibili热门视频Top100最热分区爬取与分析","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-17T22:13:12.440Z","comments":true,"path":"2019/05/03/基于Bilibili热门视频Top100最热分区爬取与分析/","link":"","permalink":"http://ailous.top/2019/05/03/基于Bilibili热门视频Top100最热分区爬取与分析/","excerpt":"","text":"网络爬虫——基于Bilibili热门视频Top100最热分区爬取与分析这个主要是配合之前报告使用的，这里就不多赘述了。先爬地址，再爬取地址里的分区，最后整合。csv存储。 实验内容目标网站：Bilibili热门视频Top100 目标网址：https://www.bilibili.com/ranking?（每过几天都会变的哦） from lxml import etree import time import jieba import numpy as np from PIL import Image import requests import re from requests.exceptions import RequestException from wordcloud import WordCloud as wc import csv class getUrl (): def __init__(self,url): self.url = url def getTxt(self): self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[1]/a//@href&#39;) return result class getAvNum(): def __init__(self,url): self.url = url def getTxt(self): self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(self.url,headers=self.headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(self): res = self.getTxt() newhtml =etree.HTML(res,etree.HTMLParser()) rlist=newhtml.xpath(&#39;//*[@id=&quot;viewbox_report&quot;]/div[1]/span[1]/a[1]//text()&#39;) return rlist class Bilibili(): def __init__(self,oid): self.headers={ &#39;Host&#39;: &#39;api.bilibili.com&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cache-Control&#39;: &#39;max-age=0&#39;, &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36&#39;, &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.9&#39;, &#39;Cookie&#39;: &#39;finger=edc6ecda; LIVE_BUVID=AUTO1415378023816310; stardustvideo=1; CURRENT_FNVAL=8; buvid3=0D8F3D74-987D-442D-99CF-42BC9A967709149017infoc; rpdid=olwimklsiidoskmqwipww; fts=1537803390&#39; } self.url=&#39;https://api.bilibili.com/x/v1/dm/list.so?oid=&#39;+str(oid) self.barrage_reault=self.get_page() def get_page(self): try: time.sleep(0.5) response=requests.get(self.url,headers=self.headers) except Exception as e: print(&#39;获取xml内容失败,%s&#39; % e) return False else: if response.status_code == 200: with open(&#39;bilibili.xml&#39;,&#39;wb&#39;) as f: f.write(response.content) return True else: return False def param_page(self): time.sleep(1) if self.barrage_reault: html=etree.parse(&#39;bilibili.xml&#39;,etree.HTMLParser()) results=html.xpath(&#39;//d//text()&#39;) return results def remove_double_barrage(resultlist): double_barrage=[] results=[] barrage=set() for result in resultlist: if result not in results: results.append(result) else: double_barrage.append(result) barrage.add(result) return double_barrage,results,barrage def make_wordCould(resultlist): double_barrages,results,barrages=remove_double_barrage(resultlist) with open(&#39;barrages.txt&#39;,&#39;w&#39;, -1, &#39;utf-8&#39;, None, None) as f: for barrage in barrages: amount=double_barrages.count(barrage) stt = barrage+&#39;:&#39;+str(amount+1)+&#39;\\n&#39; f.write(stt) stop_words=[&#39;【&#39;,&#39;】&#39;,&#39;,&#39;,&#39;.&#39;,&#39;?&#39;,&#39;!&#39;,&#39;。&#39;] words=[] if results: for result in results: for stop in stop_words: result=&#39;&#39;.join(result.split(stop)) words.append(result) # 列表拼接成字符串 words=&#39;&#39;.join(words) words=jieba.cut(words) words=&#39;&#39;.join(words) luo=np.array(Image.open(&#39;洛天依.jpg&#39;)) w=wc(font_path=&#39;‪C:/Windows/Fonts/SIMYOU.TTF&#39;,background_color=&#39;white&#39;,width=1600,height=1600,max_words=2000,mask=luo) w.generate(words) w.to_file(&#39;luo.jpg&#39;) def csvdata(data): with open(&#39;top20.csv&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;,newline=&#39;&#39;)as csvfile: fieldnames = [&#39;分类&#39;] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({&#39;分类&#39;:i[0]}) print(&#39;ok&#39;) def main(): url=&quot;https://www.bilibili.com/ranking?spm_id_from=333.851.b_7072696d61727950616765546162.3&quot; urls = getUrl(url) strUrl = urls.parsePage() ress = [] for i in strUrl: AV = getAvNum(i) oid = AV.parsePage() ress.append(oid) csvdata(ress) if __name__ == &#39;__main__&#39;: main()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"xpath","slug":"xpath","permalink":"http://ailous.top/tags/xpath/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"豆瓣电影排行榜数据抓取（初级）——BS4","slug":"网络爬虫——豆瓣电影排行榜数据抓取（初级）","date":"2019-05-03T05:47:40.000Z","updated":"2020-05-11T14:55:23.897Z","comments":true,"path":"2019/05/03/网络爬虫——豆瓣电影排行榜数据抓取（初级）/","link":"","permalink":"http://ailous.top/2019/05/03/网络爬虫——豆瓣电影排行榜数据抓取（初级）/","excerpt":"","text":"网络爬虫——豆瓣电影排行榜数据抓取（初级）目标网址：豆瓣电影排行：https://movie.douban.com/top250?start= 目标数据描述：（1）排名（2）电影名称任务明细： （1）使用requests库实现该网站网页源代码的获取； （2）使用BeautifulSoup对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 可以选择定义全局列表，将目标数据获取后添加到列表中，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。 下一阶段，目标数据增加导演、主演、评价人数等信息，将尽可能多的数据抓取保存。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding result=(result.text.replace(&#39;&lt;br&gt;&#39;,&#39;&#39;)).replace(&#39;&lt;br/&gt;&#39;,&#39;&#39;) return result except: return &quot;&quot; def findUniverse(ulist , html): soup = BeautifulSoup(html,&quot;html.parser&quot;) list = [0,0] for li in soup.find(attrs=[&#39;class&#39;,&#39;grid_view&#39;]).children: if isinstance(li ,bs4.element.Tag): list[0] = li.find(&#39;em&#39;).string list[1] = li.find(attrs=[&#39;class&#39;,&#39;title&#39;]).string ulist.append([list[0],list[1]]) def printUniverse(ulist): tplt = &#39;{0:^10}\\t{1:^10}&#39; print(tplt.format(&quot;排名&quot;,&quot;电影名称&quot;,chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],chr(12288))) def main(): ulist = [] url = &#39;https://movie.douban.com/top250?start=&#39; html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——抓取TIOBE指数前20名排行开发语言","slug":"网络爬虫——抓取TIOBE指数前20名排行开发语言","date":"2019-04-27T05:47:40.000Z","updated":"2020-05-11T14:56:13.231Z","comments":true,"path":"2019/04/27/网络爬虫——抓取TIOBE指数前20名排行开发语言/","link":"","permalink":"http://ailous.top/2019/04/27/网络爬虫——抓取TIOBE指数前20名排行开发语言/","excerpt":"","text":"网络爬虫——抓取TIOBE指数前20名排行开发语言目标网址TIOBE指数前20名排行开发语言：https://www.tiobe.com/tiobe-index/ 说明 TIOBE排行榜是根据互联网上有经验的程序员、课程和第三方厂商的数量，并使用搜索引擎（如Google、Bing、Yahoo!）以及Wikipedia、Amazon、YouTube统计出排名数据，只是反映某个编程语言的热门程度，并不能说明一门编程语言好不好，或者一门语言所编写的代码数量多少。 该指数可以用来检阅开发者的编程技能能否跟上趋势，或是否有必要作出战略改变，以及什么编程语言是应该及时掌握的。观察认为，该指数反应的虽并非当前最流行或应用最广的语言，但对世界范围内开发语言的走势仍具有重要参考意义。 目标数据：（如上表所示） （1）2020年3月的排名（2）2019年3月排名（3）编程语言（4）评分（5）变化率 明细：（1）使用urllib或者requests库抓取目标网页中的网页源代码； （2）使用lxml库中的xpath方法解析源代码，提取上面所示的目标数据，并打印输出； （3）尝试着使用try..except方法及时捕获异常。 （4）可以尝试将获取的数据保存到文本文件中。 源码import requests from requests.exceptions import RequestException from lxml import etree def one_to_page(url): headers={ &#39;user-agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3756.400 QQBrowser/10.5.4039.400&#39; } try: response=requests.get(url,headers=headers) body=response.text return body except RequestException as e: print(&#39;request is error!&#39;,e) def parsePage(html): htmlNew = etree.HTML(html,etree.HTMLParser()) result = htmlNew.xpath(&#39;//table[contains(@class,&quot;table-top20&quot;)]/tbody/tr//text()&#39;) pos = 0 for i in range(20): if i == 0: yield result[i:5] else: yield result[pos:pos+5] pos += 5 def printRank(data): for i in data: rank = { &quot;2020年3月&quot;:i[0], &quot;2019年3月&quot;:i[1], &quot;编程语言&quot;:i[2], &quot;评分&quot;:i[3], &quot;变化率&quot;:i[4], } print(rank) def printRankEasy(data): tplt = &quot;{0:^10}\\t{1:^10}\\t{2:^20}\\t{3:^10}\\t{4:^10}&quot; print(tplt.format(&quot;2020年3月&quot;,&quot;2019年3月&quot;,&quot;编程语言&quot;,&quot;评分&quot;,&quot;变化率&quot;,chr(12288))) for i in data: print(tplt.format(i[0],i[1],i[2],i[3],i[4],chr(12288))) def main(): url = &#39;https://www.tiobe.com/tiobe-index/&#39; html = one_to_page(url) data = parsePage(html) printRankEasy(data) main() 输出如下","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"Xpath","slug":"Xpath","permalink":"http://ailous.top/tags/Xpath/"},{"name":"TIOBE指数","slug":"TIOBE指数","permalink":"http://ailous.top/tags/TIOBE指数/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"中国大学排名数据抓取","slug":"网络爬虫——中国大学排名数据抓取","date":"2019-04-20T05:47:40.000Z","updated":"2020-05-11T14:56:09.877Z","comments":true,"path":"2019/04/20/网络爬虫——中国大学排名数据抓取/","link":"","permalink":"http://ailous.top/2019/04/20/网络爬虫——中国大学排名数据抓取/","excerpt":"","text":"网络爬虫——中国大学排名数据抓取目标网址中国大学排名网：http://www.zuihaodaxue.com/zuihaodaxuepaiming2019.html 全球有很多份大学排名，这里以上海交通大学研发的“软科中国最好大学排名2019”为例，，编写“大学排名爬虫”，从网络上获取数据 。拟从该网址爬取该名单上310 所国内大学的排名数据，并将它们打印出来。 大学排名爬虫的构建需要三个重要步骤： 第一，从网络上获取网页内容； 第二，分析网页内容并提取有用数据到恰当的数据结构中； 第三，利用数据结构展示或进一步处理数据。 由于大学排名是一个典型的二维数据，因此，采用二维列表存储该排名所涉及的表单数据。具体来说，采用requests 库爬取网页内容，使用beautifulsoup4 库分析网页中数据，提取310 个学校的排名及相关数据，存储到二维列表中，最后采用用户偏好的方式打印出来。 源码import requests from bs4 import BeautifulSoup import bs4 def getHtmlText(url): try: result = requests.get(url,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def findUniverse(ulist , html): soup = BeautifulSoup(html,&quot;html.parser&quot;) for tr in soup.find(attrs=[&#39;class&#39;,&#39;hidden_zhpm&#39;]).children: if isinstance(tr ,bs4.element.Tag): tds = tr(&#39;td&#39;) ulist.append([tds[0].string, tds[1].string,tds[3].string ]) def printUniverse(ulist): tplt = &#39;{0:^10}\\t{1:{3}^10}\\t{2:^10}&#39; print(tplt.format(&quot;排名&quot;,&quot;学校名称&quot;,&quot;总分&quot;,chr(12288))) for i in range(len(ulist)): u = ulist[i] print(tplt.format(u[0],u[1],u[2],chr(12288))) def main(): ulist = [] url = &#39;http://www.zuihaodaxue.com/zuihaodaxuepaiming2019.html&#39; html = getHtmlText(url) findUniverse(ulist,html) printUniverse(ulist) main() 结果输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"BS4","slug":"BS4","permalink":"http://ailous.top/tags/BS4/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——前程无忧网数据获取及存储（低级）","slug":"网络爬虫——前程无忧网数据获取及存储（低级）","date":"2019-04-10T05:47:40.000Z","updated":"2020-05-11T14:55:58.623Z","comments":true,"path":"2019/04/10/网络爬虫——前程无忧网数据获取及存储（低级）/","link":"","permalink":"http://ailous.top/2019/04/10/网络爬虫——前程无忧网数据获取及存储（低级）/","excerpt":"","text":"网络爬虫——前程无忧网数据获取及存储（低级）目标网站：前程无忧招聘网目标网址：https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html 目标数据：（1）职位名（2）公司名（3）工作地点（4）薪资 （5）发布时间 任务要求 （1）使用urllib或requests库实现该网站网页源代码的获取，并将源代码进行保存； （2）通过Xpath解析方法对保存的的源代码读取并进行解析，成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据保存到txt文本文件中。 这里使用的是Xpath，相对于之前猫眼电影使用的，这个较为简单，但是数据处理上较为复杂。 源码import requests import json import csv from requests.exceptions import RequestException from lxml import etree def getHtmlText(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Edg/80.0.361.69&#39; } try: result = requests.get(url,headers=headers,timeout=30) result.raise_for_status() result.encoding = result.apparent_encoding return result.text except: return &quot;&quot; def parsePage(html): ulist = [] clist = [] rlist = [] newhtml =etree.HTML(html,etree.HTMLParser()) result=newhtml.xpath(&#39;//*[@id=&quot;resultList&quot;]/div[@class=&quot;el&quot;]//text()&#39;) for i in range(len(result)): ulist.append(result[i].replace(&quot; &quot;,&quot;&quot;).replace(&#39;\\r&#39;,&quot;&quot;).replace(&quot;\\n&quot;,&#39;&#39;)) while &#39;&#39; in ulist: ulist.remove(&#39;&#39;) length = len(ulist) weight = int(length / 5 ) for i in range(weight): for j in range(5): clist.append(ulist[i*5+j]) rlist.append(clist) clist = [] return rlist # def txtdata(data): # with open(&#39;top20.txt&#39;,&#39;w&#39;)as file: # for i in data: # for j in i: # print(j) # print(&#39;successful&#39;) def storedata(data): with open(&#39;top20.txt&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;)as file: for i in data: file.write(json.dumps(i,ensure_ascii=False)+&#39;\\n&#39;) print(&#39;ok&#39;) def csvdata(data): with open(&#39;top20.csv&#39;,&#39;w&#39;,encoding = &#39;utf-8&#39;,newline=&#39;&#39;)as csvfile: fieldnames = [&#39;职位名&#39;,&#39;公司名&#39;,&#39;工作地点&#39;,&#39;薪资&#39;,&#39;工作时间&#39;] writer = csv.DictWriter(csvfile,fieldnames=fieldnames) writer.writeheader() for i in data: writer.writerow({&#39;职位名&#39;:i[0],&#39;公司名&#39;:i[1],&#39;工作地点&#39;:i[2],&#39;薪资&#39;:i[3],&#39;工作时间&#39;:i[4]}) print(&#39;ok&#39;) def main(): url=&quot;https://search.51job.com/list/120000,000000,0000,00,9,99,Python,2,1.html&quot; html=getHtmlText(url) rlist=parsePage(html) # txtdata(data) storedata(rlist) csvdata(rlist) main()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"Xpath","slug":"Xpath","permalink":"http://ailous.top/tags/Xpath/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"Hexo-Theme-Sakura","slug":"Hexo-Theme-Sakura","date":"2018-12-12T14:16:01.000Z","updated":"2020-05-10T17:38:03.607Z","comments":true,"path":"2018/12/12/Hexo-Theme-Sakura/","link":"","permalink":"http://ailous.top/2018/12/12/Hexo-Theme-Sakura/","excerpt":"","text":"hexo-theme-sakura主题 English document 基于WordPress主题Sakura修改成Hexo的主题。 demo预览 正在开发中…… 交流群若你是使用者，加群QQ: 801511924 若你是创作者，加群QQ: 194472590 主题特性 首页大屏视频 首页随机封面 图片懒加载 valine评论 fancy-box相册 pjax支持，音乐不间断 aplayer音乐播放器 多级导航菜单（按现在大部分hexo主题来说，这也算是个特性了） 赞赏作者如果喜欢hexo-theme-sakura主题，可以考虑资助一下哦~非常感激！ paypal | Alipay 支付宝 | WeChat Pay 微信支付 未完善的使用教程那啥？老实说我目前也不是很有条理233333333~ 1、主题下载安装hexo-theme-sakura建议下载压缩包格式，因为除了主题内容还有些source的配置对新手来说比较太麻烦，直接下载解压就省去这些麻烦咯。 下载好后解压到博客根目录（不是主题目录哦，重复的选择替换）。接着在命令行（cmd、bash）运行npm i安装依赖。 2、主题配置博客根目录下的_config配置站点 # Site title: 你的站点名 subtitle: description: 站点简介 keywords: author: 作者名 language: zh-cn timezone: 部署 deploy: type: git repo: github: 你的github仓库地址 # coding: 你的coding仓库地址 branch: master 备份 （使用hexo b发布备份到远程仓库） backup: type: git message: backup my blog of https://honjun.github.io/ repository: # 你的github仓库地址,备份分支名 （建议新建backup分支） github: https://github.com/honjun/honjun.github.io.git,backup # coding: https://git.coding.net/hojun/hojun.git,backup 主题目录下的_config配置其中标明【改】的是需要修改部门，标明【选】是可改可不改，标明【非】是不用改的部分 # site name # 站点名 【改】 prefixName: さくら荘その siteName: hojun # favicon and site master avatar # 站点的favicon和头像 输入图片路径（下面的配置是都是cdn的相对路径，没有cdn请填写完整路径，建议使用jsdeliver搭建一个cdn啦，先去下载我的cdn替换下图片就行了，简单方便~）【改】 favicon: /images/favicon.ico avatar: /img/custom/avatar.jpg # 站点url 【改】 url: https://sakura.hojun.cn # 站点介绍（或者说是个人签名）【改】 description: Live your life with passion! With some drive! # 站点cdn，没有就为空 【改】 若是cdn为空，一些图片地址就要填完整地址了，比如之前avatar就要填https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/custom/avatar.jpg cdn: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6 # 开启pjax 【选】 pjax: 1 # 站点首页的公告信息 【改】 notice: hexo-Sakura主题已经开源，目前正在开发中... # 懒加载的加载中图片 【选】 lazyloadImg: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/loader/orange.progress-bar-stripe-loader.svg # 站点菜单配置 【选】 menus: 首页: { path: /, fa: fa-fort-awesome faa-shake } 归档: { path: /archives, fa: fa-archive faa-shake, submenus: { 技术: {path: /categories/技术/, fa: fa-code }, 生活: {path: /categories/生活/, fa: fa-file-text-o }, 资源: {path: /categories/资源/, fa: fa-cloud-download }, 随想: {path: /categories/随想/, fa: fa-commenting-o }, 转载: {path: /categories/转载/, fa: fa-book } } } 清单: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { 书单: {path: /tags/悦读/, fa: fa-th-list faa-bounce }, 番组: {path: /bangumi/, fa: fa-film faa-vertical }, 歌单: {path: /music/, fa: fa-headphones }, 图集: {path: /tags/图集/, fa: fa-photo } } } 留言板: { path: /comment/, fa: fa-pencil-square-o faa-tada } 友人帐: { path: /links/, fa: fa-link faa-shake } 赞赏: { path: /donate/, fa: fa-heart faa-pulse } 关于: { path: /, fa: fa-leaf faa-wrench , submenus: { 我？: {path: /about/, fa: fa-meetup}, 主题: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } 客户端: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse } # Home page sort type: -1: newer first，1: older first. 【非】 homePageSortType: -1 # Home page article shown number) 【非】 homeArticleShown: 10 # 背景图片 【选】 bgn: 8 # startdash面板 url, title, desc img 【改】 startdash: - {url: /theme-sakura/, title: Sakura, desc: 本站 hexo 主题, img: /img/startdash/sakura.md.png} - {url: http://space.bilibili.com/271849279, title: Bilibili, desc: 博主的b站视频, img: /img/startdash/bilibili.jpg} - {url: /, title: hojun的万事屋, desc: 技术服务, img: /img/startdash/wangshiwu.jpg} # your site build time or founded date # 你的站点建立日期 【改】 siteBuildingTime: 07/17/2018 # 社交按钮(social) url, img PC端配置 【改】 social: github: {url: http://github.com/honjun, img: /img/social/github.png} sina: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/sina.png} wangyiyun: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/wangyiyun.png} zhihu: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/zhihu.png} email: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/email.svg} wechat: {url: /#, qrcode: /img/custom/wechat.jpg, img: /img/social/wechat.png} # 社交按钮(msocial) url, img 移动端配置 【改】 msocial: github: {url: http://github.com/honjun, fa: fa-github, color: 333} weibo: {url: http://weibo.com/mashirozx?is_all=1, fa: fa-weibo, color: dd4b39} qq: {url: https://wpa.qq.com/msgrd?v=3&amp;uin=954655431&amp;site=qq&amp;menu=yes, fa: fa-qq, color: 25c6fe} # 赞赏二维码（其中wechatSQ是赞赏单页面的赞赏码图片）【改】 donate: alipay: /img/custom/donate/AliPayQR.jpg wechat: /img/custom/donate/WeChanQR.jpg wechatSQ: /img/custom/donate/WeChanSQ.jpg # 首页视频地址为https://cdn.jsdelivr.net/gh/honjun/hojun@1.2/Unbroken.mp4，配置如下 【改】 movies: url: https://cdn.jsdelivr.net/gh/honjun/hojun@1.2 # 多个视频用逗号隔开，随机获取。支持的格式目前已知MP4,Flv。其他的可以试下，不保证有用 name: Unbroken.mp4 # 左下角aplayer播放器配置 主要改id和server这两项，修改详见[aplayer文档] 【改】 aplayer: id: 2660651585 server: netease type: playlist fixed: true mini: false autoplay: false loop: all order: random preload: auto volume: 0.7 mutex: true # Valine评论配置【改】 valine: true v_appId: GyC3NzMvd0hT9Yyd2hYIC0MN-gzGzoHsz v_appKey: mgOpfzbkHYqU92CV4IDlAUHQ 分类页和标签页配置分类页 标签页 配置项在\\themes\\Sakura\\languages\\zh-cn.yml里。新增一个分类或标签最好加下哦，当然嫌麻烦可以直接使用一张默认图片（可以改主题或者直接把404图片替换下，征求下意见要不要给这个在配置文件中加个开关，可以issue或群里提出来），现在是没设置的话会使用那种倒立小狗404哦。 #category # 按分类名创建 技术: #中文标题 zh: 野生技术协会 # 英文标题 en: Geek – Only for Love # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/coding.jpg 生活: zh: 生活 en: live img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/writing.jpg #tag # 标签名即是标题 悦读: # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/reading.jpg 单页面封面配置如留言板页面页面，位于source下的comment下，打开index.md如下： --- title: comment date: 2018-12-20 23:13:48 keywords: 留言板 description: comments: true # 在这里配置单页面头部图片，自定义替换哦~ photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/comment.jpg --- 单页面配置番组计划页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: bangumi title: bangumi comments: false date: 2019-02-10 21:32:48 keywords: description: bangumis: # 番组图片 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg # 番组名 title: 朝花夕誓——于离别之朝束起约定之花 # 追番状态 （追番ing/已追完） status: 已追完 # 追番进度 progress: 100 # 番剧日文名称 jp: さよならの朝に約束の花をかざろう # 放送时间 time: 放送时间: 2018-02-24 SUN. # 番剧介绍 desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg title: 朝花夕誓——于离别之朝束起约定之花 status: 已追完 progress: 50 jp: さよならの朝に約束の花をかざろう time: 放送时间: 2018-02-24 SUN. desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 --- 友链页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: links title: links # 创建日期，可以改下 date: 2018-12-19 23:11:06 # 图片上的标题，自定义修改 keywords: 友人帐 description: # true/false 开启/关闭评论 comments: true # 页面头部图片，自定义修改 photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/links.jpg # 友链配置 links: # 类型分组 - group: 个人项目 # 类型简介 desc: 充分说明这家伙是条咸鱼 &lt; (￣︶￣)&gt; items: # 友链链接 - url: https://shino.cc/fgvf # 友链头像 img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg # 友链站点名 name: Google # 友链介绍 下面雷同 desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 # 类型分组... - group: 小伙伴们 desc: 欢迎交换友链 ꉂ(ˊᗜˋ) items: - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 --- 写文章配置主题集成了个人插件hexo-tag-bili和hexo-tag-fancybox_img。其中hexo-tag-bili用来在文章或单页面中插入B站外链视频，使用语法如下： {% bili video_id [page] %} 详细使用教程详见hexo-tag-bili。 hexo-tag-fancybox_img用来在文章或单页面中图片，使用语法如下： {% fb_img src [caption] %} 详细使用教程详见hexo-tag-fancybox_img 还有啥，一时想不起来……To be continued…","categories":[{"name":"汇编","slug":"汇编","permalink":"http://ailous.top/categories/汇编/"}],"tags":[{"name":"xx","slug":"xx","permalink":"http://ailous.top/tags/xx/"}],"keywords":[{"name":"汇编","slug":"汇编","permalink":"http://ailous.top/categories/汇编/"}]},{"title":"人工智能——mnist（手写识别）","slug":"人工智能——mnist（手写识别）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-11T14:55:06.151Z","comments":true,"path":"2018/12/09/人工智能——mnist（手写识别）/","link":"","permalink":"http://ailous.top/2018/12/09/人工智能——mnist（手写识别）/","excerpt":"","text":"人工智能——mnist（手写识别）因为时间较久了，就不详细介绍了。简单的说，一开始对已有的大量数字图片进行训练生成模型。然后通过对一张手写的数字图片进行读取，进入模型匹配，输出结果。 源码# Python3 # 使用LeNet5的七层卷积神经网络用于MNIST手写数字识别 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;MNIST_data&quot;, one_hot=True) # 为输入图像和目标输出类别创建节点 x = tf.placeholder(tf.float32, shape=[None, 784]) # 训练所需数据 占位符 y_ = tf.placeholder(tf.float32, shape=[None, 10]) # 训练所需标签数据 占位符 # *************** 构建多层卷积网络 *************** # # 权重、偏置、卷积及池化操作初始化,以避免在建立模型的时候反复做初始化操作 def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) # 取随机值，符合均值为0，标准差stddev为0.1 return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) # x 的第一个参数为图片的数量，第二、三个参数分别为图片高度和宽度，第四个参数为图片通道数。 # W 的前两个参数为卷积核尺寸，第三个参数为图像通道数，第四个参数为卷积核数量 # strides为卷积步长，其第一、四个参数必须为1，因为卷积层的步长只对矩阵的长和宽有效 # padding表示卷积的形式，即是否考虑边界。&quot;SAME&quot;是考虑边界，不足的时候用0去填充周围，&quot;VALID&quot;则不考虑 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # x 参数的格式同tf.nn.conv2d中的x，ksize为池化层过滤器的尺度，strides为过滤器步长 def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) #把x更改为4维张量，第1维代表样本数量，第2维和第3维代表图像长宽， 第4维代表图像通道数 x_image = tf.reshape(x, [-1,28,28,1]) # -1表示任意数量的样本数,大小为28x28，深度为1的张量 # 第一层：卷积 W_conv1 = weight_variable([5, 5, 1, 32]) # 卷积在每个5x5的patch中算出32个特征。 b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # 第二层：池化 h_pool1 = max_pool_2x2(h_conv1) # 第三层：卷积 W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # 第四层：池化 h_pool2 = max_pool_2x2(h_conv2) # 第五层：全连接层 W_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) # 在输出层之前加入dropout以减少过拟合 keep_prob = tf.placeholder(&quot;float&quot;) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) # 第六层：全连接层 W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) # 第七层：输出层 y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) # *************** 训练和评估模型 *************** # # 为训练过程指定最小化误差用的损失函数，即目标类别和预测类别之间的交叉熵 cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv)) # 使用反向传播，利用优化器使损失函数最小化 train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # 检测我们的预测是否真实标签匹配(索引位置一样表示匹配) # tf.argmax(y_conv,dimension), 返回最大数值的下标 通常和tf.equal()一起使用，计算模型准确度 # dimension=0 按列找 dimension=1 按行找 correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) # 统计测试准确率， 将correct_prediction的布尔值转换为浮点数来代表对、错，并取平均值。 accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;)) saver = tf.train.Saver() # 定义saver # *************** 开始训练模型 *************** # with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(1000): batch = mnist.train.next_batch(50) if i%100 == 0: # 评估模型准确度，此阶段不使用Dropout train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0}) print(&quot;step %d, training accuracy %g&quot;%(i, train_accuracy)) # 训练模型，此阶段使用50%的Dropout train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) saver.save(sess, &#39;./save/model.ckpt&#39;) #模型储存位置 print(&quot;test accuracy %g&quot;%accuracy.eval(feed_dict={x: mnist.test.images [0:2000], y_: mnist.test.labels [0:2000], keep_prob: 1.0}))","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"手写识别","slug":"手写识别","permalink":"http://ailous.top/tags/手写识别/"},{"name":"mnist","slug":"mnist","permalink":"http://ailous.top/tags/mnist/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"算法——最长公共子序列","slug":"算法——最长公共子序列","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-11T14:55:10.701Z","comments":true,"path":"2018/12/09/算法——最长公共子序列/","link":"","permalink":"http://ailous.top/2018/12/09/算法——最长公共子序列/","excerpt":"","text":"算法——最长公共子序列链接：https://ac.nowcoder.com/acm/problem/19978 来源：牛客网 题目描述字符序列的子序列是指从给定字符序列中随意地（不一定连续）去掉若干个字符（可能一个也不去掉）后所形成的字符序列。令给定的字符序列X=“x0，x1，…，xm-1”，序列Y=“y0，y1，…，yk-1”是X的子序列，存在X的一个严格递增下标序列 &lt; i0，i1，…，ik-1 &gt; ，使得对所有的j=0，1，…，k-1，有xij = yj。例如，X=“ABCBDAB”，Y=“BCDB”是X的一个子序列。对给定的两个字符序列，求出他们最长的公共子序列长度，以及最长公共子序列个数。 输入描述:第1行为第1个字符序列，都是大写字母组成，以”.”结束。长度小于5000。第2行为第2个字符序列，都是大写字母组成，以”.”结束，长度小于5000。 输出描述:第1行输出上述两个最长公共子序列的长度。第2行输出所有可能出现的最长公共子序列个数，答案可能很大，只要将答案对100,000,000求余即可。 源码：#include&lt;cstdio&gt; #include&lt;cstring&gt; #define N 5010 #define mod 100000000 int f[2][N],g[2][N]; char A[N],B[N]; int main() { int n,m,i,j,d; scanf(&quot;%s%s&quot;,A+1,B+1); n=strlen(A+1)-1,m=strlen(B+1)-1; for(i=0;i&lt;=m;i++) f[0][i]=0,g[0][i]=1; for(d=i=1;i&lt;=n;i++,d^=1) { f[d][0]=0,g[d][0]=1; for(j=1;j&lt;=m;j++) { if(f[d^1][j]&gt;f[d][j-1]) f[d][j]=f[d^1][j], g[d][j]=g[d^1][j]; else if(f[d^1][j]&lt;f[d][j-1]) f[d][j]=f[d][j-1],g[d][j]=g[d][j-1]; else { f[d][j]=f[d^1][j],g[d][j]=(g[d^1][j]+g[d][j-1])%mod; if(f[d][j]==f[d^1][j-1]) g[d][j]=(g[d][j]-g[d^1][j-1]+mod)%mod; } if(A[i]==B[j]) { if(f[d^1][j-1]+1&gt;f[d][j]) f[d][j]=f[d^1][j-1]+1,g[d][j]=g[d^1][j-1]; else if(f[d^1][j-1]+1==f[d][j]) g[d][j]=(g[d][j]+g[d^1][j-1])%mod; } } } printf(&quot;%d\\n%d\\n&quot;,f[n&amp;1][m],g[n&amp;1][m]); return 0; }","categories":[{"name":"算法","slug":"算法","permalink":"http://ailous.top/categories/算法/"}],"tags":[{"name":"公共子序列","slug":"公共子序列","permalink":"http://ailous.top/tags/公共子序列/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://ailous.top/categories/算法/"}]},{"title":"网络爬虫——古诗文网中验证码（超级鹰）","slug":"网络爬虫——古诗文网中验证码（超级鹰）","date":"2018-12-09T05:47:40.000Z","updated":"2020-05-11T14:55:39.456Z","comments":true,"path":"2018/12/09/网络爬虫——古诗文网中验证码（超级鹰）/","link":"","permalink":"http://ailous.top/2018/12/09/网络爬虫——古诗文网中验证码（超级鹰）/","excerpt":"","text":"网络爬虫——古诗文网中验证码（超级鹰）目标网址: 古诗文网目标网址：https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx 任务要求：（1）通过selenium的方式模拟该网站的登录，并成功输入用户名和密码； （2）保存验证码图片，并使用输入式验证码识别的方式识别验证码的文字，获取后输入到输入框中， （3）验证登录是否成功。 源码：超级鹰源码：import requests from hashlib import md5 class Chaojiying_Client(object): def __init__(self, username, password, soft_id): self.username = username # todo:更改点一 self.password = md5(password.encode(&quot;utf-8&quot;)).hexdigest() self.soft_id = soft_id self.base_params = { &#39;user&#39;: self.username, &#39;pass2&#39;: self.password, &#39;softid&#39;: self.soft_id, } self.headers = { &#39;Connection&#39;: &#39;Keep-Alive&#39;, &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#39;, } def PostPic(self, im, codetype): params = { &#39;codetype&#39;: codetype, } params.update(self.base_params) files = {&#39;userfile&#39;: (&#39;ccc.jpg&#39;, im)} r = requests.post(&#39;http://upload.chaojiying.net/Upload/Processing.php&#39;, data=params, files=files, headers=self.headers) return r.json() def ReportError(self, im_id): &quot;&quot;&quot; im_id:报错题目的图片ID &quot;&quot;&quot; params = { &#39;id&#39;: im_id, } params.update(self.base_params) r = requests.post(&#39;http://upload.chaojiying.net/Upload/ReportError.php&#39;, data=params, headers=self.headers) return r.json() 识别源码：from selenium import webdriver from selenium.common.exceptions import TimeoutException , NoSuchElementException import time from PIL import Image import pytesseract import chaojiying browser = webdriver.Edge(&#39;E:\\\\anaconda\\\\Scripts\\\\msedgedriver.exe&#39;) # browser = webdriver.Chrome() try: browser.get(&#39;https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx&#39;) except TimeoutException: print(&#39;Time Out&#39;) try: username = browser.find_element_by_xpath(&#39;//*[@id=&quot;email&quot;]&#39;) username.send_keys(&#39;自己账号&#39;) time.sleep(1) password = browser.find_element_by_xpath(&#39;//*[@id=&quot;pwd&quot;]&#39;) password.send_keys(&#39;自己密码&#39;) time.sleep(1) pictureN = browser.find_element_by_xpath(&#39;//*[@id=&quot;imgCode&quot;]&#39;) browser.save_screenshot(&#39;login.png&#39;) loc = pictureN.location size = pictureN.size left = loc[&#39;x&#39;] top = loc[&#39;y&#39;] bottom = top+size[&#39;height&#39;] right = left+size[&#39;width&#39;] page = Image.open(&#39;login.png&#39;) Code = page.crop((left,top,right,bottom)) Code.save(&#39;code.png&#39;) chaojiying = Chaojiying_Client(&#39;超级鹰账号&#39;, &#39;密码&#39;, &#39;ID&#39;)#ID 具体看软件ID。 im = open(&#39;code.png&#39;, &#39;rb&#39;).read() text = chaojiying.PostPic(im,2004)[&#39;pic_str&#39;] print(text) # text = pytesseract.image_to_string(Image.open(&#39;code.png&#39;)) # print(text) CodeWhere = browser.find_element_by_xpath(&#39;//*[@id=&quot;code&quot;]&#39;) CodeWhere.send_keys(text) time.sleep(5) Submit = browser.find_element_by_xpath(&#39;//*[@id=&quot;denglu&quot;]&#39;) Submit.click() time.sleep(5) except NoSuchElementException: print(&#39;No Element&#39;) finally: browser.close()","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"古诗文网","slug":"古诗文网","permalink":"http://ailous.top/tags/古诗文网/"},{"name":"超级鹰","slug":"超级鹰","permalink":"http://ailous.top/tags/超级鹰/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"风格迁移——代码部分","slug":"卷积神经网络：（三）风格迁移——代码部分 (1)","date":"2018-10-29T05:47:40.000Z","updated":"2020-05-11T14:54:57.290Z","comments":true,"path":"2018/10/29/卷积神经网络：（三）风格迁移——代码部分 (1)/","link":"","permalink":"http://ailous.top/2018/10/29/卷积神经网络：（三）风格迁移——代码部分 (1)/","excerpt":"","text":"卷积神经网络：（三）风格迁移——代码部分引言本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。友情提示：风格迁移跑的时间会很长。有点耐心哦。若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，想知道原理，请点击https://blog.csdn.net/weixin_41108515/article/details/103650964&nbsp;转载请注明出处：https://blog.csdn.net/weixin_41108515/article/details/103651784&nbsp;这里引用的是：https://blog.csdn.net/aaronjny/article/details/79681080http://zh.gluon.ai/chapter_computer-vision/neural-style.html这两篇都非常详细，并且经调试可以使用，但是第二个并未使用tensorflow。以及理论帮助的一篇https://juejin.im/post/5d29e818e51d454f73356de0 第一部分 ：简介主要操作以这部分为主，这篇引用的是tensorflow练手项目三。可以通过点击查看，代码也是所有我调试过的里面较简洁的一个，能够实现基本功能。这里只是我添加了一些了解，以及操作步骤，便于新手理解。所谓风格迁移就是两张图片，你有你的风格，我有我的内容。你用你的油画风格将我的内容进行绘画一遍。这里使用的是style里面的图片（painting.jpg），数据集将存放在百度网盘。content文件选取的是qd.jpg，在content文件夹下。最终实现效果如下。 第二部分 ：操作1.获取模型VGG是Visual Geometry Group 这个实验室发明的，VGG是在2014年的 ILSVRC localization and classification 两个问题上分别取得了第一名和第二名的网络架构，是一个具有里程碑意义的CNN架构，其中最令人震惊的就是它的深度，这里使用的VGG19，有19层之多。VGG19包含了19个隐藏层（16个卷积层和3个全连接层）。VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。选择使用VGG是为了将深度卷积神经网络的训练从对数据集特征的一步步抽取的过程，从简单的特征，到复杂的特征的模式转为直接使用已经训练好的模型进行特征抽取。在imagenet数据集上训练好的模型上，直接抽取其他图像的特征，虽说这样的效果往往没有在新数据上重新训练的效果好，但能够节省大量的训练时间，在特定情况下非常有用。CNN在图片处理上表现良好，VGG19提出后，也被用在图像处理上。我这里要用到的VGG19模型就是在imagenet数据集上预训练的模型。注： 预训练好的VGG19模型可以从http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat下载，下载较慢的话，网盘https://pan.baidu.com/s/1uFinsEArbrgYRc2FWY9zVw：。 2.模型修改这里是指从预训练的VGG模型中，获取卷积层部分的参数，用于构建我们自己的模型。VGG19中的全连接层舍弃掉，这一部分对提取图像特征基本无用。VGG19模型中权重由ImageNet训练而来，全部是作为常量使用的，这些参数是不会再被训练的，在反向传播的过程中也不会改变。现在知道图片的内容表示和风格表示在卷积神经网络中是可分离的。也就是说，我们可以独立地操纵这两种表示来产生新的有感知意义上的图片。风格迁移图片是通过寻找一个同时匹配照片内容和对应的艺术风格的图片的方法而生成的。这些合成图片在保留原始照片的全局布置的同时，继承了各种艺术图片的不同艺术风格。风格表示是一个多层次的表达，包括了神经网络结构的多个层次。当风格表示只包含了少量的低层结构，（简单理解为训练模型次数少，模型特征不够强势）风格的就变得更加局部化，产生不同的视觉效果。当风格表示由网络的高层结构表示时，图像的结构会在更大的范围内和这种风格匹配（特征强势，会改变整个图的风格），产生别样的感觉。理论上简单理解了，开始操作。 这里建立py文件 models.py，下面内容我会写在注释里。# 导入必须的包 import tensorflow as tf import numpy as np import settings import scipy.io import scipy.misc class Model(object): def __init__(self, content_path, style_path): self.content = self.loadimg(content_path) # 加载内容图片 self.style = self.loadimg(style_path) # 加载风格图片 self.random_img = self.get_random_img() # 生成噪音内容图片 self.net = self.vggnet() # 建立vgg网络 def vggnet(self): # 读取预训练的vgg模型 # 这里装的是misc，安装opencv的也亦可以使用opencv等其他方法 vgg = scipy.io.loadmat(settings.VGG_MODEL_PATH) vgg_layers = vgg[&#39;layers&#39;][0] net = {} # 使用预训练的模型参数构建vgg网络的卷积层和池化层 # 全连接层不需要 # 注意，除了input之外，这里参数都为常量，不训练vgg的参数（权重比），这个以及训练完不需调整。 # 需要进行训练的是input，它即是我们最终生成的图像 net[&#39;input&#39;] = tf.Variable(np.zeros([1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3]), dtype=tf.float32) # 参数对应的层数可以参考vgg模型图 net[&#39;conv1_1&#39;] = self.conv_relu(net[&#39;input&#39;], self.get_wb(vgg_layers, 0)) net[&#39;conv1_2&#39;] = self.conv_relu(net[&#39;conv1_1&#39;], self.get_wb(vgg_layers, 2)) net[&#39;pool1&#39;] = self.pool(net[&#39;conv1_2&#39;]) net[&#39;conv2_1&#39;] = self.conv_relu(net[&#39;pool1&#39;], self.get_wb(vgg_layers, 5)) net[&#39;conv2_2&#39;] = self.conv_relu(net[&#39;conv2_1&#39;], self.get_wb(vgg_layers, 7)) net[&#39;pool2&#39;] = self.pool(net[&#39;conv2_2&#39;]) net[&#39;conv3_1&#39;] = self.conv_relu(net[&#39;pool2&#39;], self.get_wb(vgg_layers, 10)) net[&#39;conv3_2&#39;] = self.conv_relu(net[&#39;conv3_1&#39;], self.get_wb(vgg_layers, 12)) net[&#39;conv3_3&#39;] = self.conv_relu(net[&#39;conv3_2&#39;], self.get_wb(vgg_layers, 14)) net[&#39;conv3_4&#39;] = self.conv_relu(net[&#39;conv3_3&#39;], self.get_wb(vgg_layers, 16)) net[&#39;pool3&#39;] = self.pool(net[&#39;conv3_4&#39;]) net[&#39;conv4_1&#39;] = self.conv_relu(net[&#39;pool3&#39;], self.get_wb(vgg_layers, 19)) net[&#39;conv4_2&#39;] = self.conv_relu(net[&#39;conv4_1&#39;], self.get_wb(vgg_layers, 21)) net[&#39;conv4_3&#39;] = self.conv_relu(net[&#39;conv4_2&#39;], self.get_wb(vgg_layers, 23)) net[&#39;conv4_4&#39;] = self.conv_relu(net[&#39;conv4_3&#39;], self.get_wb(vgg_layers, 25)) net[&#39;pool4&#39;] = self.pool(net[&#39;conv4_4&#39;]) net[&#39;conv5_1&#39;] = self.conv_relu(net[&#39;pool4&#39;], self.get_wb(vgg_layers, 28)) net[&#39;conv5_2&#39;] = self.conv_relu(net[&#39;conv5_1&#39;], self.get_wb(vgg_layers, 30)) net[&#39;conv5_3&#39;] = self.conv_relu(net[&#39;conv5_2&#39;], self.get_wb(vgg_layers, 32)) net[&#39;conv5_4&#39;] = self.conv_relu(net[&#39;conv5_3&#39;], self.get_wb(vgg_layers, 34)) net[&#39;pool5&#39;] = self.pool(net[&#39;conv5_4&#39;]) return net def conv_relu(self, input, wb): &quot;&quot;&quot; 进行先卷积、后relu的运算 :param input: 输入层 :param wb: wb[0],wb[1] == w,b :return: relu后的结果 &quot;&quot;&quot; conv = tf.nn.conv2d(input, wb[0], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) relu = tf.nn.relu(conv + wb[1]) return relu def pool(self, input): &quot;&quot;&quot; 进行max_pool操作 :param input: 输入层 :return: 池化后的结果 &quot;&quot;&quot; return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) def get_wb(self, layers, i): &quot;&quot;&quot; 从预训练好的vgg模型中读取参数 :param layers: 训练好的vgg模型 :param i: vgg指定层数 :return: 该层的w,b &quot;&quot;&quot; w = tf.constant(layers[i][0][0][0][0][0]) bias = layers[i][0][0][0][0][1] b = tf.constant(np.reshape(bias, (bias.size))) return w, b def get_random_img(self): &quot;&quot;&quot; 根据噪音和内容图片，生成一张随机图片 :return: &quot;&quot;&quot; noise_image = np.random.uniform(-20, 20, [1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3]) random_img = noise_image * settings.NOISE + self.content * (1 - settings.NOISE) return random_img def loadimg(self, path): &quot;&quot;&quot; 加载一张图片，将其转化为符合要求的格式 :param path: :return: &quot;&quot;&quot; # 读取图片 image = scipy.misc.imread(path) # 重新设定图片大小 image = scipy.misc.imresize(image, [settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH]) # 改变数组形状，其实就是把它变成一个batch_size=1的batch image = np.reshape(image, (1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH, 3)) # 减去均值，使其数据分布接近0 image = image - settings.IMAGE_MEAN_VALUE return image if __name__ == &#39;__main__&#39;: Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE) 3.模型训练但是实际上，图片的内容和风格是不能够被完全分离的。当我们合成图片时，我们通常找不出一张能够匹配某个图片内容和另一种图片风格的图片。在我们合成图片的过程中，我们需要最小化的损失函数包含内容和风格，但它们是分开的。因此，我们需要平滑地调整内容和风格的权重比例。当损失函数分配在内容和风格的权重不同时，合成产生的图片效果也完全不一样。我们需要适当地调整内容表示和风格表示的权重比来产生具有视觉感染力的图片。是否能够找到合适的权重比是能否产生令人满意的图片的关键因素。就是将输入层的Variable训练到满意的比例，最开始输入一张噪音图片，然后不断地根据内容loss和风格loss对其进行调整，直到一定次数后，该图片兼具了风格图片的风格以及内容图片的内容。当训练结束时，输入层的参数就是我们生成的图片。这里建立py文件 train.py，下面内容我会写在注释里。# -*- coding: utf-8 -*- import tensorflow as tf import settings import models import numpy as np import scipy.misc def loss(sess, model): &quot;&quot;&quot; 定义模型的损失函数 :param sess: tf session :param model: 神经网络模型 :return: 内容损失和风格损失的加权和损失 &quot;&quot;&quot; # 先计算内容损失函数 # 获取定义内容损失的vgg层名称列表及权重 content_layers = settings.CONTENT_LOSS_LAYERS # 将内容图片作为输入，方便后面提取内容图片在各层中的特征矩阵 sess.run(tf.assign(model.net[&#39;input&#39;], model.content)) # 内容损失累加量 content_loss = 0.0 # 逐个取出衡量内容损失的vgg层名称及对应权重 for layer_name, weight in content_layers: # 提取内容图片在layer_name层中的特征矩阵 p = sess.run(model.net[layer_name]) # 提取噪音图片在layer_name层中的特征矩阵 x = model.net[layer_name] # 长x宽 M = p.shape[1] * p.shape[2] # 信道数 N = p.shape[3] # 根据公式计算损失，并进行累加 content_loss += (1.0 / (2 * M * N)) * tf.reduce_sum(tf.pow(p - x, 2)) * weight # 将损失对层数取平均 content_loss /= len(content_layers) # 再计算风格损失函数 style_layers = settings.STYLE_LOSS_LAYERS # 将风格图片作为输入，方便后面提取风格图片在各层中的特征矩阵 sess.run(tf.assign(model.net[&#39;input&#39;], model.style)) # 风格损失累加量 style_loss = 0.0 # 逐个取出衡量风格损失的vgg层名称及对应权重 for layer_name, weight in style_layers: # 提取风格图片在layer_name层中的特征矩阵 a = sess.run(model.net[layer_name]) # 提取噪音图片在layer_name层中的特征矩阵 x = model.net[layer_name] # 长x宽 M = a.shape[1] * a.shape[2] # 信道数 N = a.shape[3] # 求风格图片特征的gram矩阵 A = gram(a, M, N) # 求噪音图片特征的gram矩阵 G = gram(x, M, N) # 根据公式计算损失，并进行累加 style_loss += (1.0 / (4 * M * M * N * N)) * tf.reduce_sum(tf.pow(G - A, 2)) * weight # 将损失对层数取平均 style_loss /= len(style_layers) # 将内容损失和风格损失加权求和，构成总损失函数 loss = settings.ALPHA * content_loss + settings.BETA * style_loss return loss def gram(x, size, deep): &quot;&quot;&quot; 创建给定矩阵的格莱姆矩阵，用来衡量风格 :param x:给定矩阵 :param size:矩阵的行数与列数的乘积 :param deep:矩阵信道数 :return:格莱姆矩阵 &quot;&quot;&quot; # 改变shape为（size,deep） x = tf.reshape(x, (size, deep)) # 求xTx g = tf.matmul(tf.transpose(x), x) return g def train(): # 创建一个模型 model = models.Model(settings.CONTENT_IMAGE, settings.STYLE_IMAGE) # 创建session with tf.Session() as sess: # 全局初始化 sess.run(tf.global_variables_initializer()) # 定义损失函数 cost = loss(sess, model) # 创建优化器 optimizer = tf.train.AdamOptimizer(1.0).minimize(cost) # 再初始化一次（主要针对于第一次初始化后又定义的运算，不然可能会报错） sess.run(tf.global_variables_initializer()) # 使用噪声图片进行训练 sess.run(tf.assign(model.net[&#39;input&#39;], model.random_img)) # 迭代指定次数 for step in range(settings.TRAIN_STEPS): # 进行一次反向传播 sess.run(optimizer) # 每隔一定次数，输出一下进度，并保存当前训练结果 if step % 50 == 0: print(&#39;step {} is down.&#39;.format(step)) # 取出input的内容，这是生成的图片 img = sess.run(model.net[&#39;input&#39;]) # 训练过程是减去均值的，这里要加上 img += settings.IMAGE_MEAN_VALUE # 这里是一个batch_size=1的batch，所以img[0]才是图片内容 img = img[0] # 将像素值限定在0-255，并转为整型 img = np.clip(img, 0, 255).astype(np.uint8) # 保存图片 scipy.misc.imsave(&#39;{}-{}.jpg&#39;.format(settings.OUTPUT_IMAGE,step), img) # 保存最终训练结果 img = sess.run(model.net[&#39;input&#39;]) img += settings.IMAGE_MEAN_VALUE img = img[0] img = np.clip(img, 0, 255).astype(np.uint8) scipy.misc.imsave(&#39;{}.jpg&#39;.format(settings.OUTPUT_IMAGE), img) if __name__ == &#39;__main__&#39;: train() 4.系统文件配置这里建立py文件 setting.py。# -*- coding: utf-8 -*- # 内容图片路径 CONTENT_IMAGE = &#39;content/qd.jpg&#39; # 路径/图片 自己在工程文件夹下建立。 # 风格图片路径 STYLE_IMAGE = &#39;style/painting.jpg&#39; # 路径/图片 自己在工程文件夹下建立。 # 输出图片路径 OUTPUT_IMAGE = &#39;output/output&#39; # 路径/图片开始名 自己在工程文件夹下建立。 # 预训练的vgg模型路径 VGG_MODEL_PATH = &#39;imagenet-vgg-verydeep-19.mat&#39; # 直接置于工程文件夹即可。 # 图片宽度 IMAGE_WIDTH = 450 # 图片高度 IMAGE_HEIGHT = 300 # 定义计算内容损失的vgg层名称及对应权重的列表 CONTENT_LOSS_LAYERS = [(&#39;conv4_2&#39;, 0.5),(&#39;conv5_2&#39;,0.5)] # 定义计算风格损失的vgg层名称及对应权重的列表 STYLE_LOSS_LAYERS = [(&#39;conv1_1&#39;, 0.2), (&#39;conv2_1&#39;, 0.2), (&#39;conv3_1&#39;, 0.2), (&#39;conv4_1&#39;, 0.2), (&#39;conv5_1&#39;, 0.2)] # 噪音比率 NOISE = 0.5 # 图片RGB均值 IMAGE_MEAN_VALUE = [128.0, 128.0, 128.0] # 内容损失权重 ALPHA = 1 # 风格损失权重 BETA = 500 # 训练次数 TRAIN_STEPS = 3000 # 这里推荐几百次就行，确实时间太长。 5.总结：这是第一次写博客，有不正确的地方还望指点，做出来的效果还是一般。之前做过网页版，但是因为跑的时间太长，效果不好。后期有时间出一篇网页版的风格迁移。源码数据","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://ailous.top/tags/卷积神经网络/"},{"name":"风格迁移","slug":"风格迁移","permalink":"http://ailous.top/tags/风格迁移/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"风格迁移——原理部分","slug":"卷积神经网络：（二）风格迁移——原理部分","date":"2018-10-28T05:47:40.000Z","updated":"2020-05-11T14:54:51.694Z","comments":true,"path":"2018/10/28/卷积神经网络：（二）风格迁移——原理部分/","link":"","permalink":"http://ailous.top/2018/10/28/卷积神经网络：（二）风格迁移——原理部分/","excerpt":"","text":"卷积神经网络：（二）风格迁移——原理部分引言本文是在第一步配置完环境后基础上运行的。使用的为系统直装的python环境（在anaconda环境下一样适用，后面注意的点会提示的。）。若想查看环境配置步骤，请点击https://blog.csdn.net/weixin_41108515/article/details/103636284，因原理部分篇幅较多，所以将所有代码，知识部分移到第三篇博客上，若是对于该原理了解熟悉，或只想操作不需深入的，可以直接跳过。所有操作都在第三篇上。&nbsp;转载请注明出处：https://blog.csdn.net/weixin_41108515/article/details/103650964&nbsp;这里引用的是：http://zh.gluon.ai/chapter_computer-vision/neural-style.htmlhttps://blog.csdn.net/aaronjny/article/details/79681080这两篇都非常详细，并且经调试可以使用。 涉及到的相关原理：1、神经网络部分原理：1.1 神经网络基础介绍神经网络基本可以分成两种：一种为生物神经网络，一种为人工神经网络。生物神经网络一般是指生物的大脑神经元，细胞，触点等组成的网络，用于产生生物的意识，帮助生物进行思考和行动。其主要是由生物神经元构成，如下图所示。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。主要的研究工作集中在以下几个方面：（1）生物原型研究。从生理学、心理学、解剖学、脑科学、病理学等生物科学方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。（2）建立理论模型。根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。（3）网络模型与算法研究。在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。（4）人工神经网络应用系统。在网络模型与算法研究的基础上，利用人工神经网络组成实际的应用系统，例如，完成某种信号处理或模式识别的功能、构造专家系统、制成机器人等等。纵观当代新兴科学技术的发展历史，人类在征服宇宙空间、基本粒子，生命起源等科学技术领域的进程中历经了崎岖不平的道路。我们也会看到，探索人脑功能和神经网络的研究将伴随着重重困难的克服而日新月异。人工神经网络如下图所示。1.2 卷积神经网络基本结构卷积神经网络(Convolutional Neural Network，CNN)是一种前馈神经网络，它的人工神经元可以对周围单元的一部分进行响应，并能很好的处理大型的图片。卷积神经网络是近几年来发展起来的一种高效的识别方法，并引起了广泛的关注[16]。正是由于高效的识别准确率，对卷积神经网络的研究才层出不穷。20世纪60年代，胡贝尔和魏塞尔发现，独特的网络结构可以有效地减少反馈神经网络在大脑皮层神经元研究中的局部敏感性和方向性选择的复杂性，从而提出了卷积神经网络(Convolutional Neural Networks简称CNN)的概念。目前，卷积神经网络已成为许多科学领域的热点话题。由于内部算法避免了对图像进行复杂的预处理，所以它可以直接输入原始图片。1.2.1 输入层卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。1.2.2 隐含层1.卷积层（1）卷积层利用乘法卷积代替矩阵乘法。在图像处理的过程中，一张“小猫”的图片可以被看作是一个“薄饼”，它包括图片的高度、宽度和深度(即颜色的三原色，以RGB表示)。如上图所示，若权重不变，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度、深度都不同的新图像。得到的新图像如下图所示。 （2）卷积层参数卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充。填充依据其层数和目的可分为四类：有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积”，窄卷积输出的特征图尺寸为(L-f)/s+1。相同填充/半填充：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积。全填充：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积”。任意填充：介于有效填充和全填充之间，人为设定的填充，较少使用。（3）激励函数一个合适的激励函数可以有效地提高卷积神经网络的运行性能。激活函数应当具有的性质：1）可微性：当优化方法是基于梯度的时候，这个性质是必不可少的。2）单调性：当激活函数为单调函数时，能够确保单层网络为凸函数。3）输出值的范围：当激活函数的输出值受到限制时，基于梯度的优化方法将更加稳定，因为特征的表示更受有限权重的影响。当激活函数的输出是无限时，模型的训练将更加油效率，但在这种情形下，通常需要较小的学习速率。经常使用的非线性激活函数有sigmid、tanh、Relu等等，前两者sigmid与tanh在全连接层 比较常见，后者ReLU常见于卷积层。2.池化层池化层是卷积神经网络的一个重要组成部分，它通过减少卷积层之间的连接数量来降低计算的困难度。包括以下几种池化：（1）一般池化(General Pooling)1）mean-pooling，即只要求邻域中特征点的平均值；2）max-pooling，即在邻域中提取最大特征点；3）Stochastic-pooling，介于两者之间，根据数值给出像素的概率，并根据概率进行二次采样。特征提取的误差主要来自两个方面：1）邻域大小受限造成的估计值方差增大；2）卷积层参数误差导致估计均值的偏移。一般来说，mean-pooling能减小第一种误差并保留图像的背景信息，max-pooling能减小第二类的错误，并保留更多的纹理信息。在平均意义上，与mean-pooling近似，在局部意义上，则服从max-pooling的准则。如图下图所示， （2）空间金字塔池化(Spatial pyramid pooling)一般的卷积神将网络都需要输入图像的大小是固定的，这是因为全连接层的输入需要一个固定的维度。几乎所有作者提出了空间金字塔池化，先让图像进行卷积，然后变换为要输入到全连接层的维度，这样可以把卷积神经网络扩展到任意大小的图像。空间金字塔池化可以把任何尺度的卷积特征转化成同一维，这不仅可以让卷积神经网络处理任意大小的图像，还能避免裁剪和扭曲操作，这具有重要意义。3.全连接层卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去三维结构，被展开为向量并通过激励函数传递至下一层。1.2.3 输出层卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。1.3 卷积神经网络的卷积过程卷积神经网络的结构有很多种，但是其基本结构是类似的。如下图，它包含三个主要的层——卷积层(convolutional layer)、池化层(pooling layer)、全连接层(fully-connected layer)。 图中的卷积网络工作流程如下， 输入图片是像素是32×32的来组成输入层。然后，计算流程在卷积和抽样之间交替进行，如下所述：第一隐藏层进行卷积的工作，它由6个特征图组成，每个特征图由28×28个神经元组成，每个神经元指定5×5 的接受域。第二隐藏层实现子采样和局部平均，它同样由 6个特征图组成，但其每个特征图由14×14 个神经元组成。每个神经元具有2×2 的接受域。第三隐藏层进行第二次卷积，它由 16个特征图组成，每个特征图由 10×10个神经元组成。隐藏层中的每个神经元可以具有与下一隐藏层的多个特征图相关联的突触连接，其操作方式类似于第一层隐藏层的卷积过程。第四个隐藏层进行第二次子采样和局部平均计算。它由 16个特征图组成，但每个特征图由 5×5个神经元构成，它以与第一次采样相同的方式进行工作。第五个隐藏层实现了卷积的最后阶段，它由 120个神经元组成，每个神经元指定5×5 的接受域。端部是个全连接层，得到输出向量。卷积和采样之间的计算层的连续交替是“双尖塔”的结果，即在每个卷积或采样层中，与先前的层相比，特征图的数目随着空间分辨率的减小而增加[17]。卷积层研究输入数据的特征。卷积层由卷积核(convolutional kernel)组成，卷积核用来计算不同的特征图；激励函数(activation function)给卷积神经网络引入了非线性，常用的有sigmid、tanh、 ReLU函数；池化层减少了卷积层输出的特征向量，改良结果(使结构不易过拟合)，典型应用有average pooling 和 max pooling；全连接层将卷积层和池化层组合起来以后，然后可以形成层或多层全连接层，从而可以完成更高水平的特征取得。2、迁移学习相关原理2.1 迁移学习在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力，所以一般情况下来说，问题B的数据量是较少的。所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够很好的完成特征提取任务。迁移学习具有如下优势：更短的训练时间，更快的收敛速度，更精准的权重参数。但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的到，但是此时起码可以将底层的权重参数作为初始值来重新训练。2.2TensorFlowTensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据传递到人工智能神经网络进行处理和分析的系统。TensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从手机、单个CPU / GPU到成百上千GPU卡组成的分布式系。TensorFlow支持CNN、RNN和LSTM算法，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。TensorFlow可被用于像机器学习和深度学习的许多领域，如语音识别或者是图像处理，以及对深度学习的基础设施的各个方面进行改进。它能够运行在小到一个只能电话或数以百万计的CEN上。TensorFlow将是完全开源的，可以被任何人使用。这也是选择TensorFlow这个平台的主要原因。（1）支持多种硬件的平台例如，它支持CPU、GPU混合数据中心的训练平台，并且还支持数据中心的训练模型，它相对方便地部署到不同的移动端应用程序，并且可以支持由谷歌自主开发的TPU处理器。这种多硬件支持平台会大大给用户带来方便。（2）支持多种开发环境支持各种硬件的平台是基础，也是TensorFlow始终能够帮助尽可能多的开发人员利用深度学习技术并最终受益于广大用户的原因。基于这一思想，TensorFlow一直都非常重视各种程序员开发环境。例如，开发人员可以在各式各样的、位于主要的位置的开发环境中使用TensorFlow环境。目前TensorFlow仍处于快速开发迭代中，有大量新功能及性能优化在次持续研发。2.3VGG卷积神经网络模型VGG全称是Visual Geometry Group属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16～VGG19[20]。VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为(GG-Very-Deep-16 CNN)，VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。VGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下，下图是VGG11 ～GVV19的结构图： 在图中，A列是最基本的模型，有8个卷积层，3个全连接层，一共11层。B列是在A列的基础上，在stage1和stage2基础上分别增加了一层33卷积层，一共13层。C列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层11的卷积层。一共16层。D列是在B的基础上，在stage3,stage4和stage5基础上分别增加了一层33的卷积层，一共16层。E层是在D的基础上，在stage3,stage4和stage5基础上分别增加33的卷积层，一共19层。模型E就是VGG19网络。3、通过VGG实现风格迁移3.1 图像风格迁移的原理VGGNET是一种图像识别模型，它也拥有者卷积层和全连接层。可以这样理解VGG的结构：前面的卷积层是从图像中提取“特征”，而后面的全连接层把图片的“特征”转换为类别概率。其中VGGNET中的浅层（conv1_1,conv1_2 ），提取的特征往往是比较简单的（比如提取检测点、线、亮度），VGGNET中的深层（c比如onv5_1,conv5_2），提取的特征往往比较复杂（如是否存在人脸、某种特定物体）。VGGNET的本意是输入图像，提取特征，然后输出图像类别。图像风格迁移恰好与其相反，输入特征，之后输出对应这种特征的图片。两种过程的对比图片如下图所示： 具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。具体过程就是：先选取一副原始图像，经过VGGNET计算后得到各个卷积层的特征。之后，根据这些卷积层的特征，还原出对应这种特征的原始图像.研究发现：浅层的还原效果往往比较好，卷积特征基本保留了所有原始图像中形状、位置、颜色、纹理等信息；深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。3.2 代价函数要构建一个神经风格迁移系统，首先需要为生成的图像定义一个代价函数，通过最小化代价函数，可以大大缩短图片生成所需要的时间。为了实现神经风格迁移，需要定义一个关于G的代价函数，J用来评判某个生成图像的好坏，我们将使用梯度下降法去最小化J(G)，以便于生成这个图像。那么如何去判断生成图像的好坏，在这里把这个代价函数定义为两个部分。Jcontent(C,G)第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片G的内容与内容图片C的内容有多相似。Jstyle(S,G)然后会把结果加上一个风格代价函数，也就是关于S和G的函数，用来度量图片G的风格和图片S的风格的相似度。J(G)=αJcontent(C,G)+βJstyle(S,G)最后用两个超参数α和β来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。3.3 内容代价函数风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。首先定义内容代价部分。用隐含层m来计算内容代价，如果m是个很小的数，比如用隐含层 1，这个代价函数就会使生成图片像素上非常接近内容图片。然而如果使用很深的层，那么可能在内容图片里面有一个具体的物体，在生成的图片里就会存在这个物体。比如是一只小猫，那么在生成的图片里就一定会有一个小猫。所以在实际中，这个层m在网络中既不会选的太浅也不会选的太深。通常𝑚会选择在网络的中间层，既不太浅也不很深，然后用一个预训练的卷积模型，本篇论文使用的是 VGG 网络。之后需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度，令这个代表这两个图片α^([L][C])和α^([L][G])的l层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似。 为两个激活值不同或者相似的程度，取l层的隐含单元的激活值，按元素相减，内容图片的激活值与生成图片相比较，然后取平方，也可以在前面加上归一化或者不加，比如1/2或者其他的，都影响不大，因为这都可以由这个超参数α来调整。3.4 风格代价函数图像的风格可以用使用图像的卷积层特征的Gram矩阵来进行表示。在线性代数中这种矩阵被称为Gram矩阵，在这里可以称之为风格矩阵。风格矩阵是一组向量的内积对称矩阵，比如向量组的Gram矩阵是 取内积即欧几里得空间上的标准内积，即 假设卷积层的输出为F_ij^l，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为 设在第l层中，卷积特征的通道数为N_l,卷积的高、宽乘积数为M_l,那么F_ij^l满足l≤i≤N_l，l≤j≤M_lGram矩阵在一定程度上可以体现图片的风格。多层的风格损失是单层风格损失的加权累加。3.5 模型训练过程首先，使用VGG中的一些层的输出来表示图片的内容特征和风格特征。使用[‘conv4_2’,’conv5_2’]表示内容特征，使用[‘conv1_1’,’conv2_1’,’conv3_1’,’conv4_1’]表示风格特征。将内容图片输入网络，计算内容图片在网络指定层上的输出值。计算内容损失。可以这样定义内容损失：内容图片在指定层上提取出的特征矩阵，与噪声图片在对应层上的特征矩阵的差值的L2范数。即求两两之间的像素差值的平方。对应每一层的内容损失函数： 其中，X是噪声图片的特征矩阵，P是内容图片的特征矩阵。M是P的长*宽，N是信道数。最终的内容损失为，每一层的内容损失加权和，再对层数取平均。将风格图片输入网络，计算风格图片在网络指定层上的输出值。计算风格损失。使用风格图像在指定层上的特征矩阵的GRAM矩阵来衡量其风格，风格损失可以定义为风格图像和噪音图像特征矩阵的格莱姆矩阵的差值的L2范数。对于每一层的风格损失函数： 其中M是特征矩阵的长*宽，N是特征矩阵的信道数。G为噪音图像特征的Gram矩阵，A为风格图片特征的GRAM矩阵。最终的风格损失为，每一层的风格损失加权和，再对层数取平均。函数为内容损失和风格损失的加权和： 当训练开始时，根据内容图片和噪声，生成一张噪声图片。并将噪声图片传送给网络，计算loss，再根据loss调整噪声图片。将调整后的图片发给网络，重新计算loss，再调整，再计算，直到达到指定迭代次数，这时，噪声图片已兼具内容图片的内容和风格图片的风格，进行保存即可，其训练过程如图下所示，训练顺序依次从左向右。 原理总结：感谢能翻到这的同学们，这一篇只是为了让大家了解到一些相关知识，毕竟操作简单，主要的是算法思想。下一篇就是与代码相关的部分了，可以开始打开建的工程，写代码了！卷积神经网络：（三）风格迁移——代码部分","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://ailous.top/tags/卷积神经网络/"},{"name":"风格迁移","slug":"风格迁移","permalink":"http://ailous.top/tags/风格迁移/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"风格迁移——环境配置","slug":"卷积神经网络：（一）风格迁移——环境配置 (1)","date":"2018-10-27T05:47:40.000Z","updated":"2020-05-11T14:55:01.052Z","comments":true,"path":"2018/10/27/卷积神经网络：（一）风格迁移——环境配置 (1)/","link":"","permalink":"http://ailous.top/2018/10/27/卷积神经网络：（一）风格迁移——环境配置 (1)/","excerpt":"","text":"卷积神经网络：（一）风格迁移——环境配置引言&emsp;&emsp;本文主要在windows环境下搭建python环境，用python从零入手搭建一个简单的风格迁移模型。若为macos，linux可以参考其他博客搭建环境，再搭建该模型。&nbsp;转载请注明出处：https://blog.csdn.net/weixin_41108515/article/details/103636284&nbsp; 第一步：搭建python环境：方法一 : 直装python环境（后期主要使用这一环境，方便一些不熟悉anaconda的同学）&emsp;&emsp;直接使用电脑默认环境，即电脑直接安装python环境，（这里推荐使用python3.6版本，3.7及以上版本目前不支持）下载地址：https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe&emsp;&emsp;如图示安装install Now即可,注意添加path以及安装路径，后期使用pycharm找编译环境会使用到。注：默认路径C:\\users\\用户名\\Appdata\\local\\Programs\\Python\\Python36 也可以使用Customize installation来更换路径。安装完成后，进入cmd界面，输入python，如图 出现此界面即可。&nbsp; 方法二 ：使用anaconda搭建环境:&emsp;&emsp;因为anaconda官方网站下下载较慢，这里推荐使用 清华镜像来下载。注：anaconda官方当前最新版为基于python3.7version，这个并不意味着搭建的python环境版本就固定为3.7，但是这里还是不推荐使用下面会说明原因。清华镜像下载地址https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Windows-x86_64.exe如图所示安装anaconda，这里先用的是官方3.7的包：注：存在问题：1、下载太慢。2、3.7—version需额外对环境更改，这里只对3.7介绍安装步骤，不提供修改，清华镜像无问题安装步骤相同。安装步骤：&nbsp; 目录还是自己选取也可以默认，但是必须记住便于后期编译环境选择。 这里是指是否将anaconda里的python作为电脑默认python环境。&emsp;&emsp;安装完成后，选择Anaconda Navigator 打开，接下来选择Environments -&gt;create，:生成如下界面： &emsp;&emsp;这里是3.7版本的问题，创建新环境并不会出现其他python版本，还需要配置其他信息，这里不再赘述，使用清华镜像安装后如下，即为正确完成安装： &emsp;&emsp;接下来开始搭建环境，给环境声明一个Name，这里叫做tensorflowWork。packages选择python3.6。第一步，总结&emsp;&emsp;anaconda里的默认root其实就是可以直接使用的，但是为了便于后面操作进行以及使用理解才添加新的环境。无论是方法一的安装在默认环境下，还是方法二安装在anaconda环境下，这两者并不冲突，只不过是运行程序时，是想用哪一个作为编译环境罢了。就像你要到一个地方去，修了两条路，这两条路都能到达，你要运行这个程序你可以走这条路，也可以走那条，他们相互独立并不会相互影响。而导包相当于你想走这一条路，但是这一条路有点窄，你的代码走不过去，为了能够让你的代码过去，你需要给这个路拓宽，这个包就是扩宽的材料，对应的代码对应对应的包。&nbsp; 第二步，安装pycharm这个在pycharm官网上下载即可,如图示安装即可。 安装完成。&nbsp; 第三步，安装必需包&emsp;&emsp;这里安装的是tensorflow和opencv，pil包，不一定全部用到，因为这里推荐的几个环境各有不同，所以全部安装上，其他包通过pycharm 中的【ALT+SHIFT+ENTER】即可安装，如还有缺失请自行百度。方法一：直装python环境&emsp;&emsp;默认环境下只需进入cmd界面，输入命令即可，若是安装直装python环境又安装tensorflow的，则需进入cmd输入python：若为下图：第二行中出现anaconda。则需修改默认python环境。若为下图，第二行中不是anaconda，则继续操作即可。3.1.1 安装tensorflow包&emsp;&emsp;tensorflow可以在系统CPU和GPU上执行，AVX2和CUDA两种，这里推荐在github：https://github.com/fo40225/t与ensorflow-windows-wheel上下载，对于不同版本的python环境有不同的whl文件可以下载。https://github.com/fo40225/tensorflow-windows-wheel/raw/master/1.4.0/py36/CPU/avx2/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl&emsp;&emsp;下载完成后，找到下载位置点击界面，按住shift右击鼠标右键，选择在此处打开Powershell窗口。输入 ： pip install .\\tensorflow-1.4.0-cp36-cp36m-win_amd64.whl 运行完即可，若是提示pip版本有更新，更不更新都可。python -m pip install --upgrade pip 3.1.2 安装opencv包opencv在代码中使用时：import cv2opencv较tensorflow简单，只需输入代码即可运行：pip install opencv-python 3.1.2 安装PIL（Python Imaging Library）包PIL安装并不是通过直接键入pip install PIL 而是通过：pip install pillow 方法一总结：&emsp;&emsp;这里使用的Powershell窗口和cmd界面使用方法相同，此处命令在cmd执行亦可。&nbsp; 方法二：anaconda环境&emsp;&emsp;这里使用之前第一步在anaconda下搭建的Name为tensorflowWork环境，这三者方法一致，这里只举tensorflow的例，opencv和PIL步骤相同。点击上方的选择框设置为all，然后进行搜索tensorflw选择tensorflow，点击apply &emsp;&emsp;会生成一系列的包名，apply即可。随后进行安装，等待完成。其余各包同样操作，这里不赘述了。&nbsp; 第四步，建立风格迁移工程在pycharm中选择：file-&gt;New project &emsp;&emsp;这里要注意的是Base interpreter ，一般默认的是系统下的默认python，即直装的方法下默认为直装的那一个，若是装了anaconda环境则需选择默认环境，就是之前要选择另一条路的问题。方法一：在创建项目是直接选择anaconda下的python： 方法二：在项目创建完成后再选择路径为：File-&gt;Settings-&gt;project：项目名，如图点击齿轮——Add。 在Base interpreter下选择anaconda中python路径即可。 至此所有环境配置完毕。&nbsp; 第五步，写风格迁移代码&emsp;&emsp;这里主要参网址的为http://zh.gluon.ai/chapter_computer-vision/neural-style.html会在下一篇博客https://blog.csdn.net/weixin_41108515/article/details/103650964里详细解释过程。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}],"tags":[{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://ailous.top/tags/卷积神经网络/"},{"name":"风格迁移","slug":"风格迁移","permalink":"http://ailous.top/tags/风格迁移/"}],"keywords":[{"name":"人工智能","slug":"人工智能","permalink":"http://ailous.top/categories/人工智能/"}]},{"title":"网络爬虫——豆瓣读书数据抓取——RE（正则表达式）","slug":"网络爬虫——豆瓣读书数据抓取——RE（正则表达式）","date":"2018-06-09T05:47:40.000Z","updated":"2020-05-11T14:55:32.869Z","comments":true,"path":"2018/06/09/网络爬虫——豆瓣读书数据抓取——RE（正则表达式）/","link":"","permalink":"http://ailous.top/2018/06/09/网络爬虫——豆瓣读书数据抓取——RE（正则表达式）/","excerpt":"","text":"网络爬虫——豆瓣读书数据抓取——RE（正则表达式）目标网址：https://book.douban.com/ 目标数据：（1）书名（2）书的链接地址（3）作者（4）发行时间（5）出版社分析网页结构，通过获取网页源代码，使用re库解析网页结构，完成豆瓣读书项目中目标数据的爬取。 源码import requests import re def getcode(url): url = url headers = { &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3756.400 QQBrowser/10.5.4039.400&#39; } response = requests.get(url,headers=headers) response.encoding = &#39;utf_8&#39; code = response.text return code def parse_page(code): ulist = [] pattern = re.compile(&#39;&lt;div class=&quot;cover&quot;&gt;.*?&lt;a href=&quot;(.*?)&quot; title=&quot;(.*?)&quot;&gt;.*?&lt;div class=&quot;author&quot;&gt;(.*?)&lt;/div&gt;.*?&lt;span class=&quot;year&quot;&gt;(.*?)&lt;/span&gt;.*?&lt;span class=&quot;publisher&quot;&gt;(.*?)&lt;/span&gt;&#39;,re.S) items = re.findall(pattern,code) # print(items) for item in items: ulist.append([item[1],item[0].strip(),item[2].strip(),item[3].strip(),item[4].strip()]) return (ulist) def main(): ulist = [] url = &quot;https://book.douban.com/&quot; code = getcode(url) ulist = parse_page(code) print(len(ulist)) print() for i in range(len(ulist)): print(ulist[i]) main() 输出如下：","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"RE（正则表达式","slug":"RE（正则表达式","permalink":"http://ailous.top/tags/RE（正则表达式/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]},{"title":"网络爬虫——猫眼电影数据抓取——RE（正则表达式）","slug":"网络爬虫——猫眼电影数据抓取——RE（正则表达式）","date":"2018-05-29T05:47:40.000Z","updated":"2020-05-11T14:55:43.291Z","comments":true,"path":"2018/05/29/网络爬虫——猫眼电影数据抓取——RE（正则表达式）/","link":"","permalink":"http://ailous.top/2018/05/29/网络爬虫——猫眼电影数据抓取——RE（正则表达式）/","excerpt":"","text":"网络爬虫——猫眼电影数据抓取——RE（正则表达式）猫眼电影榜单网址：https://maoyan.com/board/4 目标数据描述：（1）排名 （2）电影名称 （3）主演 （4）上映时间 （5）评分任务要求 （1）使用requests库实现该网站网页源代码的获取； （2）使用正则表达式对获取的源代码进行解析，并成功找到目标数据所在的特定标签，进行网页结构的解析； （3）定义函数，将获取的目标数据打印输出，有能力的同学可以试着将结果写入文件中。 （4）使用框架式结构，通过参数传递实现整个特定数据的爬取。 可以使用生成器，将目标数据获取后放到字典中返回，同时，注意观察分页时url的变化，以便获取整个的排行榜数据。建议通过for循环传递变化参数实现。 源码import json import requests from requests.exceptions import RequestException import re import time def get_one_page(url): try: headers={&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&#39;} response=requests.get(&quot;https://maoyan.com/board/4&quot;,headers = headers) if response.status_code == 200: return response.text return None except RequestException: return None def parse_one_page(html): pattern= re.compile(&#39;&lt;dd&gt;.*?board-index.*?&gt;(\\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a&#39; + &#39;.*?&gt;(.*?)&lt;/a&gt;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p&gt;&#39; + &#39;.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&#39;,re.S) items = re.findall(pattern,html) for item in items: yield{ &#39;index&#39;: item[0], &#39;image&#39;: item[1], &#39;title&#39;: item[2], &#39;actor&#39;: item[3].strip()[3:], &#39;time&#39;: item[4].strip()[5:], &#39;score&#39;: item[5] + item[6] } def write_to_file(content): with open(&#39;result.txt&#39;, &#39;a&#39; , encoding=&#39;utf-8&#39;) as f: f.write(json.dumps(content, ensure_ascii=False) + &#39;\\n&#39;) def main(offset): url = &#39;https://maoyan.com/board/4&#39; html=get_one_page(url) for item in parse_one_page(html): print(item) write_to_file(item) if __name__ == &#39;__main__&#39;: for i in range(10): main(offset=i * 10) time.sleep(1) 输出如下","categories":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}],"tags":[{"name":"RE（正则表达式）","slug":"RE（正则表达式）","permalink":"http://ailous.top/tags/RE（正则表达式）/"}],"keywords":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://ailous.top/categories/网络爬虫/"}]}]}